[
    {
        "title": "WP Engine WordPress Hosting Acquires NitroPack",
        "url": "https://www.searchenginejournal.com/wp-engine-wordpress-hosting-acquires-nitropack/522532/",
        "content": "Managed WordPress web host WP Engine announced that they are acquiring NitroPack, a leading SaaS website performance optimization solution. The acquisition of of NitroPack by WP Engine demonstrates their continued focus on improving site performance for clients.\n\nNitroPack\n\nNitroPack is a relatively pricey but well regarded site performance solution that has for years been known as a leader. WP Engine and NitroPack formed a partnership in 2023 that would power WP Engine’s PageSpeed Boost product that is offered internally to customers. The NitroPack team will now become integrated within WP Engine this month, July.\n\nThere are no immediate plans to change the pricing options for NitroPack so it’s safe to say that it will continue to be a standalone product. WP Engine commented to Search Engine Journal that there will be no immediate changes in services pricing or billing for current NitroPack customers.\n\n“We have no immediate plans to change the pricing options for NitroPack products. Today NitroPack works with page builders and other hosting providers and that will continue to be available. In the coming months, we will continue to leverage NitroPack to enhance additional functionality to Page Speed Boost for WP Engine’s customers.”\n\nWhat the acquisition means for WP Engine customers is that WP Engine will continue to leverage NitroPack’s technology to add even more functionalities to their PageSpeed Boost product.\n\nThe WP Engine spokesperson said that these new integrations will be coming to WP Engine PageSpeed Boost in a matter of months.\n\nThey shared:\n\n“In the coming months, we will continue to leverage NitroPack’s strength to enhance additional functionality to Page Speed Boost.”\n\nRead the official announcement:\n\nWP Engine Acquires NitroPack, Extending Leadership in Managed WordPress Site Performance\n\nFeatured Image by Shutterstock/Asier Romero"
    },
    {
        "title": "The 10 Best Website Builders To Consider 2024",
        "url": "https://www.searchenginejournal.com/best-website-builders/483165/",
        "content": "Choosing the right website builder may depend on your goals. They have a variety of features, and some platforms excel in areas that others don’t.\n\nNot all builders will fit if you need advanced SEO or ecommerce capabilities.\n\nWe compared 10 website builders based on price, data limits, core use cases, and whether they provide domains.\n\nThe 10 Best Website Builders Compared\n\nWebsite Builder Starting Price Free Option Premium Content Gates Limits Free Domain Great For Extras We Like WordPress.com $9/month Yes Yes 1-50 GB Yes (annual plans only) Blogging and text-based sites Easily work between the .com and self-hosted sites.\n\nCustomizability. Wix $17/month Yes Yes 2 GB-Unlimited Yes Small businesses & entrepreneurs Educational programs and support.\n\nScheduling.\n\nAd management.\n\nEmail campaigns. Duda $25/month 14 days Yes 1-4 sites No Getting started Excellent help and support.\n\nZapier integration.\n\nMultiple language sites.\n\nContent library and free assets. HubSpot $15/month Yes Yes Up to 30 pages on the free plan No Scaling Conversational bots.\n\nWide range of free tools for sales, marketing, and services.\n\nExtensive site and business owner education.\n\nMobile app. Squarespace $25/month 14 days Yes Unlimited bandwidth, 30 minutes of video storage Yes (annual plans only) Quick, no-fuss sites Custom product creation without worrying about fulfillment and shipping.\n\nIntegrated ecommerce on larger plans. Webflow $18/month Yes Yes Starts with 1 GB bandwidth and 50 CMS items Yes Designers & Agencies Schema markup and structured search support.\n\nPre-built interactions. IONOS $6/month No No 50-75 GB Yes Small businesses on a budget Affordable.\n\nCompetitor tracking.\n\nOnline booking included.\n\nBuilt-in privacy and SSL. Shopify $5/month 3 days No Unlimited products, bandwidth, and online storage No Ecommerce Wide range of ecommerce features.\n\nLarge app store for extensions. Weebly $12/month Yes No Unlimited storage Yes Beginners Ease of use.\n\nBuilt-in SEO tools. Hostinger $2.99/month No No 25,000 visits,\n\n100 GB SSD storage,\n\n400,000 files Yes Budget sites Very affordable plans.\n\n24/7 customer support.\n\n10 Best Website Builders For 2024\n\n1. WordPress.com\n\nWith 62.7% of the market share held between WordPress.com and .org, WordPress is the largest and most prominent website builder.\n\nKey Features\n\nOver 50,000 plugins and 8,000 themes for customization.\n\nAbility to transition between hosted and self-hosted options.\n\nWith paid plans, custom domains, site security, and advanced features are available.\n\nBenefits & SEO Highlights\n\nUser-friendly interface suitable for beginners.\n\nFlexibility to create various types of websites.\n\nBuilt-in SEO tools and options to optimize your site for search engines.\n\nCost\n\n$0-$70/month ($0-$45/month, billed annually), plus custom options.\n\n2. Wix\n\nWix controls only 4% of the CMS market, but that small number translates into hundreds of millions of users and makes it one of the most popular website builders.\n\nIt offers ease of use and flexibility, making it suitable for creating professional websites with expanded functionality.\n\nKey Features\n\nCustomizable templates with drag-and-drop editing.\n\nWide range of elements and third-party apps for added functionality.\n\nComprehensive business solutions, including ecommerce and marketing tools.\n\nBenefits & SEO Highlights\n\nSuitable for beginners and those needing advanced features.\n\nSEO Wiz tool for optimizing your site’s SEO settings.\n\nExtensive help, resources, and guides for website creation and promotion.\n\nCost\n\n$0-$159/month, plus custom options.\n\n3. Duda\n\nDuda is a website builder that balances ease of use with advanced customization options, making it popular among designers and developers.\n\nKey Features\n\nDrag-and-drop interface and customizable templates.\n\nWidgets and add-ons for expanded functionality, including ecommerce.\n\nMobile editor for creating mobile-friendly versions of your site\n\nBenefits & SEO Highlights\n\nSuitable for businesses and individuals seeking a professional website.\n\nBuilt-in SEO optimization features, including meta descriptions and sitemaps.\n\nExcellent customer support with live chat, email, and resources.\n\nCost:\n\n$25-$199/month ($19-$149/month, billed annually), plus custom options.\n\n4. HubSpot\n\nHubSpot is an all-in-one marketing, sales, and customer service platform with a powerful website builder.\n\nKey Features\n\nDrag-and-drop interface and customizable templates.\n\nPre-built modules for forms, CTAs, and social media integration.\n\nIntegrated CMS, marketing automation, and sales tools.\n\nBenefits & SEO Highlights\n\nIdeal for businesses seeking a comprehensive solution.\n\nBuilt-in SEO tools for keyword research, on-page optimization, and analytics.\n\nScalable platform that grows.\n\nCost\n\n$0-$450/month, plus custom options.\n\n5. Squarespace\n\nSquarespace is a website builder that offers beautifully designed templates and powerful ecommerce features.\n\nKey Features\n\nCustomizable templates that work across devices.\n\nEcommerce tools for inventory management, order tracking, and payment processing.\n\nMarketing tools for SEO, video, and audience management\n\nBenefits & SEO Highlights\n\nIdeal for businesses focusing on ecommerce and brand promotion.\n\nBuilt-in SEO features and integration with Google Analytics.\n\nMobile app for managing your site on the go.\n\nCost\n\n$25-$72/month ($16-$52/month, billed annually), and enterprise plans.\n\n6. Webflow\n\nWebflow is a website builder offering advanced design and development features suitable for users of all skill levels.\n\nKey Features\n\nFree plan for getting started with basic features.\n\nEcommerce plan with advanced tools for selling products and managing orders.\n\nTeam plan with collaboration features and client billing.\n\nBenefits & SEO Highlights\n\nSuitable for individuals and teams looking for advanced customization options.\n\nAdvanced SEO features, including schema and Open Graph.\n\nUnique features like scheduled publishing, logic flows, and animations.\n\nCost\n\n$0-$235/month ($0-$212/month, billed annually), including enterprise plans.\n\n7. IONOS\n\nIONOS is an affordable and simple website builder that offers all the essential features for creating a functional and beautiful site.\n\nKey Features\n\nThree-step site design process: choosing a design, adding content, and promoting.\n\nSearch engine-optimized templates built for performance.\n\nPresence Suite for managing and promoting your site\n\nBenefits & SEO Highlights\n\nIdeal for quick website setups, test projects, and DIYers.\n\nTemplates are pre-optimized for search engines.\n\nAffordable pricing plans with essential features.\n\nCost\n\n$6-$15/month ($4-$8/month billed three years in advance).\n\n8. Shopify\n\nShopify is a comprehensive ecommerce platform that enables businesses to create online stores and sell products easily.\n\nKey Features\n\nCustomizable templates and drag-and-drop editing.\n\nPowerful ecommerce tools for inventory management, payment processing, and shipping.\n\nThe app store has thousands of apps to extend functionality.\n\nBenefits & SEO Highlights\n\nIdeal for businesses of all sizes looking to create an online store.\n\nBuilt-in SEO features and the ability to edit meta tags, URLs, and site structure.\n\n24/7 customer support and extensive documentation.\n\nCost\n\n$19-$399/month ($29-$299/month billed annually).\n\n9. Weebly\n\nWeebly is a user-friendly website builder that offers a wide range of features for creating professional websites and online stores.\n\nKey Features\n\nDrag-and-drop interface and customizable templates.\n\nEcommerce functionality with inventory management and payment processing.\n\nBlogging platform and app center for additional features.\n\nBenefits & SEO Highlights\n\nSuitable for beginners and small businesses.\n\nBuilt-in SEO tools, including meta descriptions, alt tags, and sitemaps.\n\nResponsive customer support and community forum.\n\nCost\n\n$$0-$29/month ($10-$26/month billed annually).\n\n10. Hostinger\n\nHostinger offers an easy-to-use website-building tool in its web hosting plans, designed to help users get sites up and running fast.\n\nKey Features\n\nIntuitive and user-friendly interface.\n\nSuitable for beginners and those needing a website up and running quickly.\n\nFree domain, website migration, email, and SSL are included in the hosting package.\n\nBenefits & SEO Highlights\n\nOptimized for speed using LiteSpeed Web Server technology, advanced cache solutions, and Object Cache for WordPress.\n\nAdvanced security features, including unlimited SSL certificates, DDoS protection, automatic backups, and a 99.9% uptime guarantee.\n\nCost\n\n$2.99-$9.99 for the first month ($7.99-$19.99/month on renewal).\n\nFind The Right Website Builder For Your Needs\n\nWhen choosing a website builder, consider your needs, budget, and skill level.\n\nWordPress.com offers flexibility and customization for bloggers and content-heavy sites.\n\nSmall businesses and entrepreneurs may prefer all-in-one solutions like Wix or HubSpot for marketing integration.\n\nEcommerce stores should evaluate dedicated platforms like Shopify for robust selling tools.\n\nBeginners can start with user-friendly builders like Weebly, while designers and agencies may prefer more advanced options like Webflow.\n\nWith the variety of website builders available, there’s a solution for every need.\n\nMore resources:\n\nFeatured Image: Kaspars Grinvalds/Shutterstock"
    },
    {
        "title": "Interaction To Next Paint (INP): Everything You Need To Know",
        "url": "https://www.searchenginejournal.com/core-web-vitals/interaction-to-next-paint/",
        "content": "The SEO field has no shortage of acronyms.\n\nFrom SEO to FID to INP – these are some of the more common ones you will run into when it comes to page speed.\n\nThere’s a new metric in the mix: INP, which stands for Interaction to Next Paint. It refers to how the page responds to specific user interactions and is measured by Google Chrome’s lab data and field data.\n\nWhat, Exactly, Is Interaction To Next Paint?\n\nInteraction to Next Paint, or INP, is a new Core Web Vitals metric designed to represent the overall interaction delay of a page throughout the user journey.\n\nFor example, when you click the Add to Cart button on a product page, it measures how long it takes for the button’s visual state to update, such as changing the color of the button on click.\n\nIf you have heavy scripts running that take a long time to complete, they may cause the page to freeze temporarily, negatively impacting the INP metric.\n\nHere is the example video illustrating how it looks in real life:\n\nNotice how the first button responds visually instantly, whereas it takes a couple of seconds for the second button to update its visual state.\n\nHow Is INP Different From FID?\n\nThe main difference between INP and First Input Delay, or FID, is that FID considers only the first interaction on the page. It measures the input delay metric only and doesn’t consider how long it takes for the browser to respond to the interaction.\n\nIn contrast, INP considers all page interactions and measures the time browsers need to process them. INP, however, takes into account the following types of interactions:\n\nAny mouse click of an interactive element.\n\nAny tap of an interactive element on any device that includes a touchscreen.\n\nThe press of a key on a physical or onscreen keyboard.\n\nWhat Is A Good INP Value?\n\nAccording to Google, a good INP value is around 200 milliseconds or less. It has the following thresholds:\n\nThreshold Value Description 200 Good responsiveness. Above 200 milliseconds and up to 500 milliseconds Moderate and needs improvement. Above 500 milliseconds Poor responsiveness.\n\nGoogle also notes that INP is still experimental and that the guidance it recommends regarding this metric is likely to change.\n\nHow Is INP Measured?\n\nGoogle measures INP from Chrome browsers anonymously from a sample of the single longest interactions that happen when a user visits the page.\n\nEach interaction has a few phases: presentation time, processing time, and input delay. The callback of associated events contains the total time involved for all three phases to execute.\n\nIf a page has fewer than 50 total interactions, INP considers the interaction with the absolute worst delay; if it has over 50 interactions, it ignores the longest interactions per 50 interactions.\n\nWhen the user leaves the page, these measurements are then sent to the Chrome User Experience Report called CrUX, which aggregates the performance data to provide insights into real-world user experiences, known as field data.\n\nWhat Are The Common Reasons Causing High INPs?\n\nUnderstanding the underlying causes of high INPs is crucial for optimizing your website’s performance. Here are the common causes:\n\nLong tasks that can block the main thread, delaying user interactions.\n\nSynchronous event listeners on click events, as we saw in the example video above.\n\nChanges to the DOM cause multiple reflows and repaints, which usually happens when the DOM size is too large ( > 1,500 HTML elements).\n\nHow To Troubleshoot INP Issues?\n\nFirst, read our guide on how to measure CWV metrics and try the troubleshooting techniques offered there. But if that still doesn’t help you find what interactions cause high INP, this is where the “Performance” report of the Chrome (or, better, Canary) browser can help.\n\nGo to the webpage you want to analyze.\n\nOpen DevTools of your Canary browser, which doesn’t have browser extensions (usually by pressing F12 or Ctrl+Shift+I).\n\nSwitch to the Performance tab.\n\nDisable cache from the Network tab.\n\nChoose mobile emulator.\n\nClick the Record button and interact with the page elements as you normally would.\n\nStop the recording once you’ve captured the interaction you’re interested in.\n\nThrottle the CPU by 4x using the “slowdown” dropdown to simulate average mobile devices and choose a 4G network, which is used in 90% of mobile devices when users are outdoors. If you don’t change this setting, you will run your simulation using your PC’s powerful CPU, which is not equivalent to mobile devices.\n\nIt is a highly important nuance since Google uses field data gathered from real users’ devices. You may not face INP issues with a powerful device – that is a tricky point that makes it hard to debug INP. By choosing these settings, you bring your emulator state as close as possible to the real device’s state.\n\nHere is a video guide that shows the whole process. I highly recommend you try this as you read the article to gain experience.\n\nWhat we have spotted in the video is that long tasks cause interaction to take longer and a list of JavaScript files that are responsible for those tasks.\n\nIf you expand the Interactions section, you can see a detailed breakdown of the long task associated with that interaction, and clicking on those script URLs will open JavaScript code lines that are responsible for the delay, which you can use to optimize your code.\n\nA total of 321 ms long interaction consists of:\n\nInput delay: 207 ms.\n\nProcessing duration: 102 ms.\n\nPresentation delay: 12 ms.\n\nBelow in the main thread timeline, you’ll see a long red bar representing the total duration of the long task.\n\nUnderneath the long red taskbar, you can see a yellow bar labeled “Evaluate Script,” indicating that the long task was primarily caused by JavaScript execution.\n\nIn the first screenshot time distance between (point 1) and (point 2) is a delay caused by a red long task because of script evaluation.\n\nWhat Is Script Evaluation?\n\nScript evaluation is a necessary step for JavaScript execution. During this crucial stage, the browser executes the code line by line, which includes assigning values to variables, defining functions, and registering event listeners.\n\nUsers might interact with a partially rendered page while JavaScript files are still being loaded, parsed, compiled, and evaluated.\n\nWhen a user interacts with an element (clicks, taps, etc.) and the browser is in the stage of evaluating a script that contains an event listener attached to the interaction, it may delay the interaction until the script evaluation is complete.\n\nThis ensures that the event listener is properly registered and can respond to the interaction.\n\nIn the screenshot (point 2), the 207 ms delay likely occurred because the browser was still evaluating the script that contained the event listener for the click.\n\nThis is where Total Blocking Time (TBT) comes in, which measures the total amount of time that long tasks (longer than 50 ms) block the main thread until the page becomes interactive.\n\nIf that time is long and users interact with the website as soon as the page renders, the browser may not be able to respond promptly to the user interaction.\n\nIt is not a part of CWV metrics but often correlates with high INPs. So, in order to optimize for the INP metric, you should aim to lower your TBT.\n\nWhat Are Common JavaScripts That Cause High TBT?\n\nAnalytics scripts – such as Google Analytics 4, tracking pixels, google re-captcha, or AdSense ads – usually cause high script evaluation time, thus contributing to TBT.\n\nOne strategy you may want to implement to reduce TBT is to delay the loading of non-essential scripts until after the initial page content has finished loading.\n\nAnother important point is that when delaying scripts, it’s essential to prioritize them based on their impact on user experience. Critical scripts (e.g., those essential for key interactions) should be loaded earlier than less critical ones.\n\nImproving Your INP Is Not A Silver Bullet\n\nIt’s important to note that improving your INP is not a silver bullet that guarantees instant SEO success.\n\nInstead, it is one item among many that may need to be completed as part of a batch of quality changes that can help make a difference in your overall SEO performance.\n\nThese include optimizing your content, building high-quality backlinks, enhancing meta tags and descriptions, using structured data, improving site architecture, addressing any crawl errors, and many others.\n\nMore resources:\n\nFeatured Image: BestForBest/Shutterstock"
    },
    {
        "title": "Google Confirms Ranking Boost For Country Code Domains",
        "url": "https://www.searchenginejournal.com/google-confirms-ranking-preference/522477/",
        "content": "Google’s Gary Illyes answered a question about a ranking preference given to sites that use country level domain names and explained how that compares to non-country domain names. The question occurred in the SEO Office Hours podcast.\n\nccTLD Aka Country Code Domain Names\n\nDomain names that are specific to countries are called ccTLDs (Country Code Top Level Domains). These are domain names that target specific countries. Examples of these ccTLDs are .de (Germany), .in (India) and .kr (Korea). These kinds of domain names don’t target specific languages, they only target Internet users in a specific country.\n\nSome ccTLDs are treated by Google for ranking purposes as if they are regular Generic Top Level Domains (gTLDs), which are domains that are not specific to a country. A popular example is .io, which technically is a ccTLD (pertaining to the British Indian Ocean Territory) but because of how it’s used, Google treats it like a regular gTLD (generic top level domain).\n\nRanking Boosts For ccTLDs\n\nThe question that Gary Illyes answered was about the ranking boost given to ccTLDs.\n\nThis is the question:\n\n“When a Korean person searches Google in Korean, does a com.kr domain or a .com domain do better?”\n\nGary Illyes answered:\n\n“Good question. Generally speaking the local domain names, in your case .kr, tend to do better because Google Search promotes content local to the user.”\n\nA lot of people want to rank better in a specific country and one of the best practices for doing that is to register a domain name that is specific to the country. Google will give it a ranking boost over other sites that are not explicitly targeting a specific country.\n\nGary continued his answer by explaining the ranking boost of a ccTLD over a generic top level domain (gTLD), like .com, .net and so on.\n\nThis is Gary’s explanation:\n\n“That’s not to say that a .com domain can’t do well, it can, but generally .kr has a little more benefit, albeit not too much. “\n\nRelated:\n\nTargeting Country Versus Targeting Language\n\nLastly, Gary mentioned that targeting a user’s language has more impact than the domain name.\n\nHe continued his answer:\n\n“If the language of a site matches the user’s query language, that probably has more impact than the domain name itself.”\n\nA benefit of targeting a language is that a site is able regardless of the country that a user is searching from whereas the country code top level domain name targets a country.\n\nSomething that Gary didn’t mention is that using a ccTLD can inspire user trust from searchers whose country matches the country that the domain name is targeting and because of that searchers on Google may be more inclined to click on a search result that uses the geotargeted ccTLD.\n\nIf a user is in Korea they may feel that a .kr domain is meant specifically for them. If a searcher is in Australia they may feel more inclined to click on a .au domain name.\n\nListen to the podcast answer from the 3:35 minute mark:\n\nSee also:\n\nFeatured Image by Shutterstock/Dean Drobot"
    },
    {
        "title": "The 10 Best Website Builders To Consider 2024",
        "url": "https://www.searchenginejournal.com/best-website-builders/483165/",
        "content": "Choosing the right website builder may depend on your goals. They have a variety of features, and some platforms excel in areas that others don’t.\n\nNot all builders will fit if you need advanced SEO or ecommerce capabilities.\n\nWe compared 10 website builders based on price, data limits, core use cases, and whether they provide domains.\n\nThe 10 Best Website Builders Compared\n\nWebsite Builder Starting Price Free Option Premium Content Gates Limits Free Domain Great For Extras We Like WordPress.com $9/month Yes Yes 1-50 GB Yes (annual plans only) Blogging and text-based sites Easily work between the .com and self-hosted sites.\n\nCustomizability. Wix $17/month Yes Yes 2 GB-Unlimited Yes Small businesses & entrepreneurs Educational programs and support.\n\nScheduling.\n\nAd management.\n\nEmail campaigns. Duda $25/month 14 days Yes 1-4 sites No Getting started Excellent help and support.\n\nZapier integration.\n\nMultiple language sites.\n\nContent library and free assets. HubSpot $15/month Yes Yes Up to 30 pages on the free plan No Scaling Conversational bots.\n\nWide range of free tools for sales, marketing, and services.\n\nExtensive site and business owner education.\n\nMobile app. Squarespace $25/month 14 days Yes Unlimited bandwidth, 30 minutes of video storage Yes (annual plans only) Quick, no-fuss sites Custom product creation without worrying about fulfillment and shipping.\n\nIntegrated ecommerce on larger plans. Webflow $18/month Yes Yes Starts with 1 GB bandwidth and 50 CMS items Yes Designers & Agencies Schema markup and structured search support.\n\nPre-built interactions. IONOS $6/month No No 50-75 GB Yes Small businesses on a budget Affordable.\n\nCompetitor tracking.\n\nOnline booking included.\n\nBuilt-in privacy and SSL. Shopify $5/month 3 days No Unlimited products, bandwidth, and online storage No Ecommerce Wide range of ecommerce features.\n\nLarge app store for extensions. Weebly $12/month Yes No Unlimited storage Yes Beginners Ease of use.\n\nBuilt-in SEO tools. Hostinger $2.99/month No No 25,000 visits,\n\n100 GB SSD storage,\n\n400,000 files Yes Budget sites Very affordable plans.\n\n24/7 customer support.\n\n10 Best Website Builders For 2024\n\n1. WordPress.com\n\nWith 62.7% of the market share held between WordPress.com and .org, WordPress is the largest and most prominent website builder.\n\nKey Features\n\nOver 50,000 plugins and 8,000 themes for customization.\n\nAbility to transition between hosted and self-hosted options.\n\nWith paid plans, custom domains, site security, and advanced features are available.\n\nBenefits & SEO Highlights\n\nUser-friendly interface suitable for beginners.\n\nFlexibility to create various types of websites.\n\nBuilt-in SEO tools and options to optimize your site for search engines.\n\nCost\n\n$0-$70/month ($0-$45/month, billed annually), plus custom options.\n\n2. Wix\n\nWix controls only 4% of the CMS market, but that small number translates into hundreds of millions of users and makes it one of the most popular website builders.\n\nIt offers ease of use and flexibility, making it suitable for creating professional websites with expanded functionality.\n\nKey Features\n\nCustomizable templates with drag-and-drop editing.\n\nWide range of elements and third-party apps for added functionality.\n\nComprehensive business solutions, including ecommerce and marketing tools.\n\nBenefits & SEO Highlights\n\nSuitable for beginners and those needing advanced features.\n\nSEO Wiz tool for optimizing your site’s SEO settings.\n\nExtensive help, resources, and guides for website creation and promotion.\n\nCost\n\n$0-$159/month, plus custom options.\n\n3. Duda\n\nDuda is a website builder that balances ease of use with advanced customization options, making it popular among designers and developers.\n\nKey Features\n\nDrag-and-drop interface and customizable templates.\n\nWidgets and add-ons for expanded functionality, including ecommerce.\n\nMobile editor for creating mobile-friendly versions of your site\n\nBenefits & SEO Highlights\n\nSuitable for businesses and individuals seeking a professional website.\n\nBuilt-in SEO optimization features, including meta descriptions and sitemaps.\n\nExcellent customer support with live chat, email, and resources.\n\nCost:\n\n$25-$199/month ($19-$149/month, billed annually), plus custom options.\n\n4. HubSpot\n\nHubSpot is an all-in-one marketing, sales, and customer service platform with a powerful website builder.\n\nKey Features\n\nDrag-and-drop interface and customizable templates.\n\nPre-built modules for forms, CTAs, and social media integration.\n\nIntegrated CMS, marketing automation, and sales tools.\n\nBenefits & SEO Highlights\n\nIdeal for businesses seeking a comprehensive solution.\n\nBuilt-in SEO tools for keyword research, on-page optimization, and analytics.\n\nScalable platform that grows.\n\nCost\n\n$0-$450/month, plus custom options.\n\n5. Squarespace\n\nSquarespace is a website builder that offers beautifully designed templates and powerful ecommerce features.\n\nKey Features\n\nCustomizable templates that work across devices.\n\nEcommerce tools for inventory management, order tracking, and payment processing.\n\nMarketing tools for SEO, video, and audience management\n\nBenefits & SEO Highlights\n\nIdeal for businesses focusing on ecommerce and brand promotion.\n\nBuilt-in SEO features and integration with Google Analytics.\n\nMobile app for managing your site on the go.\n\nCost\n\n$25-$72/month ($16-$52/month, billed annually), and enterprise plans.\n\n6. Webflow\n\nWebflow is a website builder offering advanced design and development features suitable for users of all skill levels.\n\nKey Features\n\nFree plan for getting started with basic features.\n\nEcommerce plan with advanced tools for selling products and managing orders.\n\nTeam plan with collaboration features and client billing.\n\nBenefits & SEO Highlights\n\nSuitable for individuals and teams looking for advanced customization options.\n\nAdvanced SEO features, including schema and Open Graph.\n\nUnique features like scheduled publishing, logic flows, and animations.\n\nCost\n\n$0-$235/month ($0-$212/month, billed annually), including enterprise plans.\n\n7. IONOS\n\nIONOS is an affordable and simple website builder that offers all the essential features for creating a functional and beautiful site.\n\nKey Features\n\nThree-step site design process: choosing a design, adding content, and promoting.\n\nSearch engine-optimized templates built for performance.\n\nPresence Suite for managing and promoting your site\n\nBenefits & SEO Highlights\n\nIdeal for quick website setups, test projects, and DIYers.\n\nTemplates are pre-optimized for search engines.\n\nAffordable pricing plans with essential features.\n\nCost\n\n$6-$15/month ($4-$8/month billed three years in advance).\n\n8. Shopify\n\nShopify is a comprehensive ecommerce platform that enables businesses to create online stores and sell products easily.\n\nKey Features\n\nCustomizable templates and drag-and-drop editing.\n\nPowerful ecommerce tools for inventory management, payment processing, and shipping.\n\nThe app store has thousands of apps to extend functionality.\n\nBenefits & SEO Highlights\n\nIdeal for businesses of all sizes looking to create an online store.\n\nBuilt-in SEO features and the ability to edit meta tags, URLs, and site structure.\n\n24/7 customer support and extensive documentation.\n\nCost\n\n$19-$399/month ($29-$299/month billed annually).\n\n9. Weebly\n\nWeebly is a user-friendly website builder that offers a wide range of features for creating professional websites and online stores.\n\nKey Features\n\nDrag-and-drop interface and customizable templates.\n\nEcommerce functionality with inventory management and payment processing.\n\nBlogging platform and app center for additional features.\n\nBenefits & SEO Highlights\n\nSuitable for beginners and small businesses.\n\nBuilt-in SEO tools, including meta descriptions, alt tags, and sitemaps.\n\nResponsive customer support and community forum.\n\nCost\n\n$$0-$29/month ($10-$26/month billed annually).\n\n10. Hostinger\n\nHostinger offers an easy-to-use website-building tool in its web hosting plans, designed to help users get sites up and running fast.\n\nKey Features\n\nIntuitive and user-friendly interface.\n\nSuitable for beginners and those needing a website up and running quickly.\n\nFree domain, website migration, email, and SSL are included in the hosting package.\n\nBenefits & SEO Highlights\n\nOptimized for speed using LiteSpeed Web Server technology, advanced cache solutions, and Object Cache for WordPress.\n\nAdvanced security features, including unlimited SSL certificates, DDoS protection, automatic backups, and a 99.9% uptime guarantee.\n\nCost\n\n$2.99-$9.99 for the first month ($7.99-$19.99/month on renewal).\n\nFind The Right Website Builder For Your Needs\n\nWhen choosing a website builder, consider your needs, budget, and skill level.\n\nWordPress.com offers flexibility and customization for bloggers and content-heavy sites.\n\nSmall businesses and entrepreneurs may prefer all-in-one solutions like Wix or HubSpot for marketing integration.\n\nEcommerce stores should evaluate dedicated platforms like Shopify for robust selling tools.\n\nBeginners can start with user-friendly builders like Weebly, while designers and agencies may prefer more advanced options like Webflow.\n\nWith the variety of website builders available, there’s a solution for every need.\n\nMore resources:\n\nFeatured Image: Kaspars Grinvalds/Shutterstock"
    },
    {
        "title": "WP Engine WordPress Hosting Acquires NitroPack",
        "url": "https://www.searchenginejournal.com/wp-engine-wordpress-hosting-acquires-nitropack/522532/",
        "content": "Managed WordPress web host WP Engine announced that they are acquiring NitroPack, a leading SaaS website performance optimization solution. The acquisition of of NitroPack by WP Engine demonstrates their continued focus on improving site performance for clients.\n\nNitroPack\n\nNitroPack is a relatively pricey but well regarded site performance solution that has for years been known as a leader. WP Engine and NitroPack formed a partnership in 2023 that would power WP Engine’s PageSpeed Boost product that is offered internally to customers. The NitroPack team will now become integrated within WP Engine this month, July.\n\nThere are no immediate plans to change the pricing options for NitroPack so it’s safe to say that it will continue to be a standalone product. WP Engine commented to Search Engine Journal that there will be no immediate changes in services pricing or billing for current NitroPack customers.\n\n“We have no immediate plans to change the pricing options for NitroPack products. Today NitroPack works with page builders and other hosting providers and that will continue to be available. In the coming months, we will continue to leverage NitroPack to enhance additional functionality to Page Speed Boost for WP Engine’s customers.”\n\nWhat the acquisition means for WP Engine customers is that WP Engine will continue to leverage NitroPack’s technology to add even more functionalities to their PageSpeed Boost product.\n\nThe WP Engine spokesperson said that these new integrations will be coming to WP Engine PageSpeed Boost in a matter of months.\n\nThey shared:\n\n“In the coming months, we will continue to leverage NitroPack’s strength to enhance additional functionality to Page Speed Boost.”\n\nRead the official announcement:\n\nWP Engine Acquires NitroPack, Extending Leadership in Managed WordPress Site Performance\n\nFeatured Image by Shutterstock/Asier Romero"
    },
    {
        "title": "Interaction To Next Paint (INP): Everything You Need To Know",
        "url": "https://www.searchenginejournal.com/core-web-vitals/interaction-to-next-paint/",
        "content": "The SEO field has no shortage of acronyms.\n\nFrom SEO to FID to INP – these are some of the more common ones you will run into when it comes to page speed.\n\nThere’s a new metric in the mix: INP, which stands for Interaction to Next Paint. It refers to how the page responds to specific user interactions and is measured by Google Chrome’s lab data and field data.\n\nWhat, Exactly, Is Interaction To Next Paint?\n\nInteraction to Next Paint, or INP, is a new Core Web Vitals metric designed to represent the overall interaction delay of a page throughout the user journey.\n\nFor example, when you click the Add to Cart button on a product page, it measures how long it takes for the button’s visual state to update, such as changing the color of the button on click.\n\nIf you have heavy scripts running that take a long time to complete, they may cause the page to freeze temporarily, negatively impacting the INP metric.\n\nHere is the example video illustrating how it looks in real life:\n\nNotice how the first button responds visually instantly, whereas it takes a couple of seconds for the second button to update its visual state.\n\nHow Is INP Different From FID?\n\nThe main difference between INP and First Input Delay, or FID, is that FID considers only the first interaction on the page. It measures the input delay metric only and doesn’t consider how long it takes for the browser to respond to the interaction.\n\nIn contrast, INP considers all page interactions and measures the time browsers need to process them. INP, however, takes into account the following types of interactions:\n\nAny mouse click of an interactive element.\n\nAny tap of an interactive element on any device that includes a touchscreen.\n\nThe press of a key on a physical or onscreen keyboard.\n\nWhat Is A Good INP Value?\n\nAccording to Google, a good INP value is around 200 milliseconds or less. It has the following thresholds:\n\nThreshold Value Description 200 Good responsiveness. Above 200 milliseconds and up to 500 milliseconds Moderate and needs improvement. Above 500 milliseconds Poor responsiveness.\n\nGoogle also notes that INP is still experimental and that the guidance it recommends regarding this metric is likely to change.\n\nHow Is INP Measured?\n\nGoogle measures INP from Chrome browsers anonymously from a sample of the single longest interactions that happen when a user visits the page.\n\nEach interaction has a few phases: presentation time, processing time, and input delay. The callback of associated events contains the total time involved for all three phases to execute.\n\nIf a page has fewer than 50 total interactions, INP considers the interaction with the absolute worst delay; if it has over 50 interactions, it ignores the longest interactions per 50 interactions.\n\nWhen the user leaves the page, these measurements are then sent to the Chrome User Experience Report called CrUX, which aggregates the performance data to provide insights into real-world user experiences, known as field data.\n\nWhat Are The Common Reasons Causing High INPs?\n\nUnderstanding the underlying causes of high INPs is crucial for optimizing your website’s performance. Here are the common causes:\n\nLong tasks that can block the main thread, delaying user interactions.\n\nSynchronous event listeners on click events, as we saw in the example video above.\n\nChanges to the DOM cause multiple reflows and repaints, which usually happens when the DOM size is too large ( > 1,500 HTML elements).\n\nHow To Troubleshoot INP Issues?\n\nFirst, read our guide on how to measure CWV metrics and try the troubleshooting techniques offered there. But if that still doesn’t help you find what interactions cause high INP, this is where the “Performance” report of the Chrome (or, better, Canary) browser can help.\n\nGo to the webpage you want to analyze.\n\nOpen DevTools of your Canary browser, which doesn’t have browser extensions (usually by pressing F12 or Ctrl+Shift+I).\n\nSwitch to the Performance tab.\n\nDisable cache from the Network tab.\n\nChoose mobile emulator.\n\nClick the Record button and interact with the page elements as you normally would.\n\nStop the recording once you’ve captured the interaction you’re interested in.\n\nThrottle the CPU by 4x using the “slowdown” dropdown to simulate average mobile devices and choose a 4G network, which is used in 90% of mobile devices when users are outdoors. If you don’t change this setting, you will run your simulation using your PC’s powerful CPU, which is not equivalent to mobile devices.\n\nIt is a highly important nuance since Google uses field data gathered from real users’ devices. You may not face INP issues with a powerful device – that is a tricky point that makes it hard to debug INP. By choosing these settings, you bring your emulator state as close as possible to the real device’s state.\n\nHere is a video guide that shows the whole process. I highly recommend you try this as you read the article to gain experience.\n\nWhat we have spotted in the video is that long tasks cause interaction to take longer and a list of JavaScript files that are responsible for those tasks.\n\nIf you expand the Interactions section, you can see a detailed breakdown of the long task associated with that interaction, and clicking on those script URLs will open JavaScript code lines that are responsible for the delay, which you can use to optimize your code.\n\nA total of 321 ms long interaction consists of:\n\nInput delay: 207 ms.\n\nProcessing duration: 102 ms.\n\nPresentation delay: 12 ms.\n\nBelow in the main thread timeline, you’ll see a long red bar representing the total duration of the long task.\n\nUnderneath the long red taskbar, you can see a yellow bar labeled “Evaluate Script,” indicating that the long task was primarily caused by JavaScript execution.\n\nIn the first screenshot time distance between (point 1) and (point 2) is a delay caused by a red long task because of script evaluation.\n\nWhat Is Script Evaluation?\n\nScript evaluation is a necessary step for JavaScript execution. During this crucial stage, the browser executes the code line by line, which includes assigning values to variables, defining functions, and registering event listeners.\n\nUsers might interact with a partially rendered page while JavaScript files are still being loaded, parsed, compiled, and evaluated.\n\nWhen a user interacts with an element (clicks, taps, etc.) and the browser is in the stage of evaluating a script that contains an event listener attached to the interaction, it may delay the interaction until the script evaluation is complete.\n\nThis ensures that the event listener is properly registered and can respond to the interaction.\n\nIn the screenshot (point 2), the 207 ms delay likely occurred because the browser was still evaluating the script that contained the event listener for the click.\n\nThis is where Total Blocking Time (TBT) comes in, which measures the total amount of time that long tasks (longer than 50 ms) block the main thread until the page becomes interactive.\n\nIf that time is long and users interact with the website as soon as the page renders, the browser may not be able to respond promptly to the user interaction.\n\nIt is not a part of CWV metrics but often correlates with high INPs. So, in order to optimize for the INP metric, you should aim to lower your TBT.\n\nWhat Are Common JavaScripts That Cause High TBT?\n\nAnalytics scripts – such as Google Analytics 4, tracking pixels, google re-captcha, or AdSense ads – usually cause high script evaluation time, thus contributing to TBT.\n\nOne strategy you may want to implement to reduce TBT is to delay the loading of non-essential scripts until after the initial page content has finished loading.\n\nAnother important point is that when delaying scripts, it’s essential to prioritize them based on their impact on user experience. Critical scripts (e.g., those essential for key interactions) should be loaded earlier than less critical ones.\n\nImproving Your INP Is Not A Silver Bullet\n\nIt’s important to note that improving your INP is not a silver bullet that guarantees instant SEO success.\n\nInstead, it is one item among many that may need to be completed as part of a batch of quality changes that can help make a difference in your overall SEO performance.\n\nThese include optimizing your content, building high-quality backlinks, enhancing meta tags and descriptions, using structured data, improving site architecture, addressing any crawl errors, and many others.\n\nMore resources:\n\nFeatured Image: BestForBest/Shutterstock"
    },
    {
        "title": "How To Spot SEO Myths: 26 Common SEO Myths, Debunked",
        "url": "https://www.searchenginejournal.com/seo/seo-myths/",
        "content": "SEO is a complex, vast, and sometimes mysterious practice. There are a lot of aspects to SEO that can lead to confusion.\n\nNot everyone will agree with what SEO entails – where technical SEO stops and development begins.\n\nWhat also doesn’t help is the vast amount of misinformation that goes around. There are a lot of “experts” online and not all of them should bear that self-proclaimed title. How do you know who to trust?\n\nEven Google employees can sometimes add to the confusion. They struggle to define their own updates and systems and sometimes offer advice that conflicts with previously given statements.\n\nThe Dangers Of SEO Myths\n\nThe issue is that we simply don’t know exactly how the search engines work. Due to this, much of what we do as SEO professionals is trial and error and educated guesswork.\n\nWhen you are learning about SEO, it can be difficult to test out all the claims you hear.\n\nThat’s when the SEO myths begin to take hold. Before you know it, you’re proudly telling your line manager that you’re planning to “AI Overview optimize” your website copy.\n\nSEO myths can be busted a lot of the time with a pause and some consideration.\n\nHow, exactly, would Google be able to measure that? Would that actually benefit the end user in any way?\n\nThere is a danger in SEO of considering the search engines to be omnipotent, and because of this, wild myths about how they understand and measure our websites start to grow.\n\nWhat Is An SEO Myth?\n\nBefore we debunk some common SEO myths, we should first understand what forms they take.\n\nUntested Wisdom\n\nMyths in SEO tend to take the form of handed-down wisdom that isn’t tested.\n\nAs a result, something that might well have no impact on driving qualified organic traffic to a site gets treated like it matters.\n\nMinor Factors Blown Out Of Proportion\n\nSEO myths might also be something that has a small impact on organic rankings or conversion but are given too much importance.\n\nThis might be a “tick box” exercise that is hailed as being a critical factor in SEO success, or simply an activity that might only cause your site to eke ahead if everything else with your competition was truly equal.\n\nMyths can arise simply because what used to be effective in helping sites rank and convert well no longer does but is still being advised. It might be that something used to work really well.\n\nOver time, the algorithms have grown smarter. The public is more adverse to being marketed to.\n\nSimply, what was once good advice is now defunct.\n\nGoogle Being Misunderstood\n\nMany times, the start of a myth is Google itself.\n\nUnfortunately, a slightly obscure or just not straightforward piece of advice from a Google representative gets misunderstood and run away with.\n\nBefore we know it, a new optimization service is being sold off the back of a flippant comment a Googler made in jest.\n\nSEO myths can be based on fact, or perhaps these are, more accurately, SEO legends?\n\nIn the case of Google-born myths, it tends to be that the fact has been so distorted by the SEO industry’s interpretation of the statement that it no longer resembles useful information.\n\n26 Common SEO Myths\n\nSo, now that we know what causes and perpetuates SEO myths, let’s find out the truth behind some of the more common ones.\n\n1. The Google Sandbox And Honeymoon Effects\n\nSome SEO professionals believe that Google will automatically suppress new websites in the organic search results for a period of time before they are able to rank more freely.\n\nOthers suggest there is a sort of Honeymoon Period, during which Google will rank new content highly to test what users think of it.\n\nThe content would be promoted to ensure more users see it. Signals like click-through rate and bounces back to the search engine results pages (SERPs) would then be used to measure if the content is well received and deserves to remain ranked highly.\n\nThere is, however, the Google Privacy Sandbox. This is designed to help maintain peoples’ privacy online. This is a different sandbox from the one that allegedly suppresses new websites.\n\nWhen asked specifically about the Honeymoon Effect and the rankings Sandbox, John Mueller answered:\n\n“In the SEO world, this is sometimes called kind of like a sandbox where Google is like keeping things back to prevent new pages from showing up, which is not the case. Or some people call it like the honeymoon period where new content comes out and Google really loves it and tries to promote it. And it’s again not the case that we’re explicitly trying to promote new content or demote new content. It’s just, we don’t know and we have to make assumptions. And then sometimes those assumptions are right and nothing really changes over time. Sometimes things settle down a little bit lower, sometimes a little bit higher.”\n\nSo, there is no systematic promotion or demotion of new content by Google, but what you might be noticing is that Google’s assumptions are based on the rest of the website’s rankings.\n\nVerdict: Officially? It’s a myth.\n\n2. Duplicate Content Penalty\n\nThis is a myth that I hear a lot. The idea is that if you have content on your website that is duplicated elsewhere on the web, Google will penalize you for it.\n\nThe key to understanding what is really going on here is knowing the difference between algorithmic suppression and manual action.\n\nA manual action, the situation that can result in webpages being removed from Google’s index, will be actioned by a human at Google.\n\nThe website owner will be notified through Google Search Console.\n\nAn algorithmic suppression occurs when your page cannot rank well due to it being caught by a filter from an algorithm.\n\nEssentially, having copy that is taken from another webpage might mean you can’t outrank that other page.\n\nThe search engines may determine that the original host of the copy is more relevant to the search query than yours.\n\nAs there is no benefit to having both in the search results, yours gets suppressed. This is not a penalty. This is the algorithm doing its job.\n\nThere are some content-related manual actions, but essentially, copying one or two pages of someone else’s content is not going to trigger them.\n\nIt is, however, potentially going to land you in other trouble if you have no legal right to use that content. It also can detract from the value your website brings to the user.\n\nWhat about content that is duplicated across your own site? Mueller clarifies that duplicate content is not a negative ranking factor. If there are multiple pages with the same content, Google may choose one to be the canonical page, and the others will not be ranked.\n\nVerdict: SEO myth.\n\n3. PPC Advertising Helps Rankings\n\nThis is a common myth. It’s also quite quick to debunk.\n\nThe idea is that Google will favor websites that spend money with it through pay-per-click advertising. This is simply false.\n\nGoogle’s algorithm for ranking organic search results is completely separate from the one used to determine PPC ad placements.\n\nRunning a paid search advertising campaign through Google while carrying out SEO might benefit your site for other reasons, but it won’t directly benefit your ranking.\n\nVerdict: SEO myth.\n\n4. Domain Age Is A Ranking Factor\n\nThis claim is seated firmly in the “confusing causation and correlation” camp.\n\nBecause a website has been around for a long time and is ranking well, age must be a ranking factor.\n\nGoogle has debunked this myth itself many times.\n\nIn July 2019, Mueller replied to a post on Twitter.com (recovered through Wayback Machine) that suggested that domain age was one of “200 signals of ranking” saying, “No, domain age helps nothing.”\n\nThe truth behind this myth is that an older website has had more time to do things well.\n\nFor instance, a website that has been live and active for 10 years may well have acquired a high volume of relevant backlinks to its key pages.\n\nA website that has been running for less than six months will be unlikely to compete with that.\n\nThe older website appears to be ranking better, and the conclusion is that age must be the determining factor.\n\nVerdict: SEO myth\n\n5. Tabbed Content Affects Rankings\n\nThis idea is one that has roots going back a long way.\n\nThe premise is that Google will not assign as much value to the content sitting behind a tab or accordion.\n\nFor example, text that is not viewable on the first load of a page.\n\nGoogle again debunked this myth in March 2020, but it has been a contentious idea among many SEO professionals for years.\n\nIn September 2018, Gary Illyes, Webmaster Trends Analyst at Google, answered a tweet thread about using tabs to display content.\n\nHis response:\n\n“AFAIK, nothing’s changed here, Bill: we index the content, and its weight is fully considered for ranking, but it might not get bolded in the snippets. It’s another, more technical question of how that content is surfaced by the site. Indexing does have limitations.”\n\nIf the content is visible in the HTML, there is no reason to assume that it is being devalued just because it is not apparent to the user on the first load of the page. This is not an example of cloaking, and Google can easily fetch the content.\n\nAs long as there is nothing else that is stopping the text from being viewed by Google, it should be weighted the same as copy, which isn’t in tabs.\n\nWant more clarification on this? Then check out this SEJ article that discusses this subject in detail.\n\nVerdict: SEO myth.\n\n6. Google Uses Google Analytics Data In Rankings\n\nThis is a common fear among business owners.\n\nThey study their Google Analytics reports. They feel their average sitewide bounce rate is too high, or their time on page is too low.\n\nSo, they worry that Google will perceive their site to be low quality because of that. They fear they won’t rank well because of it.\n\nThe myth is that Google uses the data in your Google Analytics account as part of its ranking algorithm.\n\nIt’s a myth that has been around for a long time.\n\nIllyes has again debunked this idea simply with, “We don’t use *anything* from Google analytics [sic] in the “algo.”\n\nMore recently, John Mueller dispelled this idea yet again, saying, “That’s not going to happen” when he received the suggestion telling SEO pros that GA4 is a ranking factor would improve its uptake.\n\nIf we think about this logically, using Google Analytics data as a ranking factor would be really hard to police.\n\nFor instance, using filters could manipulate data to make it seem like the site was performing in a way that it isn’t really.\n\nWhat is good performance anyway?\n\nHigh “time on page” might be good for some long-form content.\n\nLow “time on page” could be understandable for shorter content.\n\nIs either one right or wrong?\n\nGoogle would also need to understand the intricate ways in which each Google Analytics account had been configured.\n\nSome might be excluding all known bots, and others might not. Some might use custom dimensions and channel groupings, and others haven’t configured anything.\n\nUsing this data reliably would be extremely complicated to do. Consider the hundreds of thousands of websites that use other analytics programs.\n\nHow would Google treat them?\n\nVerdict: SEO myth.\n\nThis myth is another case of “causation, not correlation.”\n\nA high sitewide bounce rate might be indicative of a quality problem, or it might not be. Low time on page could mean your site isn’t engaging, or it could mean your content is quickly digestible.\n\nThese metrics give you clues as to why you might not be ranking well, they aren’t the cause of it.\n\n7. Google Cares About Domain Authority\n\nPageRank is a link analysis algorithm used by Google to measure the importance of a webpage.\n\nGoogle used to display a page’s PageRank score a number up to 10 on its toolbar. It stopped updating the PageRank displayed in toolbars in 2013.\n\nIn 2016, Google confirmed that the PageRank toolbar metric was not going to be used going forward.\n\nIn the absence of PageRank, many other third-party authority scores have been developed.\n\nCommonly known ones are:\n\nMoz’s Domain Authority and Page Authority scores.\n\nMajestic’s Trust Flow and Citation Flow.\n\nAhrefs’ Domain Rating and URL Rating.\n\nSome SEO pros use these scores to determine the “value” of a page.\n\nThat calculation can never be an entirely accurate reflection of how a search engine values a page, however.\n\nSEO pros will sometimes refer to the ranking power of a website often in conjunction with its backlink profile and this, too, is known as the domain’s authority.\n\nYou can see where the confusion lies.\n\nGoogle representatives have dispelled the notion of a domain authority metric used by them.\n\nJohn Mueller said in 2022:\n\n“We don’t use domain authority. We generally try to have our metrics as granular as possible, sometimes that’s not so easy, in which case we look at things a bit broader (e.g., we’ve talked about this in regards to some of the older quality updates).”\n\nVerdict: SEO myth.\n\n8. Longer Content Is Better\n\nYou will have definitely heard it said before that longer content ranks better.\n\nMore words on a page automatically make yours more rank-worthy than your competitor’s. This is “wisdom” that is often shared around SEO forums without little evidence to substantiate it.\n\nThere are a lot of studies that have been released over the years that state facts about the top-ranking webpages, such as “on average pages in the top 10 positions in the SERPs have over 1,450 words on them.”\n\nIt would be quite easy for someone to take this information in isolation and assume it means that pages need approximately 1,500 words to rank on Page 1. That isn’t what the study is saying, however.\n\nUnfortunately, this is an example of correlation, not necessarily causation.\n\nJust because the top-ranking pages in a particular study happened to have more words on them than the pages ranking 11th and lower does not make word count a ranking factor.\n\nMueller dispelled this myth yet again in a Google SEO Office Hours in February 2021.\n\n“From our point of view the number of words on a page is not a quality factor, not a ranking factor.”\n\nFor more information on how content length can impact SEO, check out Sam Hollingsworth’s article.\n\nVerdict: SEO myth.\n\n9. LSI Keywords Will Help You Rank\n\nWhat exactly are LSI keywords? LSI stands for “latent semantic indexing.”\n\nIt is a technique used in information retrieval that allows concepts within the text to be analyzed and relationships between them identified.\n\nWords have nuances dependent on their context. The word “right” has a different connotation when paired with “left” than when it is paired with “wrong.”\n\nHumans can quickly gauge concepts in a text. It is harder for machines to do so.\n\nThe ability of machines to understand the context and linking between entities is fundamental to their understanding of concepts.\n\nLSI is a huge step forward for a machine’s ability to understand text. What it isn’t is synonyms.\n\nUnfortunately, the field of LSI has been devolved by the SEO community into the understanding that using words that are similar or linked thematically will boost rankings for words that aren’t expressly mentioned in the text.\n\nIt’s simply not true. Google has gone far beyond LSI in its understanding of text with the introduction of BERT, as just one example.\n\nFor more about what LSI is and how it does or doesn’t affect rankings, take a look at this article.\n\nVerdict: SEO myth.\n\n10. SEO Takes 3 Months\n\nIt helps us get out of sticky conversations with our bosses or clients. It leaves a lot of wiggle room if you aren’t getting the results you promised. “SEO takes at least three months to have an effect.”\n\nIt is fair to say that there are some changes that will take time for the search engine bots to process.\n\nThere is then, of course, some time to see if those changes are having a positive or negative effect. Then more time might be needed to refine and tweak your work.\n\nThat doesn’t mean that any activity you carry out in the name of SEO is going to have no effect for three months. Day 90 of your work will not be when the ranking changes kick in. There is a lot more to it than that.\n\nIf you are in a very low-competition market, targeting niche terms, you might see ranking changes as soon as Google recrawls your page. A competitive term could take much longer to see changes in rank.\n\nA study by Semrush suggested that of the 28,000 domains they analyzed, only 19% of domains started ranking in the top 10 positions within six months and managed to maintain those rankings for the rest of the 13-month study.\n\nThis study indicates that newer pages struggle to rank high.\n\nHowever, there is more to SEO than ranking in the top 10 of Google.\n\nFor instance, a well-positioned Google Business Profile listing with great reviews can pay dividends for a company. Bing, Yandex, and Baidu might make it easier for your brand to conquer the SERPs.\n\nA small tweak to a page title could see an improvement in click-through rates. That could be the same day if the search engine were to recrawl the page quickly.\n\nAlthough it can take a long time to see first page rankings in Google, it is naïve of us to reduce SEO success just down to that.\n\nTherefore, “SEO takes 3 months” simply isn’t accurate.\n\nVerdict: SEO myth.\n\n11. Bounce Rate Is A Ranking Factor\n\nBounce rate is the percentage of visits to your website that result in no interactions beyond landing on the page. It is typically measured by a website’s analytics program, such as Google Analytics.\n\nSome SEO professionals have argued that bounce rate is a ranking factor because it is a measure of quality.\n\nUnfortunately, it is not a good measure of quality.\n\nThere are many reasons why a visitor might land on a webpage and leave again without interacting further with the site. They may well have read all the information they needed on that page and left the site to call the company and book an appointment.\n\nIn that instance, the visitor bouncing has resulted in a lead for the company.\n\nAlthough a visitor leaving a page having landed on it could be an indicator of poor quality content, it isn’t always. Therefore, it wouldn’t be reliable enough for a search engine to use as a measure of quality.\n\n“Pogo-sticking,” or a visitor clicking on a search result and then returning to the SERPs, would be a more reliable indicator of the quality of the landing page.\n\nIt would suggest that the content of the page was not what the user was after, so much so that they have returned to the search results to find another page or re-search.\n\nJohn Mueller cleared this up (again) during Google Webmaster Central Office Hours in June 2020. He was asked if sending users to a login page would appear to be a “bounce” to Google and damage their rankings:\n\n“So, I think there is a bit of misconception here, that we’re looking at things like the analytics bounce rate when it comes to ranking websites, and that’s definitely not the case.”\n\nBack on another Google Webmaster Central Office Hours in July 2018, he also said:\n\n“We try not to use signals like that when it comes to search. So that’s something where there are lots of reasons why users might go back and forth, or look at different things in the search results, or stay just briefly on a page and move back again. I think that’s really hard to refine and say, “well, we could turn this into a ranking factor.”\n\nSo, why does this keep coming up? Well, for a lot of people, it’s because of this one paragraph in Google’s How Search Works:\n\n“Beyond looking at keywords, our systems also analyze if content is relevant to a query in other ways. We also use aggregated and anonymised interaction data to assess whether Search results are relevant to queries.”\n\nThe issue with this is that Google doesn’t specify what this “aggregated and anonymised interaction data” is. This has led to a lot of speculation and of course, arguments.\n\nMy opinion? Until we have some more conclusive studies, or hear something else from Google, we need to keep testing to determine what this interaction data is.\n\nFor now, regarding the traditional definition of a bounce, I’m leaning towards “myth.”\n\nIn itself, bounce rate (measured through the likes of Google Analytics) is a very noisy, easily manipulated figure. Could something akin to a bounce be a ranking signal? Absolutely, but it will need to be a reliable, repeatable data point that genuinely measures quality.\n\nIn the meantime, if your pages are not satisfying user intent, that is definitely something you need to work on – not simply because of bounce rate.\n\nFundamentally, your pages should encourage users to interact, or if not that sort of page, at least leave your site with a positive brand association.\n\nVerdict: SEO myth.\n\n12. It’s All About Backlinks\n\nBacklinks are important – that’s without much contention within the SEO community. However, exactly how important is still debated.\n\nSome SEO pros will tell you that backlinks are one of the many tactics that will influence rankings, but they are not the most important. Others will tell you it’s the only real game-changer.\n\nWhat we do know is that the effectiveness of links has changed over time. Back in the wild pre-Jagger days, link-building consisted of adding a link to your website wherever you could.\n\nForum comments had spun articles, and irrelevant directories were all good sources of links.\n\nIt was easy to build effective links. It’s not so easy now.\n\nGoogle has continued to make changes to its algorithms that reward higher-quality, more relevant links and disregard or penalize “spammy” links.\n\nHowever, the power of links to affect rankings is still great.\n\nThere will be some industries that are so immature in SEO that a site can rank well without investing in link-building, purely through the strength of their content and technical efficiency.\n\nThat’s not the case with most industries.\n\nRelevant backlinks will, of course, help with ranking, but they need to go hand-in-hand with other optimizations. Your website still needs to have relevant content, and it must be crawlable.\n\nIf you want your traffic to actually do something when they hit your website, it’s definitely not all about backlinks.\n\nRanking is only one part of getting converting visitors to your site. The content and usability of the site are extremely important in user engagement.\n\nFollowing the slew of Helpful Content updates and a better understanding of what Google considers E-E-A-T, we know that content quality is extremely important.\n\nBacklinks can definitely help to indicate that a page would be useful to a reader, but there are many other factors that would suggest that, too.\n\nVerdict: SEO myth.\n\n13. Keywords In URLs Are Very Important\n\nCram your URLs full of keywords. It’ll help.\n\nUnfortunately, it’s not quite as powerful as that.\n\nJohn Mueller has said several times that keywords in a URL are a very minor, lightweight ranking signal.\n\nIn a Google SEO Office Hours in 2021, he affirmed again:\n\n“We use the words in a URL as a very, very lightweight factor. And from what I recall, this is primarily something that we would take into account when we haven’t had access to the content yet. So, if this is the absolute first time we see this URL and we don’t know how to classify its content, then we might use the words in the URL as something to help rank us better. But as soon as we’ve crawled and indexed the content there, then we have a lot more information.”\n\nIf you are looking to rewrite your URLs to include more keywords, you are likely to do more damage than good.\n\nThe process of redirecting URLs en masse should be when necessary, as there is always a risk when restructuring a site.\n\nFor the sake of adding keywords to a URL? Not worth it.\n\nVerdict: SEO myth.\n\n14. Website Migrations Are All About Redirects\n\nSEO professionals hear this too often. If you are migrating a website, all you need to do is remember to redirect any URLs that are changing.\n\nIf only this one were true.\n\nIn actuality, website migration is one of the most fraught and complicated procedures in SEO.\n\nA website changing its layout, content management system (CMS), domain, and/or content can all be considered a website migration.\n\nIn each of those examples, there are several aspects that could affect how the search engines perceive the quality and relevance of the pages to their targeted keywords.\n\nAs a result, there are numerous checks and configurations that need to occur if the site is to maintain its rankings and organic traffic – ensuring tracking hasn’t been lost, maintaining the same content targeting, and making sure the search engine bots can still access the right pages.\n\nAll of this needs to be considered when a website is significantly changing.\n\nRedirecting URLs that are changing is a very important part of website migration. It is in no way the only thing to be concerned about.\n\nVerdict: SEO myth.\n\n15. Well-Known Websites Will Always Outrank Unknown Websites\n\nIt stands to reason that a larger brand will have resources that smaller brands do not. As a result, more can be invested in SEO.\n\nMore exciting content pieces can be created, leading to a higher volume of backlinks acquired. The brand name alone can lend more credence to outreach attempts.\n\nThe real question is, does Google algorithmically or manually boost big brands because of their fame?\n\nThis one is a bit contentious.\n\nSome people say that Google favors big brands. Google says otherwise.\n\nIn 2009, Google released an algorithm update named “Vince.” This update had a huge impact on how brands were treated in the SERPs.\n\nBrands that were well-known offline saw ranking increases for broad competitive keywords. It stands to reason that brand awareness can help with discovery through Search.\n\nIt’s not necessarily time for smaller brands to throw in the towel.\n\nThe Vince update falls very much in line with other Google moves towards valuing authority and quality.\n\nBig brands are often more authoritative on broad-level keywords than smaller contenders.\n\nHowever, small brands can still win.\n\nLong-tail keyword targeting, niche product lines, and local presence can all make smaller brands more relevant to a search result than established brands.\n\nYes, the odds are stacked in favor of big brands, but it’s not impossible to outrank them.\n\nVerdict: Not entirely truth or myth.\n\n16. Your Page Needs To Include ‘Near Me’ To Rank Well For Local SEO\n\nIt’s understandable that this myth is still prevalent.\n\nThere is still a lot of focus on keyword search volumes in the SEO industry, sometimes at the expense of considering user intent and how the search engines understand it.\n\nWhen a searcher is looking for something with local intent, i.e., a place or service relevant to a physical location, the search engines will take this into consideration when returning results.\n\nWith Google, you will likely see the Google Maps results as well as the standard organic listings.\n\nThe Maps results are clearly centered around the location searched. However, so are the standard organic listings when the search query denotes local intent.\n\nSo, why do “near me” searches confuse some?\n\nA typical keyword research exercise might yield something like the following:\n\n“pizza restaurant manhattan” – 110 searches per month.\n\n“pizza restaurants in manhattan” – 110 searches per month.\n\n“best pizza restaurant manhattan” – 90 searches per month.\n\n“best pizza restaurants in manhattan” – 90 searches per month.\n\n“best pizza restaurant in manhattan”– 90 searches per month.\n\n“pizza restaurants near me” – 90,500 searches per month.\n\nWith search volume like that, you would think [pizza restaurants near me] would be the one to rank for, right?\n\nIt is likely, however, that people searching for [pizza restaurant manhattan] are in the Manhattan area or planning to travel there for pizza.\n\n[pizza restaurant near me] has 90,500 searches across the USA. The likelihood is that the vast majority of those searchers are not looking for Manhattan pizzas.\n\nGoogle knows this and, therefore, will serve pizza restaurant results relevant to the searcher’s location.\n\nTherefore, the “near me” element of the search becomes less about the keyword and more about the intent behind the keyword. Google will just consider it to be the location the searcher is in.\n\nSo, do you need to include “near me” in your content to rank for those [near me] searches?\n\nNo, you need to be relevant to the location the searcher is in.\n\nVerdict: SEO myth.\n\n17. Better Content Equals Better Rankings\n\nIt’s prevalent in SEO forums and X (formally Twitter) threads. The common complaint is, “My competitor is ranking above me, but I have amazing content, and theirs is terrible.”\n\nThe cry is one of indignation. After all, shouldn’t search engines reward sites for their “amazing” content?\n\nThis is both a myth and sometimes a delusion.\n\nThe quality of content is a subjective consideration. If it is your own content, it’s harder still to be objective.\n\nPerhaps in Google’s eyes, your content isn’t better than your competitors’ for the search terms you are looking to rank for.\n\nPerhaps you don’t meet searcher intent as well as they do. Maybe you have “over-optimized” your content and reduced its quality.\n\nIn some instances, better content will equal better rankings. In others, the technical performance of the site or its lack of local relevance may cause it to rank lower.\n\nContent is one factor within the ranking algorithms.\n\nVerdict: SEO myth.\n\n18. You Need To Blog Every Day\n\nThis is a frustrating myth because it seems to have spread outside of the SEO industry.\n\nGoogle loves frequent content. You should add new content or tweak existing content daily for “freshness.”\n\nWhere did this idea come from?\n\nGoogle had an algorithm update in 2011 that rewards fresher results in the SERPs.\n\nThis is because, for some queries, the fresher the results, the better the likelihood of accuracy.\n\nFor instance, if you search for [royal baby] in the UK in 2013, you will be served with news articles about Prince George. Search it again in 2015, and you will see pages about Princess Charlotte.\n\nIn 2018, you would see reports about Prince Louis at the top of the Google SERPs, and in 2019 it would be baby Archie.\n\nIf you were to search [royal baby] in 2021, shortly after the birth of Lilibet, then seeing news articles on Prince George would likely be unhelpful.\n\nIn this instance, Google discerns the user’s search intent and decides showing articles related to the newest UK royal baby would be better than showing an article that is arguably more rank-worthy due to authority, etc.\n\nWhat this algorithm update doesn’t mean is that newer content will always outrank older content. Google decides if the “query deserves freshness” or not.\n\nIf it does, then the age of content becomes a more important ranking factor.\n\nThis means that if you are creating content purely to make sure it is newer than competitors’ content, you are not necessarily going to benefit.\n\nIf the query you are looking to rank for does not deserve freshness, i.e., [who is Prince William’s third child?] a fact that will not change, then the age of content will not play a significant part in rankings.\n\nIf you are writing content every day thinking it is keeping your website fresh and, therefore, more rank-worthy, then you are likely wasting time.\n\nIt would be better to write well-considered, researched, and useful content pieces less frequently and reserve your resources to make those highly authoritative and shareable.\n\nVerdict: SEO myth.\n\n19. You Can Optimize Copy Once & Then It’s Done\n\nThe phrase “SEO optimized” copy is a common one in agency-land.\n\nIt’s used as a way to explain the process of creating copy that will be relevant to frequently searched queries.\n\nThe trouble with this is that it suggests that once you have written that copy – and ensured it adequately answers searchers’ queries – you can move on.\n\nUnfortunately, over time, how searchers look for content might change. The keywords they use, the type of content they want could alter.\n\nThe search engines, too, may change what they feel is the most relevant answer to the query. Perhaps the intent behind the keyword is perceived differently.\n\nThe layout of the SERPs might alter, meaning videos are being shown at the top of the search results where previously it was just webpage results.\n\nIf you look at a page only once and then don’t continue to update it and evolve it with user needs, then you risk falling behind.\n\nVerdict: SEO myth.\n\n20. Google Respects The Declared Canonical URL As The Preferred Version For Search Results\n\nThis can be very frustrating. You have several pages that are near duplicates of each other. You know which one is your main page, the one you want to rank, the “canonical.” You tell Google that through the specially selected “rel=canonical” tag.\n\nYou’ve chosen it. You’ve identified it in the HTML.\n\nGoogle ignores your wishes, and another of the duplicate pages ranks in its place.\n\nThe idea that Google will take your chosen page and treat it like the canonical out of a set of duplicates isn’t a challenging one.\n\nIt makes sense that the website owner would know best which page should be the one that ranks above its cousins. However, Google will sometimes disagree.\n\nThere may be instances where another page from the set is chosen by Google as a better candidate to show in the search results.\n\nThis could be because the page receives more backlinks from external sites than your chosen page. It could be that it’s included in the sitemap or is being linked to your main navigation.\n\nEssentially, the canonical tag is a signal – one of many that will be taken into consideration when Google chooses which page from a set of duplicates should rank.\n\nIf you have conflicting signals on your site, or externally, then your chosen canonical page may be overlooked in favor of another page.\n\nWant to know if Google has selected another URL to be the canonical despite your canonical tag? In Google Search Console, in the Index Coverage report, you might see this: “Duplicate, Google chose different canonical than user.”\n\nGoogle’s support documents helpfully explain what this means:\n\n“This page is marked as canonical for a set of pages, but Google thinks another URL makes a better canonical. Google has indexed the page that we consider canonical rather than this one.”\n\nVerdict: SEO myth.\n\n21. Google Has 3 Top Ranking Factors\n\nIt’s links, content, and Rank Brain, right?\n\nThis idea that these are the three top ranking factors seems to come from a WebPromo Q&A in 2016 with Andrei Lipattsev, a search quality senior strategist at Google at the time (recovered through Wayback Machine; find this discussion at around the 30-minute mark).\n\nWhen questioned on the “other two” top ranking factors, the questioner assumed that Rank Brain was one, Lipattsev stated that links pointing to a site, and content were the other two. He does clarify by saying:\n\n“Third place is a hotly contested issue. I think… It’s a funny one. Take this with a grain of salt. […] And so I guess, if you do that, then you’ll see elements of RankBrain having been involved in here, rewriting this query, applying it like this over here… And so you’d say, ‘I see this two times as often as the other thing, and two times as often as the other thing’. So it’s somewhere in number three. It’s not like having three links is ‘X’ important, and having five keywords is ‘Y’ important, and RankBrain is some ‘Z’ factor that is also somehow important, and you multiply all of that … That’s not how this works.”\n\nHowever it started, the concept prevails. A good backlink profile, great copy, and “Rank Brain” type signals are what matter most with rankings, according to many SEO pros.\n\nWhat we have to take into consideration when reviewing this idea is John Mueller’s response to a question in a 2017 English Google Webmaster Central office-hours hangout.\n\nMueller is asked if there is a one-size-fits-all approach to the top three ranking signals in Google. His answer is a clear “No.”\n\nHe follows that statement with a discussion around the timeliness of searches and how that might require different search results to be shown.\n\nHe also mentions that depending on the context of the search, different results may need to be shown, for instance, brand or shopping.\n\nHe continues to explain that he doesn’t think that there is one set of ranking factors that can be declared the top three that apply to all search results all the time.\n\nWithin the “How Search Works” documentation it clearly states:\n\n“To give you the most useful information, Search algorithms look at many factors and signals, including the words of your query, relevance and usability of pages, expertise of sources, and your location and settings. The weight applied to each factor varies depending on the nature of your query. For example, the freshness of the content plays a bigger role in answering queries about current news topics than it does about dictionary definitions. ”\n\nVerdict: Not entirely true or myth.\n\n22. Use The Disavow File To Proactively Maintain A Site’s Link Profile\n\nTo disavow or not disavow — this question has popped up a lot over the years since Penguin 4.0.\n\nSome SEO professionals are in favor of adding any link that could be considered spammy to their site’s disavow file. Others are more confident that Google will ignore them anyway and save themselves the trouble.\n\nIt’s definitely more nuanced than that.\n\nIn a 2019 Webmaster Central Office Hours Hangout, Mueller was asked about the disavow tool and whether we should have confidence that Google is ignoring medium (but not very) spammy links.\n\nHis answer indicated that there are two instances where you might want to use a disavow file:\n\nIn cases where a manual action has been given.\n\nAnd where you might think if someone from the webspam team saw it, they would issue a manual action.\n\nYou might not want to add every spammy link to your disavow file. In practice, that could take a long time if you have a very visible site that accrues thousands of these links a month.\n\nThere will be some links that are obviously spammy, and their acquisition is not a result of activity on your part.\n\nHowever, where they are a result of some less-than-awesome link building strategies (buying links, link exchanges, etc.) you may want to proactively disavow them.\n\nRead Roger Montti’s full breakdown of the 2019 exchange with John Mueller to get a better idea of the context around this discussion.\n\nVerdict: Not a myth, but don’t waste your time unnecessarily.\n\n23. Google Values Backlinks From All High Authority Domains\n\nThe better the website authority, the bigger the impact it will have on your site’s ability to rank. You will hear that in many SEO pitches, client meetings, and training sessions.\n\nHowever, that’s not the whole story.\n\nFor one, it’s arguable whether Google has a concept of domain authority (see “Google Cares About Domain Authority” above).\n\nAnd more importantly, it is the understanding that there is a lot that goes into Google’s calculations of whether a link will impact a site’s ability to rank highly or not.\n\nRelevancy, contextual clues, no-follow link attributes. None of these should be ignored when chasing a link from a high “domain authority” website.\n\nJohn Mueller also threw a cat among the pigeons during a live Search Off the Record podcast recorded at BrightonSEO in 2022 when he said:\n\n“And to some extent, links will always be something that we care about because we have to find pages somehow. It’s like how do you find a page on the web without some reference to it?” But my guess is over time, it won’t be such a big factor as sometimes it is today. I think already, that’s something that’s been changing quite a bit.”\n\nVerdict: Myth.\n\n24. You Cannot Rank A Page Without Lightning-Fast Loading Speed\n\nThere are many reasons to make your pages fast: usability, crawlability, and conversion. Arguably, it is important for the health and performance of your website, and that should be enough to make it a priority.\n\nHowever, is it something that is absolutely key to ranking your website?\n\nAs this Google Search Central post from 2010 suggests, it was definitely something that factored into the ranking algorithms. Back when it was published, Google stated:\n\n“While site speed is a new signal, it doesn’t carry as much weight as the relevance of a page. Currently, fewer than 1% of search queries are affected by the site speed signal in our implementation and the signal for site speed only applies for visitors searching in English on Google.com at this point.”\n\nIs it still only affecting such a low percentage of visitors?\n\nIn 2021, the Google Page Experience system, which incorporates the Core Web Vitals for which speed is important, rolled out on mobile. It was followed in 2022 with a rollout of the system to desktop.\n\nThis was met with a flurry of activity from SEO pros, trying to get ready for the update.\n\nMany perceive it to be something that would make or break their site’s ranking potential. However, over time, Google representatives have downplayed the ranking effect of Core Web Vitals.\n\nMore recently, in May 2023, Google introduced Interaction to Next Paint (INP) to the Core Web Vitals to replace First Input Delay (FID).\n\nGoogle claims that INP helps to deal with some of the limitations found with FID. This change in how a page’s responsiveness is measured shows that Google still cares about accurately measuring user experience.\n\nFrom Google’s previous statements and recent focus on Core Web Vitals, we can see that load speed continues to be an important ranking factor.\n\nHowever, it will not necessarily cause your website to dramatically increase or decrease in rankings.\n\nGoogle representatives Gary Illyes, Martin Splitt, and John Mueller hypothesized in 2021 during a “Search off the Record” podcast about the weighting of speed as a ranking factor.\n\nTheir discussion drew out the thinking around page load speed as a ranking metric and how it would need to be considered a fairly lightweight signal.\n\nThey went on to talk about it being more of a tie-breaker, as you can make an empty page lightning-fast, but it will not serve much use for a searcher.\n\nJohn Mueller reinforced this in 2022 during Google SEO Office Hours when he said:\n\n“Core Web Vitals is definitely a ranking factor. We have that for mobile and desktop now. It is based on what users actually see and not kind of a theoretical test of your pages […] What you don’t tend to see is big ranking changes overall for that. But rather, you would see changes for queries where we have similar content in the search results. So if someone is searching for your company name, we would not show some random blog, just because it’s a little bit faster, instead of your homepage. We would show your homepage, even if it’s very slow. On the other hand, if someone is searching for, I don’t know, running shoes, and there are lots of people writing about running shoes, then that’s where the speed aspect does play a bit more of a role.”\n\nWith this in mind, can we consider page speed a major ranking factor?\n\nMy opinion is no, page speed is definitely one of the ways Google decides which pages should rank above others, but not a major one.\n\nVerdict: Myth.\n\n25. Crawl Budget Isn’t An Issue\n\nCrawl budget – the idea that every time Googlebot visits your website, there is a limited number of resources it will visit – isn’t a contentious issue. However, how much attention should be paid to it is.\n\nFor instance, many SEO professionals will consider crawl budget optimization a central part of any technical SEO roadmap. Others will only consider it if a site reaches a certain size or complexity.\n\nGoogle is a company with finite resources. It cannot possibly crawl every single page of every site every time its bots visit them. Therefore, some of the sites that get visited might not see all of their pages crawled every time.\n\nGoogle has helpfully created a guide for owners of large and frequently updated websites to help them understand how to enable their sites to be crawled.\n\nIn the guide, Google states:\n\n“If your site does not have a large number of pages that change rapidly, or if your pages seem to be crawled the same day that they are published, you don’t need to read this guide; merely keeping your sitemap up to date and checking your index coverage regularly is adequate.”\n\nTherefore, it would seem that Google is in favor of some sites paying attention to its advice on managing crawl budget, but doesn’t consider it necessary for all.\n\nFor some sites, particularly ones that have a complex technical setup and many hundreds of thousands of pages, managing crawl budget is important. For those with a handful of easily crawled pages, it isn’t.\n\nVerdict: SEO myth.\n\n26. There Is A Right Way To Do SEO\n\nThis is probably a myth in many industries, but it seems prevalent in SEO. There is a lot of gatekeeping in SEO social media, forums, and chats.\n\nUnfortunately, it’s not that simple.\n\nWe know some core tenets about SEO.\n\nUsually, something is stated by a search engine representative that has been dissected, tested, and ultimately declared true.\n\nThe rest is a result of personal and collective trial and error, testing, and experience.\n\nProcesses are extremely valuable within SEO business functions, but they have to evolve and be applied appropriately.\n\nDifferent websites within different industries will respond to changes in ways others would not. Altering a meta title so it is under 60 characters long might help the click-through rate for one page and not for another.\n\nUltimately, we have to hold any SEO advice we’re given lightly before deciding whether it is right for the website you are working on.\n\nVerdict: SEO myth.\n\nWhen Can Something Appear To Be A Myth\n\nSometimes an SEO technique can be written off as a myth by others purely because they have not experienced success from carrying out this activity for their own site.\n\nIt is important to remember that every website has its own industry, set of competitors, the technology powering it, and other factors that make it unique.\n\nBlanket application of techniques to every website and expecting them to have the same outcome is naive.\n\nSomeone may not have had success with a technique when they have tried it in their highly competitive vertical.\n\nIt doesn’t mean it won’t help someone in a less competitive industry have success.\n\nCausation & Correlation Being Confused\n\nSometimes, SEO myths arise because of an inappropriate connection between an activity that was carried out and a rise in organic search performance.\n\nIf an SEO has seen a benefit from something they did, then it is natural that they would advise others to try the same.\n\nUnfortunately, we’re not always great at separating causation and correlation.\n\nJust because rankings or click-through rates increased around the same time as you implemented a new tactic doesn’t mean it caused the increase. There could be other factors at play.\n\nSoon, an SEO myth will arise from an overeager SEO who wants to share what they incorrectly believe to be a golden ticket.\n\nSteering Clear Of SEO Myths\n\nIt can save you from experiencing headaches, lost revenue, and a whole lot of time if you learn to spot SEO myths and act accordingly.\n\nTest\n\nThe key to not falling for SEO myths is making sure you can test advice whenever possible.\n\nIf you have been given the advice that structuring your page titles a certain way will help your pages rank better for their chosen keywords, then try it with one or two pages first.\n\nThis can help you measure whether making a change across many pages will be worth the time before you commit to it.\n\nIs Google Just Testing?\n\nSometimes, there will be a big uproar in the SEO community because of changes in the way Google displays or orders search results.\n\nThese changes are often tested in the wild before they are rolled out to more search results.\n\nOnce a big change has been spotted by one or two SEO pros, advice on how to optimize for it begins to spread.\n\nRemember the favicons in the desktop search results? The upset that caused the SEO industry (and Google users in general) was vast.\n\nSuddenly, articles sprang up about the importance of favicons in attracting users to your search results. There was barely time to study whether favicons would impact the click-through rate that much.\n\nBecause just like that, Google changed it back.\n\nBefore you jump for the latest SEO advice being spread around Twitter as a result of a change by Google, wait to see if it will hold.\n\nIt could be that the advice that appears sound now will quickly become a myth if Google rolls back changes.\n\nMore resources:\n\nFeatured Image: Search Engine Journal/Paulo Bobita"
    },
    {
        "title": "Google Confirms Ranking Boost For Country Code Domains",
        "url": "https://www.searchenginejournal.com/google-confirms-ranking-preference/522477/",
        "content": "Google’s Gary Illyes answered a question about a ranking preference given to sites that use country level domain names and explained how that compares to non-country domain names. The question occurred in the SEO Office Hours podcast.\n\nccTLD Aka Country Code Domain Names\n\nDomain names that are specific to countries are called ccTLDs (Country Code Top Level Domains). These are domain names that target specific countries. Examples of these ccTLDs are .de (Germany), .in (India) and .kr (Korea). These kinds of domain names don’t target specific languages, they only target Internet users in a specific country.\n\nSome ccTLDs are treated by Google for ranking purposes as if they are regular Generic Top Level Domains (gTLDs), which are domains that are not specific to a country. A popular example is .io, which technically is a ccTLD (pertaining to the British Indian Ocean Territory) but because of how it’s used, Google treats it like a regular gTLD (generic top level domain).\n\nRanking Boosts For ccTLDs\n\nThe question that Gary Illyes answered was about the ranking boost given to ccTLDs.\n\nThis is the question:\n\n“When a Korean person searches Google in Korean, does a com.kr domain or a .com domain do better?”\n\nGary Illyes answered:\n\n“Good question. Generally speaking the local domain names, in your case .kr, tend to do better because Google Search promotes content local to the user.”\n\nA lot of people want to rank better in a specific country and one of the best practices for doing that is to register a domain name that is specific to the country. Google will give it a ranking boost over other sites that are not explicitly targeting a specific country.\n\nGary continued his answer by explaining the ranking boost of a ccTLD over a generic top level domain (gTLD), like .com, .net and so on.\n\nThis is Gary’s explanation:\n\n“That’s not to say that a .com domain can’t do well, it can, but generally .kr has a little more benefit, albeit not too much. “\n\nRelated:\n\nTargeting Country Versus Targeting Language\n\nLastly, Gary mentioned that targeting a user’s language has more impact than the domain name.\n\nHe continued his answer:\n\n“If the language of a site matches the user’s query language, that probably has more impact than the domain name itself.”\n\nA benefit of targeting a language is that a site is able regardless of the country that a user is searching from whereas the country code top level domain name targets a country.\n\nSomething that Gary didn’t mention is that using a ccTLD can inspire user trust from searchers whose country matches the country that the domain name is targeting and because of that searchers on Google may be more inclined to click on a search result that uses the geotargeted ccTLD.\n\nIf a user is in Korea they may feel that a .kr domain is meant specifically for them. If a searcher is in Australia they may feel more inclined to click on a .au domain name.\n\nListen to the podcast answer from the 3:35 minute mark:\n\nSee also:\n\nFeatured Image by Shutterstock/Dean Drobot"
    },
    {
        "title": "How To Spot SEO Myths: 26 Common SEO Myths, Debunked",
        "url": "https://www.searchenginejournal.com/seo/seo-myths/",
        "content": "SEO is a complex, vast, and sometimes mysterious practice. There are a lot of aspects to SEO that can lead to confusion.\n\nNot everyone will agree with what SEO entails – where technical SEO stops and development begins.\n\nWhat also doesn’t help is the vast amount of misinformation that goes around. There are a lot of “experts” online and not all of them should bear that self-proclaimed title. How do you know who to trust?\n\nEven Google employees can sometimes add to the confusion. They struggle to define their own updates and systems and sometimes offer advice that conflicts with previously given statements.\n\nThe Dangers Of SEO Myths\n\nThe issue is that we simply don’t know exactly how the search engines work. Due to this, much of what we do as SEO professionals is trial and error and educated guesswork.\n\nWhen you are learning about SEO, it can be difficult to test out all the claims you hear.\n\nThat’s when the SEO myths begin to take hold. Before you know it, you’re proudly telling your line manager that you’re planning to “AI Overview optimize” your website copy.\n\nSEO myths can be busted a lot of the time with a pause and some consideration.\n\nHow, exactly, would Google be able to measure that? Would that actually benefit the end user in any way?\n\nThere is a danger in SEO of considering the search engines to be omnipotent, and because of this, wild myths about how they understand and measure our websites start to grow.\n\nWhat Is An SEO Myth?\n\nBefore we debunk some common SEO myths, we should first understand what forms they take.\n\nUntested Wisdom\n\nMyths in SEO tend to take the form of handed-down wisdom that isn’t tested.\n\nAs a result, something that might well have no impact on driving qualified organic traffic to a site gets treated like it matters.\n\nMinor Factors Blown Out Of Proportion\n\nSEO myths might also be something that has a small impact on organic rankings or conversion but are given too much importance.\n\nThis might be a “tick box” exercise that is hailed as being a critical factor in SEO success, or simply an activity that might only cause your site to eke ahead if everything else with your competition was truly equal.\n\nMyths can arise simply because what used to be effective in helping sites rank and convert well no longer does but is still being advised. It might be that something used to work really well.\n\nOver time, the algorithms have grown smarter. The public is more adverse to being marketed to.\n\nSimply, what was once good advice is now defunct.\n\nGoogle Being Misunderstood\n\nMany times, the start of a myth is Google itself.\n\nUnfortunately, a slightly obscure or just not straightforward piece of advice from a Google representative gets misunderstood and run away with.\n\nBefore we know it, a new optimization service is being sold off the back of a flippant comment a Googler made in jest.\n\nSEO myths can be based on fact, or perhaps these are, more accurately, SEO legends?\n\nIn the case of Google-born myths, it tends to be that the fact has been so distorted by the SEO industry’s interpretation of the statement that it no longer resembles useful information.\n\n26 Common SEO Myths\n\nSo, now that we know what causes and perpetuates SEO myths, let’s find out the truth behind some of the more common ones.\n\n1. The Google Sandbox And Honeymoon Effects\n\nSome SEO professionals believe that Google will automatically suppress new websites in the organic search results for a period of time before they are able to rank more freely.\n\nOthers suggest there is a sort of Honeymoon Period, during which Google will rank new content highly to test what users think of it.\n\nThe content would be promoted to ensure more users see it. Signals like click-through rate and bounces back to the search engine results pages (SERPs) would then be used to measure if the content is well received and deserves to remain ranked highly.\n\nThere is, however, the Google Privacy Sandbox. This is designed to help maintain peoples’ privacy online. This is a different sandbox from the one that allegedly suppresses new websites.\n\nWhen asked specifically about the Honeymoon Effect and the rankings Sandbox, John Mueller answered:\n\n“In the SEO world, this is sometimes called kind of like a sandbox where Google is like keeping things back to prevent new pages from showing up, which is not the case. Or some people call it like the honeymoon period where new content comes out and Google really loves it and tries to promote it. And it’s again not the case that we’re explicitly trying to promote new content or demote new content. It’s just, we don’t know and we have to make assumptions. And then sometimes those assumptions are right and nothing really changes over time. Sometimes things settle down a little bit lower, sometimes a little bit higher.”\n\nSo, there is no systematic promotion or demotion of new content by Google, but what you might be noticing is that Google’s assumptions are based on the rest of the website’s rankings.\n\nVerdict: Officially? It’s a myth.\n\n2. Duplicate Content Penalty\n\nThis is a myth that I hear a lot. The idea is that if you have content on your website that is duplicated elsewhere on the web, Google will penalize you for it.\n\nThe key to understanding what is really going on here is knowing the difference between algorithmic suppression and manual action.\n\nA manual action, the situation that can result in webpages being removed from Google’s index, will be actioned by a human at Google.\n\nThe website owner will be notified through Google Search Console.\n\nAn algorithmic suppression occurs when your page cannot rank well due to it being caught by a filter from an algorithm.\n\nEssentially, having copy that is taken from another webpage might mean you can’t outrank that other page.\n\nThe search engines may determine that the original host of the copy is more relevant to the search query than yours.\n\nAs there is no benefit to having both in the search results, yours gets suppressed. This is not a penalty. This is the algorithm doing its job.\n\nThere are some content-related manual actions, but essentially, copying one or two pages of someone else’s content is not going to trigger them.\n\nIt is, however, potentially going to land you in other trouble if you have no legal right to use that content. It also can detract from the value your website brings to the user.\n\nWhat about content that is duplicated across your own site? Mueller clarifies that duplicate content is not a negative ranking factor. If there are multiple pages with the same content, Google may choose one to be the canonical page, and the others will not be ranked.\n\nVerdict: SEO myth.\n\n3. PPC Advertising Helps Rankings\n\nThis is a common myth. It’s also quite quick to debunk.\n\nThe idea is that Google will favor websites that spend money with it through pay-per-click advertising. This is simply false.\n\nGoogle’s algorithm for ranking organic search results is completely separate from the one used to determine PPC ad placements.\n\nRunning a paid search advertising campaign through Google while carrying out SEO might benefit your site for other reasons, but it won’t directly benefit your ranking.\n\nVerdict: SEO myth.\n\n4. Domain Age Is A Ranking Factor\n\nThis claim is seated firmly in the “confusing causation and correlation” camp.\n\nBecause a website has been around for a long time and is ranking well, age must be a ranking factor.\n\nGoogle has debunked this myth itself many times.\n\nIn July 2019, Mueller replied to a post on Twitter.com (recovered through Wayback Machine) that suggested that domain age was one of “200 signals of ranking” saying, “No, domain age helps nothing.”\n\nThe truth behind this myth is that an older website has had more time to do things well.\n\nFor instance, a website that has been live and active for 10 years may well have acquired a high volume of relevant backlinks to its key pages.\n\nA website that has been running for less than six months will be unlikely to compete with that.\n\nThe older website appears to be ranking better, and the conclusion is that age must be the determining factor.\n\nVerdict: SEO myth\n\n5. Tabbed Content Affects Rankings\n\nThis idea is one that has roots going back a long way.\n\nThe premise is that Google will not assign as much value to the content sitting behind a tab or accordion.\n\nFor example, text that is not viewable on the first load of a page.\n\nGoogle again debunked this myth in March 2020, but it has been a contentious idea among many SEO professionals for years.\n\nIn September 2018, Gary Illyes, Webmaster Trends Analyst at Google, answered a tweet thread about using tabs to display content.\n\nHis response:\n\n“AFAIK, nothing’s changed here, Bill: we index the content, and its weight is fully considered for ranking, but it might not get bolded in the snippets. It’s another, more technical question of how that content is surfaced by the site. Indexing does have limitations.”\n\nIf the content is visible in the HTML, there is no reason to assume that it is being devalued just because it is not apparent to the user on the first load of the page. This is not an example of cloaking, and Google can easily fetch the content.\n\nAs long as there is nothing else that is stopping the text from being viewed by Google, it should be weighted the same as copy, which isn’t in tabs.\n\nWant more clarification on this? Then check out this SEJ article that discusses this subject in detail.\n\nVerdict: SEO myth.\n\n6. Google Uses Google Analytics Data In Rankings\n\nThis is a common fear among business owners.\n\nThey study their Google Analytics reports. They feel their average sitewide bounce rate is too high, or their time on page is too low.\n\nSo, they worry that Google will perceive their site to be low quality because of that. They fear they won’t rank well because of it.\n\nThe myth is that Google uses the data in your Google Analytics account as part of its ranking algorithm.\n\nIt’s a myth that has been around for a long time.\n\nIllyes has again debunked this idea simply with, “We don’t use *anything* from Google analytics [sic] in the “algo.”\n\nMore recently, John Mueller dispelled this idea yet again, saying, “That’s not going to happen” when he received the suggestion telling SEO pros that GA4 is a ranking factor would improve its uptake.\n\nIf we think about this logically, using Google Analytics data as a ranking factor would be really hard to police.\n\nFor instance, using filters could manipulate data to make it seem like the site was performing in a way that it isn’t really.\n\nWhat is good performance anyway?\n\nHigh “time on page” might be good for some long-form content.\n\nLow “time on page” could be understandable for shorter content.\n\nIs either one right or wrong?\n\nGoogle would also need to understand the intricate ways in which each Google Analytics account had been configured.\n\nSome might be excluding all known bots, and others might not. Some might use custom dimensions and channel groupings, and others haven’t configured anything.\n\nUsing this data reliably would be extremely complicated to do. Consider the hundreds of thousands of websites that use other analytics programs.\n\nHow would Google treat them?\n\nVerdict: SEO myth.\n\nThis myth is another case of “causation, not correlation.”\n\nA high sitewide bounce rate might be indicative of a quality problem, or it might not be. Low time on page could mean your site isn’t engaging, or it could mean your content is quickly digestible.\n\nThese metrics give you clues as to why you might not be ranking well, they aren’t the cause of it.\n\n7. Google Cares About Domain Authority\n\nPageRank is a link analysis algorithm used by Google to measure the importance of a webpage.\n\nGoogle used to display a page’s PageRank score a number up to 10 on its toolbar. It stopped updating the PageRank displayed in toolbars in 2013.\n\nIn 2016, Google confirmed that the PageRank toolbar metric was not going to be used going forward.\n\nIn the absence of PageRank, many other third-party authority scores have been developed.\n\nCommonly known ones are:\n\nMoz’s Domain Authority and Page Authority scores.\n\nMajestic’s Trust Flow and Citation Flow.\n\nAhrefs’ Domain Rating and URL Rating.\n\nSome SEO pros use these scores to determine the “value” of a page.\n\nThat calculation can never be an entirely accurate reflection of how a search engine values a page, however.\n\nSEO pros will sometimes refer to the ranking power of a website often in conjunction with its backlink profile and this, too, is known as the domain’s authority.\n\nYou can see where the confusion lies.\n\nGoogle representatives have dispelled the notion of a domain authority metric used by them.\n\nJohn Mueller said in 2022:\n\n“We don’t use domain authority. We generally try to have our metrics as granular as possible, sometimes that’s not so easy, in which case we look at things a bit broader (e.g., we’ve talked about this in regards to some of the older quality updates).”\n\nVerdict: SEO myth.\n\n8. Longer Content Is Better\n\nYou will have definitely heard it said before that longer content ranks better.\n\nMore words on a page automatically make yours more rank-worthy than your competitor’s. This is “wisdom” that is often shared around SEO forums without little evidence to substantiate it.\n\nThere are a lot of studies that have been released over the years that state facts about the top-ranking webpages, such as “on average pages in the top 10 positions in the SERPs have over 1,450 words on them.”\n\nIt would be quite easy for someone to take this information in isolation and assume it means that pages need approximately 1,500 words to rank on Page 1. That isn’t what the study is saying, however.\n\nUnfortunately, this is an example of correlation, not necessarily causation.\n\nJust because the top-ranking pages in a particular study happened to have more words on them than the pages ranking 11th and lower does not make word count a ranking factor.\n\nMueller dispelled this myth yet again in a Google SEO Office Hours in February 2021.\n\n“From our point of view the number of words on a page is not a quality factor, not a ranking factor.”\n\nFor more information on how content length can impact SEO, check out Sam Hollingsworth’s article.\n\nVerdict: SEO myth.\n\n9. LSI Keywords Will Help You Rank\n\nWhat exactly are LSI keywords? LSI stands for “latent semantic indexing.”\n\nIt is a technique used in information retrieval that allows concepts within the text to be analyzed and relationships between them identified.\n\nWords have nuances dependent on their context. The word “right” has a different connotation when paired with “left” than when it is paired with “wrong.”\n\nHumans can quickly gauge concepts in a text. It is harder for machines to do so.\n\nThe ability of machines to understand the context and linking between entities is fundamental to their understanding of concepts.\n\nLSI is a huge step forward for a machine’s ability to understand text. What it isn’t is synonyms.\n\nUnfortunately, the field of LSI has been devolved by the SEO community into the understanding that using words that are similar or linked thematically will boost rankings for words that aren’t expressly mentioned in the text.\n\nIt’s simply not true. Google has gone far beyond LSI in its understanding of text with the introduction of BERT, as just one example.\n\nFor more about what LSI is and how it does or doesn’t affect rankings, take a look at this article.\n\nVerdict: SEO myth.\n\n10. SEO Takes 3 Months\n\nIt helps us get out of sticky conversations with our bosses or clients. It leaves a lot of wiggle room if you aren’t getting the results you promised. “SEO takes at least three months to have an effect.”\n\nIt is fair to say that there are some changes that will take time for the search engine bots to process.\n\nThere is then, of course, some time to see if those changes are having a positive or negative effect. Then more time might be needed to refine and tweak your work.\n\nThat doesn’t mean that any activity you carry out in the name of SEO is going to have no effect for three months. Day 90 of your work will not be when the ranking changes kick in. There is a lot more to it than that.\n\nIf you are in a very low-competition market, targeting niche terms, you might see ranking changes as soon as Google recrawls your page. A competitive term could take much longer to see changes in rank.\n\nA study by Semrush suggested that of the 28,000 domains they analyzed, only 19% of domains started ranking in the top 10 positions within six months and managed to maintain those rankings for the rest of the 13-month study.\n\nThis study indicates that newer pages struggle to rank high.\n\nHowever, there is more to SEO than ranking in the top 10 of Google.\n\nFor instance, a well-positioned Google Business Profile listing with great reviews can pay dividends for a company. Bing, Yandex, and Baidu might make it easier for your brand to conquer the SERPs.\n\nA small tweak to a page title could see an improvement in click-through rates. That could be the same day if the search engine were to recrawl the page quickly.\n\nAlthough it can take a long time to see first page rankings in Google, it is naïve of us to reduce SEO success just down to that.\n\nTherefore, “SEO takes 3 months” simply isn’t accurate.\n\nVerdict: SEO myth.\n\n11. Bounce Rate Is A Ranking Factor\n\nBounce rate is the percentage of visits to your website that result in no interactions beyond landing on the page. It is typically measured by a website’s analytics program, such as Google Analytics.\n\nSome SEO professionals have argued that bounce rate is a ranking factor because it is a measure of quality.\n\nUnfortunately, it is not a good measure of quality.\n\nThere are many reasons why a visitor might land on a webpage and leave again without interacting further with the site. They may well have read all the information they needed on that page and left the site to call the company and book an appointment.\n\nIn that instance, the visitor bouncing has resulted in a lead for the company.\n\nAlthough a visitor leaving a page having landed on it could be an indicator of poor quality content, it isn’t always. Therefore, it wouldn’t be reliable enough for a search engine to use as a measure of quality.\n\n“Pogo-sticking,” or a visitor clicking on a search result and then returning to the SERPs, would be a more reliable indicator of the quality of the landing page.\n\nIt would suggest that the content of the page was not what the user was after, so much so that they have returned to the search results to find another page or re-search.\n\nJohn Mueller cleared this up (again) during Google Webmaster Central Office Hours in June 2020. He was asked if sending users to a login page would appear to be a “bounce” to Google and damage their rankings:\n\n“So, I think there is a bit of misconception here, that we’re looking at things like the analytics bounce rate when it comes to ranking websites, and that’s definitely not the case.”\n\nBack on another Google Webmaster Central Office Hours in July 2018, he also said:\n\n“We try not to use signals like that when it comes to search. So that’s something where there are lots of reasons why users might go back and forth, or look at different things in the search results, or stay just briefly on a page and move back again. I think that’s really hard to refine and say, “well, we could turn this into a ranking factor.”\n\nSo, why does this keep coming up? Well, for a lot of people, it’s because of this one paragraph in Google’s How Search Works:\n\n“Beyond looking at keywords, our systems also analyze if content is relevant to a query in other ways. We also use aggregated and anonymised interaction data to assess whether Search results are relevant to queries.”\n\nThe issue with this is that Google doesn’t specify what this “aggregated and anonymised interaction data” is. This has led to a lot of speculation and of course, arguments.\n\nMy opinion? Until we have some more conclusive studies, or hear something else from Google, we need to keep testing to determine what this interaction data is.\n\nFor now, regarding the traditional definition of a bounce, I’m leaning towards “myth.”\n\nIn itself, bounce rate (measured through the likes of Google Analytics) is a very noisy, easily manipulated figure. Could something akin to a bounce be a ranking signal? Absolutely, but it will need to be a reliable, repeatable data point that genuinely measures quality.\n\nIn the meantime, if your pages are not satisfying user intent, that is definitely something you need to work on – not simply because of bounce rate.\n\nFundamentally, your pages should encourage users to interact, or if not that sort of page, at least leave your site with a positive brand association.\n\nVerdict: SEO myth.\n\n12. It’s All About Backlinks\n\nBacklinks are important – that’s without much contention within the SEO community. However, exactly how important is still debated.\n\nSome SEO pros will tell you that backlinks are one of the many tactics that will influence rankings, but they are not the most important. Others will tell you it’s the only real game-changer.\n\nWhat we do know is that the effectiveness of links has changed over time. Back in the wild pre-Jagger days, link-building consisted of adding a link to your website wherever you could.\n\nForum comments had spun articles, and irrelevant directories were all good sources of links.\n\nIt was easy to build effective links. It’s not so easy now.\n\nGoogle has continued to make changes to its algorithms that reward higher-quality, more relevant links and disregard or penalize “spammy” links.\n\nHowever, the power of links to affect rankings is still great.\n\nThere will be some industries that are so immature in SEO that a site can rank well without investing in link-building, purely through the strength of their content and technical efficiency.\n\nThat’s not the case with most industries.\n\nRelevant backlinks will, of course, help with ranking, but they need to go hand-in-hand with other optimizations. Your website still needs to have relevant content, and it must be crawlable.\n\nIf you want your traffic to actually do something when they hit your website, it’s definitely not all about backlinks.\n\nRanking is only one part of getting converting visitors to your site. The content and usability of the site are extremely important in user engagement.\n\nFollowing the slew of Helpful Content updates and a better understanding of what Google considers E-E-A-T, we know that content quality is extremely important.\n\nBacklinks can definitely help to indicate that a page would be useful to a reader, but there are many other factors that would suggest that, too.\n\nVerdict: SEO myth.\n\n13. Keywords In URLs Are Very Important\n\nCram your URLs full of keywords. It’ll help.\n\nUnfortunately, it’s not quite as powerful as that.\n\nJohn Mueller has said several times that keywords in a URL are a very minor, lightweight ranking signal.\n\nIn a Google SEO Office Hours in 2021, he affirmed again:\n\n“We use the words in a URL as a very, very lightweight factor. And from what I recall, this is primarily something that we would take into account when we haven’t had access to the content yet. So, if this is the absolute first time we see this URL and we don’t know how to classify its content, then we might use the words in the URL as something to help rank us better. But as soon as we’ve crawled and indexed the content there, then we have a lot more information.”\n\nIf you are looking to rewrite your URLs to include more keywords, you are likely to do more damage than good.\n\nThe process of redirecting URLs en masse should be when necessary, as there is always a risk when restructuring a site.\n\nFor the sake of adding keywords to a URL? Not worth it.\n\nVerdict: SEO myth.\n\n14. Website Migrations Are All About Redirects\n\nSEO professionals hear this too often. If you are migrating a website, all you need to do is remember to redirect any URLs that are changing.\n\nIf only this one were true.\n\nIn actuality, website migration is one of the most fraught and complicated procedures in SEO.\n\nA website changing its layout, content management system (CMS), domain, and/or content can all be considered a website migration.\n\nIn each of those examples, there are several aspects that could affect how the search engines perceive the quality and relevance of the pages to their targeted keywords.\n\nAs a result, there are numerous checks and configurations that need to occur if the site is to maintain its rankings and organic traffic – ensuring tracking hasn’t been lost, maintaining the same content targeting, and making sure the search engine bots can still access the right pages.\n\nAll of this needs to be considered when a website is significantly changing.\n\nRedirecting URLs that are changing is a very important part of website migration. It is in no way the only thing to be concerned about.\n\nVerdict: SEO myth.\n\n15. Well-Known Websites Will Always Outrank Unknown Websites\n\nIt stands to reason that a larger brand will have resources that smaller brands do not. As a result, more can be invested in SEO.\n\nMore exciting content pieces can be created, leading to a higher volume of backlinks acquired. The brand name alone can lend more credence to outreach attempts.\n\nThe real question is, does Google algorithmically or manually boost big brands because of their fame?\n\nThis one is a bit contentious.\n\nSome people say that Google favors big brands. Google says otherwise.\n\nIn 2009, Google released an algorithm update named “Vince.” This update had a huge impact on how brands were treated in the SERPs.\n\nBrands that were well-known offline saw ranking increases for broad competitive keywords. It stands to reason that brand awareness can help with discovery through Search.\n\nIt’s not necessarily time for smaller brands to throw in the towel.\n\nThe Vince update falls very much in line with other Google moves towards valuing authority and quality.\n\nBig brands are often more authoritative on broad-level keywords than smaller contenders.\n\nHowever, small brands can still win.\n\nLong-tail keyword targeting, niche product lines, and local presence can all make smaller brands more relevant to a search result than established brands.\n\nYes, the odds are stacked in favor of big brands, but it’s not impossible to outrank them.\n\nVerdict: Not entirely truth or myth.\n\n16. Your Page Needs To Include ‘Near Me’ To Rank Well For Local SEO\n\nIt’s understandable that this myth is still prevalent.\n\nThere is still a lot of focus on keyword search volumes in the SEO industry, sometimes at the expense of considering user intent and how the search engines understand it.\n\nWhen a searcher is looking for something with local intent, i.e., a place or service relevant to a physical location, the search engines will take this into consideration when returning results.\n\nWith Google, you will likely see the Google Maps results as well as the standard organic listings.\n\nThe Maps results are clearly centered around the location searched. However, so are the standard organic listings when the search query denotes local intent.\n\nSo, why do “near me” searches confuse some?\n\nA typical keyword research exercise might yield something like the following:\n\n“pizza restaurant manhattan” – 110 searches per month.\n\n“pizza restaurants in manhattan” – 110 searches per month.\n\n“best pizza restaurant manhattan” – 90 searches per month.\n\n“best pizza restaurants in manhattan” – 90 searches per month.\n\n“best pizza restaurant in manhattan”– 90 searches per month.\n\n“pizza restaurants near me” – 90,500 searches per month.\n\nWith search volume like that, you would think [pizza restaurants near me] would be the one to rank for, right?\n\nIt is likely, however, that people searching for [pizza restaurant manhattan] are in the Manhattan area or planning to travel there for pizza.\n\n[pizza restaurant near me] has 90,500 searches across the USA. The likelihood is that the vast majority of those searchers are not looking for Manhattan pizzas.\n\nGoogle knows this and, therefore, will serve pizza restaurant results relevant to the searcher’s location.\n\nTherefore, the “near me” element of the search becomes less about the keyword and more about the intent behind the keyword. Google will just consider it to be the location the searcher is in.\n\nSo, do you need to include “near me” in your content to rank for those [near me] searches?\n\nNo, you need to be relevant to the location the searcher is in.\n\nVerdict: SEO myth.\n\n17. Better Content Equals Better Rankings\n\nIt’s prevalent in SEO forums and X (formally Twitter) threads. The common complaint is, “My competitor is ranking above me, but I have amazing content, and theirs is terrible.”\n\nThe cry is one of indignation. After all, shouldn’t search engines reward sites for their “amazing” content?\n\nThis is both a myth and sometimes a delusion.\n\nThe quality of content is a subjective consideration. If it is your own content, it’s harder still to be objective.\n\nPerhaps in Google’s eyes, your content isn’t better than your competitors’ for the search terms you are looking to rank for.\n\nPerhaps you don’t meet searcher intent as well as they do. Maybe you have “over-optimized” your content and reduced its quality.\n\nIn some instances, better content will equal better rankings. In others, the technical performance of the site or its lack of local relevance may cause it to rank lower.\n\nContent is one factor within the ranking algorithms.\n\nVerdict: SEO myth.\n\n18. You Need To Blog Every Day\n\nThis is a frustrating myth because it seems to have spread outside of the SEO industry.\n\nGoogle loves frequent content. You should add new content or tweak existing content daily for “freshness.”\n\nWhere did this idea come from?\n\nGoogle had an algorithm update in 2011 that rewards fresher results in the SERPs.\n\nThis is because, for some queries, the fresher the results, the better the likelihood of accuracy.\n\nFor instance, if you search for [royal baby] in the UK in 2013, you will be served with news articles about Prince George. Search it again in 2015, and you will see pages about Princess Charlotte.\n\nIn 2018, you would see reports about Prince Louis at the top of the Google SERPs, and in 2019 it would be baby Archie.\n\nIf you were to search [royal baby] in 2021, shortly after the birth of Lilibet, then seeing news articles on Prince George would likely be unhelpful.\n\nIn this instance, Google discerns the user’s search intent and decides showing articles related to the newest UK royal baby would be better than showing an article that is arguably more rank-worthy due to authority, etc.\n\nWhat this algorithm update doesn’t mean is that newer content will always outrank older content. Google decides if the “query deserves freshness” or not.\n\nIf it does, then the age of content becomes a more important ranking factor.\n\nThis means that if you are creating content purely to make sure it is newer than competitors’ content, you are not necessarily going to benefit.\n\nIf the query you are looking to rank for does not deserve freshness, i.e., [who is Prince William’s third child?] a fact that will not change, then the age of content will not play a significant part in rankings.\n\nIf you are writing content every day thinking it is keeping your website fresh and, therefore, more rank-worthy, then you are likely wasting time.\n\nIt would be better to write well-considered, researched, and useful content pieces less frequently and reserve your resources to make those highly authoritative and shareable.\n\nVerdict: SEO myth.\n\n19. You Can Optimize Copy Once & Then It’s Done\n\nThe phrase “SEO optimized” copy is a common one in agency-land.\n\nIt’s used as a way to explain the process of creating copy that will be relevant to frequently searched queries.\n\nThe trouble with this is that it suggests that once you have written that copy – and ensured it adequately answers searchers’ queries – you can move on.\n\nUnfortunately, over time, how searchers look for content might change. The keywords they use, the type of content they want could alter.\n\nThe search engines, too, may change what they feel is the most relevant answer to the query. Perhaps the intent behind the keyword is perceived differently.\n\nThe layout of the SERPs might alter, meaning videos are being shown at the top of the search results where previously it was just webpage results.\n\nIf you look at a page only once and then don’t continue to update it and evolve it with user needs, then you risk falling behind.\n\nVerdict: SEO myth.\n\n20. Google Respects The Declared Canonical URL As The Preferred Version For Search Results\n\nThis can be very frustrating. You have several pages that are near duplicates of each other. You know which one is your main page, the one you want to rank, the “canonical.” You tell Google that through the specially selected “rel=canonical” tag.\n\nYou’ve chosen it. You’ve identified it in the HTML.\n\nGoogle ignores your wishes, and another of the duplicate pages ranks in its place.\n\nThe idea that Google will take your chosen page and treat it like the canonical out of a set of duplicates isn’t a challenging one.\n\nIt makes sense that the website owner would know best which page should be the one that ranks above its cousins. However, Google will sometimes disagree.\n\nThere may be instances where another page from the set is chosen by Google as a better candidate to show in the search results.\n\nThis could be because the page receives more backlinks from external sites than your chosen page. It could be that it’s included in the sitemap or is being linked to your main navigation.\n\nEssentially, the canonical tag is a signal – one of many that will be taken into consideration when Google chooses which page from a set of duplicates should rank.\n\nIf you have conflicting signals on your site, or externally, then your chosen canonical page may be overlooked in favor of another page.\n\nWant to know if Google has selected another URL to be the canonical despite your canonical tag? In Google Search Console, in the Index Coverage report, you might see this: “Duplicate, Google chose different canonical than user.”\n\nGoogle’s support documents helpfully explain what this means:\n\n“This page is marked as canonical for a set of pages, but Google thinks another URL makes a better canonical. Google has indexed the page that we consider canonical rather than this one.”\n\nVerdict: SEO myth.\n\n21. Google Has 3 Top Ranking Factors\n\nIt’s links, content, and Rank Brain, right?\n\nThis idea that these are the three top ranking factors seems to come from a WebPromo Q&A in 2016 with Andrei Lipattsev, a search quality senior strategist at Google at the time (recovered through Wayback Machine; find this discussion at around the 30-minute mark).\n\nWhen questioned on the “other two” top ranking factors, the questioner assumed that Rank Brain was one, Lipattsev stated that links pointing to a site, and content were the other two. He does clarify by saying:\n\n“Third place is a hotly contested issue. I think… It’s a funny one. Take this with a grain of salt. […] And so I guess, if you do that, then you’ll see elements of RankBrain having been involved in here, rewriting this query, applying it like this over here… And so you’d say, ‘I see this two times as often as the other thing, and two times as often as the other thing’. So it’s somewhere in number three. It’s not like having three links is ‘X’ important, and having five keywords is ‘Y’ important, and RankBrain is some ‘Z’ factor that is also somehow important, and you multiply all of that … That’s not how this works.”\n\nHowever it started, the concept prevails. A good backlink profile, great copy, and “Rank Brain” type signals are what matter most with rankings, according to many SEO pros.\n\nWhat we have to take into consideration when reviewing this idea is John Mueller’s response to a question in a 2017 English Google Webmaster Central office-hours hangout.\n\nMueller is asked if there is a one-size-fits-all approach to the top three ranking signals in Google. His answer is a clear “No.”\n\nHe follows that statement with a discussion around the timeliness of searches and how that might require different search results to be shown.\n\nHe also mentions that depending on the context of the search, different results may need to be shown, for instance, brand or shopping.\n\nHe continues to explain that he doesn’t think that there is one set of ranking factors that can be declared the top three that apply to all search results all the time.\n\nWithin the “How Search Works” documentation it clearly states:\n\n“To give you the most useful information, Search algorithms look at many factors and signals, including the words of your query, relevance and usability of pages, expertise of sources, and your location and settings. The weight applied to each factor varies depending on the nature of your query. For example, the freshness of the content plays a bigger role in answering queries about current news topics than it does about dictionary definitions. ”\n\nVerdict: Not entirely true or myth.\n\n22. Use The Disavow File To Proactively Maintain A Site’s Link Profile\n\nTo disavow or not disavow — this question has popped up a lot over the years since Penguin 4.0.\n\nSome SEO professionals are in favor of adding any link that could be considered spammy to their site’s disavow file. Others are more confident that Google will ignore them anyway and save themselves the trouble.\n\nIt’s definitely more nuanced than that.\n\nIn a 2019 Webmaster Central Office Hours Hangout, Mueller was asked about the disavow tool and whether we should have confidence that Google is ignoring medium (but not very) spammy links.\n\nHis answer indicated that there are two instances where you might want to use a disavow file:\n\nIn cases where a manual action has been given.\n\nAnd where you might think if someone from the webspam team saw it, they would issue a manual action.\n\nYou might not want to add every spammy link to your disavow file. In practice, that could take a long time if you have a very visible site that accrues thousands of these links a month.\n\nThere will be some links that are obviously spammy, and their acquisition is not a result of activity on your part.\n\nHowever, where they are a result of some less-than-awesome link building strategies (buying links, link exchanges, etc.) you may want to proactively disavow them.\n\nRead Roger Montti’s full breakdown of the 2019 exchange with John Mueller to get a better idea of the context around this discussion.\n\nVerdict: Not a myth, but don’t waste your time unnecessarily.\n\n23. Google Values Backlinks From All High Authority Domains\n\nThe better the website authority, the bigger the impact it will have on your site’s ability to rank. You will hear that in many SEO pitches, client meetings, and training sessions.\n\nHowever, that’s not the whole story.\n\nFor one, it’s arguable whether Google has a concept of domain authority (see “Google Cares About Domain Authority” above).\n\nAnd more importantly, it is the understanding that there is a lot that goes into Google’s calculations of whether a link will impact a site’s ability to rank highly or not.\n\nRelevancy, contextual clues, no-follow link attributes. None of these should be ignored when chasing a link from a high “domain authority” website.\n\nJohn Mueller also threw a cat among the pigeons during a live Search Off the Record podcast recorded at BrightonSEO in 2022 when he said:\n\n“And to some extent, links will always be something that we care about because we have to find pages somehow. It’s like how do you find a page on the web without some reference to it?” But my guess is over time, it won’t be such a big factor as sometimes it is today. I think already, that’s something that’s been changing quite a bit.”\n\nVerdict: Myth.\n\n24. You Cannot Rank A Page Without Lightning-Fast Loading Speed\n\nThere are many reasons to make your pages fast: usability, crawlability, and conversion. Arguably, it is important for the health and performance of your website, and that should be enough to make it a priority.\n\nHowever, is it something that is absolutely key to ranking your website?\n\nAs this Google Search Central post from 2010 suggests, it was definitely something that factored into the ranking algorithms. Back when it was published, Google stated:\n\n“While site speed is a new signal, it doesn’t carry as much weight as the relevance of a page. Currently, fewer than 1% of search queries are affected by the site speed signal in our implementation and the signal for site speed only applies for visitors searching in English on Google.com at this point.”\n\nIs it still only affecting such a low percentage of visitors?\n\nIn 2021, the Google Page Experience system, which incorporates the Core Web Vitals for which speed is important, rolled out on mobile. It was followed in 2022 with a rollout of the system to desktop.\n\nThis was met with a flurry of activity from SEO pros, trying to get ready for the update.\n\nMany perceive it to be something that would make or break their site’s ranking potential. However, over time, Google representatives have downplayed the ranking effect of Core Web Vitals.\n\nMore recently, in May 2023, Google introduced Interaction to Next Paint (INP) to the Core Web Vitals to replace First Input Delay (FID).\n\nGoogle claims that INP helps to deal with some of the limitations found with FID. This change in how a page’s responsiveness is measured shows that Google still cares about accurately measuring user experience.\n\nFrom Google’s previous statements and recent focus on Core Web Vitals, we can see that load speed continues to be an important ranking factor.\n\nHowever, it will not necessarily cause your website to dramatically increase or decrease in rankings.\n\nGoogle representatives Gary Illyes, Martin Splitt, and John Mueller hypothesized in 2021 during a “Search off the Record” podcast about the weighting of speed as a ranking factor.\n\nTheir discussion drew out the thinking around page load speed as a ranking metric and how it would need to be considered a fairly lightweight signal.\n\nThey went on to talk about it being more of a tie-breaker, as you can make an empty page lightning-fast, but it will not serve much use for a searcher.\n\nJohn Mueller reinforced this in 2022 during Google SEO Office Hours when he said:\n\n“Core Web Vitals is definitely a ranking factor. We have that for mobile and desktop now. It is based on what users actually see and not kind of a theoretical test of your pages […] What you don’t tend to see is big ranking changes overall for that. But rather, you would see changes for queries where we have similar content in the search results. So if someone is searching for your company name, we would not show some random blog, just because it’s a little bit faster, instead of your homepage. We would show your homepage, even if it’s very slow. On the other hand, if someone is searching for, I don’t know, running shoes, and there are lots of people writing about running shoes, then that’s where the speed aspect does play a bit more of a role.”\n\nWith this in mind, can we consider page speed a major ranking factor?\n\nMy opinion is no, page speed is definitely one of the ways Google decides which pages should rank above others, but not a major one.\n\nVerdict: Myth.\n\n25. Crawl Budget Isn’t An Issue\n\nCrawl budget – the idea that every time Googlebot visits your website, there is a limited number of resources it will visit – isn’t a contentious issue. However, how much attention should be paid to it is.\n\nFor instance, many SEO professionals will consider crawl budget optimization a central part of any technical SEO roadmap. Others will only consider it if a site reaches a certain size or complexity.\n\nGoogle is a company with finite resources. It cannot possibly crawl every single page of every site every time its bots visit them. Therefore, some of the sites that get visited might not see all of their pages crawled every time.\n\nGoogle has helpfully created a guide for owners of large and frequently updated websites to help them understand how to enable their sites to be crawled.\n\nIn the guide, Google states:\n\n“If your site does not have a large number of pages that change rapidly, or if your pages seem to be crawled the same day that they are published, you don’t need to read this guide; merely keeping your sitemap up to date and checking your index coverage regularly is adequate.”\n\nTherefore, it would seem that Google is in favor of some sites paying attention to its advice on managing crawl budget, but doesn’t consider it necessary for all.\n\nFor some sites, particularly ones that have a complex technical setup and many hundreds of thousands of pages, managing crawl budget is important. For those with a handful of easily crawled pages, it isn’t.\n\nVerdict: SEO myth.\n\n26. There Is A Right Way To Do SEO\n\nThis is probably a myth in many industries, but it seems prevalent in SEO. There is a lot of gatekeeping in SEO social media, forums, and chats.\n\nUnfortunately, it’s not that simple.\n\nWe know some core tenets about SEO.\n\nUsually, something is stated by a search engine representative that has been dissected, tested, and ultimately declared true.\n\nThe rest is a result of personal and collective trial and error, testing, and experience.\n\nProcesses are extremely valuable within SEO business functions, but they have to evolve and be applied appropriately.\n\nDifferent websites within different industries will respond to changes in ways others would not. Altering a meta title so it is under 60 characters long might help the click-through rate for one page and not for another.\n\nUltimately, we have to hold any SEO advice we’re given lightly before deciding whether it is right for the website you are working on.\n\nVerdict: SEO myth.\n\nWhen Can Something Appear To Be A Myth\n\nSometimes an SEO technique can be written off as a myth by others purely because they have not experienced success from carrying out this activity for their own site.\n\nIt is important to remember that every website has its own industry, set of competitors, the technology powering it, and other factors that make it unique.\n\nBlanket application of techniques to every website and expecting them to have the same outcome is naive.\n\nSomeone may not have had success with a technique when they have tried it in their highly competitive vertical.\n\nIt doesn’t mean it won’t help someone in a less competitive industry have success.\n\nCausation & Correlation Being Confused\n\nSometimes, SEO myths arise because of an inappropriate connection between an activity that was carried out and a rise in organic search performance.\n\nIf an SEO has seen a benefit from something they did, then it is natural that they would advise others to try the same.\n\nUnfortunately, we’re not always great at separating causation and correlation.\n\nJust because rankings or click-through rates increased around the same time as you implemented a new tactic doesn’t mean it caused the increase. There could be other factors at play.\n\nSoon, an SEO myth will arise from an overeager SEO who wants to share what they incorrectly believe to be a golden ticket.\n\nSteering Clear Of SEO Myths\n\nIt can save you from experiencing headaches, lost revenue, and a whole lot of time if you learn to spot SEO myths and act accordingly.\n\nTest\n\nThe key to not falling for SEO myths is making sure you can test advice whenever possible.\n\nIf you have been given the advice that structuring your page titles a certain way will help your pages rank better for their chosen keywords, then try it with one or two pages first.\n\nThis can help you measure whether making a change across many pages will be worth the time before you commit to it.\n\nIs Google Just Testing?\n\nSometimes, there will be a big uproar in the SEO community because of changes in the way Google displays or orders search results.\n\nThese changes are often tested in the wild before they are rolled out to more search results.\n\nOnce a big change has been spotted by one or two SEO pros, advice on how to optimize for it begins to spread.\n\nRemember the favicons in the desktop search results? The upset that caused the SEO industry (and Google users in general) was vast.\n\nSuddenly, articles sprang up about the importance of favicons in attracting users to your search results. There was barely time to study whether favicons would impact the click-through rate that much.\n\nBecause just like that, Google changed it back.\n\nBefore you jump for the latest SEO advice being spread around Twitter as a result of a change by Google, wait to see if it will hold.\n\nIt could be that the advice that appears sound now will quickly become a myth if Google rolls back changes.\n\nMore resources:\n\nFeatured Image: Search Engine Journal/Paulo Bobita"
    }
]