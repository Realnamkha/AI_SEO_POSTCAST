[
    {
        "title": "Google Confirms Ranking Boost For Country Code Domains",
        "url": "https://www.searchenginejournal.com/google-confirms-ranking-preference/522477/",
        "content": "Google’s Gary Illyes answered a question about a ranking preference given to sites that use country level domain names and explained how that compares to non-country domain names. The question occurred in the SEO Office Hours podcast.\n\nccTLD Aka Country Code Domain Names\n\nDomain names that are specific to countries are called ccTLDs (Country Code Top Level Domains). These are domain names that target specific countries. Examples of these ccTLDs are .de (Germany), .in (India) and .kr (Korea). These kinds of domain names don’t target specific languages, they only target Internet users in a specific country.\n\nSome ccTLDs are treated by Google for ranking purposes as if they are regular Generic Top Level Domains (gTLDs), which are domains that are not specific to a country. A popular example is .io, which technically is a ccTLD (pertaining to the British Indian Ocean Territory) but because of how it’s used, Google treats it like a regular gTLD (generic top level domain).\n\nRanking Boosts For ccTLDs\n\nThe question that Gary Illyes answered was about the ranking boost given to ccTLDs.\n\nThis is the question:\n\n“When a Korean person searches Google in Korean, does a com.kr domain or a .com domain do better?”\n\nGary Illyes answered:\n\n“Good question. Generally speaking the local domain names, in your case .kr, tend to do better because Google Search promotes content local to the user.”\n\nA lot of people want to rank better in a specific country and one of the best practices for doing that is to register a domain name that is specific to the country. Google will give it a ranking boost over other sites that are not explicitly targeting a specific country.\n\nGary continued his answer by explaining the ranking boost of a ccTLD over a generic top level domain (gTLD), like .com, .net and so on.\n\nThis is Gary’s explanation:\n\n“That’s not to say that a .com domain can’t do well, it can, but generally .kr has a little more benefit, albeit not too much. “\n\nTargeting Country Versus Targeting Language\n\nLastly, Gary mentioned that targeting a user’s language has more impact than the domain name.\n\nHe continued his answer:\n\n“If the language of a site matches the user’s query language, that probably has more impact than the domain name itself.”\n\nA benefit of targeting a language is that a site is able regardless of the country that a user is searching from whereas the country code top level domain name targets a country.\n\nSomething that Gary didn’t mention is that using a ccTLD can inspire user trust from searchers whose country matches the country that the domain name is targeting and because of that searchers on Google may be more inclined to click on a search result that uses the geotargeted ccTLD.\n\nIf a user is in Korea they may feel that a .kr domain is meant specifically for them. If a searcher is in Australia they may feel more inclined to click on a .au domain name.\n\nListen to the podcast answer from the 3:35 minute mark:\n\nFeatured Image by Shutterstock/Dean Drobot"
    },
    {
        "title": "Google Confirms Ranking Boost For Country Code Domains",
        "url": "https://www.searchenginejournal.com/google-confirms-ranking-preference/522477/",
        "content": "Google’s Gary Illyes answered a question about a ranking preference given to sites that use country level domain names and explained how that compares to non-country domain names. The question occurred in the SEO Office Hours podcast.\n\nccTLD Aka Country Code Domain Names\n\nDomain names that are specific to countries are called ccTLDs (Country Code Top Level Domains). These are domain names that target specific countries. Examples of these ccTLDs are .de (Germany), .in (India) and .kr (Korea). These kinds of domain names don’t target specific languages, they only target Internet users in a specific country.\n\nSome ccTLDs are treated by Google for ranking purposes as if they are regular Generic Top Level Domains (gTLDs), which are domains that are not specific to a country. A popular example is .io, which technically is a ccTLD (pertaining to the British Indian Ocean Territory) but because of how it’s used, Google treats it like a regular gTLD (generic top level domain).\n\nRanking Boosts For ccTLDs\n\nThe question that Gary Illyes answered was about the ranking boost given to ccTLDs.\n\nThis is the question:\n\n“When a Korean person searches Google in Korean, does a com.kr domain or a .com domain do better?”\n\nGary Illyes answered:\n\n“Good question. Generally speaking the local domain names, in your case .kr, tend to do better because Google Search promotes content local to the user.”\n\nA lot of people want to rank better in a specific country and one of the best practices for doing that is to register a domain name that is specific to the country. Google will give it a ranking boost over other sites that are not explicitly targeting a specific country.\n\nGary continued his answer by explaining the ranking boost of a ccTLD over a generic top level domain (gTLD), like .com, .net and so on.\n\nThis is Gary’s explanation:\n\n“That’s not to say that a .com domain can’t do well, it can, but generally .kr has a little more benefit, albeit not too much. “\n\nTargeting Country Versus Targeting Language\n\nLastly, Gary mentioned that targeting a user’s language has more impact than the domain name.\n\nHe continued his answer:\n\n“If the language of a site matches the user’s query language, that probably has more impact than the domain name itself.”\n\nA benefit of targeting a language is that a site is able regardless of the country that a user is searching from whereas the country code top level domain name targets a country.\n\nSomething that Gary didn’t mention is that using a ccTLD can inspire user trust from searchers whose country matches the country that the domain name is targeting and because of that searchers on Google may be more inclined to click on a search result that uses the geotargeted ccTLD.\n\nIf a user is in Korea they may feel that a .kr domain is meant specifically for them. If a searcher is in Australia they may feel more inclined to click on a .au domain name.\n\nListen to the podcast answer from the 3:35 minute mark:\n\nFeatured Image by Shutterstock/Dean Drobot"
    },
    {
        "title": "Google Clarifies H1-H6 Headings For SEO",
        "url": "https://www.searchenginejournal.com/google-clarifies-h1-h6-headings-for-seo/522454/",
        "content": "Google’s Gary Illyes answered a question about the SEO value of hierarchically ordering heading elements (H1, H2, etc.). His answer offered an insight into the actual value of heading elements for digital marketing.\n\nHeading Elements\n\nIn simple terms, HTML Elements are the building blocks of a web page and they all have their place much like the foundation and a roof of a home have their places in the overall structure.\n\nHeading elements communicate the topic and subtopics of a web page and are literally a list of topics when a page is viewed just by their headings.\n\nThe World Wide Web Consortium (W3C), which defines HTML, describes headings like this:\n\n“HTML defines six levels of headings. A heading element implies all the font changes, paragraph breaks before and after, and any white space necessary to render the heading. The heading elements are H1, H2, H3, H4, H5, and H6 with H1 being the highest (or most important) level and H6 the least. Headers play a related role to lists in structuring documents, and it is common to number headers or to include a graphic that acts like a bullet in lists.”\n\nStrictly speaking, it is absolutely correct to order headings according to their hierarchical structure.\n\nWhat Google Says About Headings\n\nThe person asking the question commented that the SEO Starter Guide recommends using heading elements in “semantic” order for people who use screen readers (devices that translate text into spoken words) but that otherwise it’s not important for Google. The person asking the question wanted to know if the SEO Starter Guide was out of date because an SEO tool had a different recommendation.\n\nGary narrated the submitted question:\n\n“I recently read on the SEO starter guide that “Having headings in semantic order is fantastic for screen readers, but from Google Search perspective, it doesn’t matter if you’re using them out of order.” Is this correct because an SEO tool told me otherwise.”\n\nIt’s a good question because it makes sense to use heading elements in a way that shows the hierarchical importance of different sections of a web page, right?\n\nHere’s Gary’s response:\n\n“We update our documentation quite frequently to ensure that it’s always up to date. In fact the SEO starter guide was refreshed just a couple months back to ensure it’s still relevant, so what you read in the guide is as accurate as it can get. Also, just because a non-Google tool tells you something is good or bad, that doesn’t make it relevant for Google; it may still be a good idea, just not necessarily relevant to Google.”\n\nSee also:\n\nIs It Relevant For Google?\n\nThe official HTML standards are flexible about the use of headings.\n\nHere’s what the standards say here:\n\n“A heading element briefly describes the topic of the section it introduces. Heading information may be used by user agents, for example, to construct a table of contents for a document automatically.”\n\nAnd here:\n\n“The heading elements are H1, H2, H3, H4, H5, and H6 with H1 being the highest (or most important) level and H6 the least.”\n\nThe official HTML5 specifications for headings state that the hierarchical ordering is implied but that in both cases the headings communicate the start of a new section within a web page. Also, while the official standards encourage “nesting” headings for subtopics but that’s a “strong” encouragement and not a rigid rule.\n\n“The first element of heading content in an element of sectioning content represents the heading for that section. Subsequent headings of equal or higher rank start new (implied) sections, headings of lower rank start implied subsections that are part of the previous one. In both cases, the element represents the heading of the implied section. Sections may contain headings of any rank, but authors are strongly encouraged to either use only h1 elements, or to use elements of the appropriate rank for the section’s nesting level.”\n\nThat last part of the official standards is quite explicit that users are “encouraged” to only use H1 elements, which might sound crazy to some people, but that’s the reality. Still, that’s just an encouragement, not a rigid rule.\n\nIt’s only in the official HTML standards for heading elements in the context of accessibility that the recommendations are more rigid about using heading elements with a hierarchical structure (important to least important).\n\nSo as you can see, Google’s usage of heading elements appear to be in line with the official standards because the standards allow for deviation, except for accessibility reasons.\n\nThe SEO tool is correct that the proper use of heading elements is to put them into hierarchical order. But the tool is incorrect in saying that it’s better for SEO.\n\nThis means that H1 is the most important heading for screen readers but it’s not the most important for Google. When I was doing SEO in 2001, the H1 was the most important heading element. But that hasn’t been the case for decades.\n\nFor some reason, some SEO tools (and SEOs) still believe that H1 is the most important heading for Google. But that’s simply not correct.\n\nListen to the SEO Office Hours Podcast at the 13:17 minute mark:\n\nSee also:\n\nFeatured Image by Shutterstock/AlenD"
    },
    {
        "title": "Google Clarifies H1-H6 Headings For SEO",
        "url": "https://www.searchenginejournal.com/google-clarifies-h1-h6-headings-for-seo/522454/",
        "content": "Google’s Gary Illyes answered a question about the SEO value of hierarchically ordering heading elements (H1, H2, etc.). His answer offered an insight into the actual value of heading elements for digital marketing.\n\nHeading Elements\n\nIn simple terms, HTML Elements are the building blocks of a web page and they all have their place much like the foundation and a roof of a home have their places in the overall structure.\n\nHeading elements communicate the topic and subtopics of a web page and are literally a list of topics when a page is viewed just by their headings.\n\nThe World Wide Web Consortium (W3C), which defines HTML, describes headings like this:\n\n“HTML defines six levels of headings. A heading element implies all the font changes, paragraph breaks before and after, and any white space necessary to render the heading. The heading elements are H1, H2, H3, H4, H5, and H6 with H1 being the highest (or most important) level and H6 the least. Headers play a related role to lists in structuring documents, and it is common to number headers or to include a graphic that acts like a bullet in lists.”\n\nStrictly speaking, it is absolutely correct to order headings according to their hierarchical structure.\n\nWhat Google Says About Headings\n\nThe person asking the question commented that the SEO Starter Guide recommends using heading elements in “semantic” order for people who use screen readers (devices that translate text into spoken words) but that otherwise it’s not important for Google. The person asking the question wanted to know if the SEO Starter Guide was out of date because an SEO tool had a different recommendation.\n\nGary narrated the submitted question:\n\n“I recently read on the SEO starter guide that “Having headings in semantic order is fantastic for screen readers, but from Google Search perspective, it doesn’t matter if you’re using them out of order.” Is this correct because an SEO tool told me otherwise.”\n\nIt’s a good question because it makes sense to use heading elements in a way that shows the hierarchical importance of different sections of a web page, right?\n\nHere’s Gary’s response:\n\n“We update our documentation quite frequently to ensure that it’s always up to date. In fact the SEO starter guide was refreshed just a couple months back to ensure it’s still relevant, so what you read in the guide is as accurate as it can get. Also, just because a non-Google tool tells you something is good or bad, that doesn’t make it relevant for Google; it may still be a good idea, just not necessarily relevant to Google.”\n\nSee also:\n\nIs It Relevant For Google?\n\nThe official HTML standards are flexible about the use of headings.\n\nHere’s what the standards say here:\n\n“A heading element briefly describes the topic of the section it introduces. Heading information may be used by user agents, for example, to construct a table of contents for a document automatically.”\n\nAnd here:\n\n“The heading elements are H1, H2, H3, H4, H5, and H6 with H1 being the highest (or most important) level and H6 the least.”\n\nThe official HTML5 specifications for headings state that the hierarchical ordering is implied but that in both cases the headings communicate the start of a new section within a web page. Also, while the official standards encourage “nesting” headings for subtopics but that’s a “strong” encouragement and not a rigid rule.\n\n“The first element of heading content in an element of sectioning content represents the heading for that section. Subsequent headings of equal or higher rank start new (implied) sections, headings of lower rank start implied subsections that are part of the previous one. In both cases, the element represents the heading of the implied section. Sections may contain headings of any rank, but authors are strongly encouraged to either use only h1 elements, or to use elements of the appropriate rank for the section’s nesting level.”\n\nThat last part of the official standards is quite explicit that users are “encouraged” to only use H1 elements, which might sound crazy to some people, but that’s the reality. Still, that’s just an encouragement, not a rigid rule.\n\nIt’s only in the official HTML standards for heading elements in the context of accessibility that the recommendations are more rigid about using heading elements with a hierarchical structure (important to least important).\n\nSo as you can see, Google’s usage of heading elements appear to be in line with the official standards because the standards allow for deviation, except for accessibility reasons.\n\nThe SEO tool is correct that the proper use of heading elements is to put them into hierarchical order. But the tool is incorrect in saying that it’s better for SEO.\n\nThis means that H1 is the most important heading for screen readers but it’s not the most important for Google. When I was doing SEO in 2001, the H1 was the most important heading element. But that hasn’t been the case for decades.\n\nFor some reason, some SEO tools (and SEOs) still believe that H1 is the most important heading for Google. But that’s simply not correct.\n\nListen to the SEO Office Hours Podcast at the 13:17 minute mark:\n\nSee also:\n\nFeatured Image by Shutterstock/AlenD"
    },
    {
        "title": "Google’s John Mueller On How To Verify An SEO Agency’s Work",
        "url": "https://www.searchenginejournal.com/googles-john-mueller-on-how-to-verify-an-seo-agencys-work/522449/",
        "content": "In a recent session of Google’s SEO office-hours Q&A, the Search Relations team addressed a common concern among business owners: how to determine if an SEO agency is actively optimizing your website.\n\nThe Business Owner’s Question\n\nThe discussion was prompted by a business owner who asked:\n\n“If I have an agency that is managing our organic SEO on a monthly basis, how can I tell if anyone has been actively optimizing? I have a suspicion that the agency has not been optimized out of site for years.”\n\nGoogle’s Response\n\nIn response, John Mueller, a Search Relations team member, shared his experience collaborating with an agency on Google’s Search Central content.\n\nKey Points from Mueller’s Advice\n\nRegular Meetings: Hold frequent discussions with the SEO agency to review their work. Progress Reports: Request reports that detail the site’s progress over time. Future Planning: Discussing upcoming work helps ensure the agency addresses your needs. Client Education: Clients should have a basic understanding of SEO work to better evaluate the agency’s efforts.\n\nWhile acknowledging that increased engagement requires additional time from both parties, Mueller believes it’s worth the effort.\n\nThis allows you to check if the SEO agency is meeting your needs. However, he notes that you need to have some trust in your relationship with the agency.\n\nResources For SEO Education\n\nTo assist businesses in managing their SEO efforts, Mueller pointed to two valuable resources:\n\nGoogle’s guide on hiring an SEO provides insights into the selection process. The SEO starter guide offers a foundational understanding of SEO principles.\n\nMueller’s Full Response\n\n“This is a great question. When we worked with an SEO agency for some of the Search Central content, we had regular meetings to discuss the work that they did, to look at reports about the site’s progress, and to discuss any upcoming work. This did require a bit more time, both from them and from us, but I found it very insightful. I think it helps to lightly understand the kind of work that an agency would do, so that you can confirm that they’re doing what you expect them to do, and even then there’s a component of trust involved. We have a page about hiring an SEO which has some insights, and there’s our SEO starter guide, which can explain a bit more. And also, perhaps some folks from the SEO industry can comment on how they’d help a client understand how they’re spending their time.”\n\nPrevious Discussions On SEO Hiring\n\nThis advice from Mueller echoes a similar discussion he initiated last year, where he sought recommendations on what businesses should look for when hiring SEO consultants.\n\nThe conversation among industry experts highlighted key factors such as experience, customization, transparency, and adherence to ethical practices.\n\nFor more insights on choosing the right SEO professional, refer to our previous coverage of that discussion.\n\nWhen To Seek Professional SEO Help\n\nFor businesses unsure about when to seek professional SEO help, here’s an article that outlines five critical situations that warrant hiring an SEO expert.\n\nSee: 5 Times You Absolutely Must Hire An SEO Pro\n\nThese include when Google isn’t indexing your site, during site migrations or redesigns, when organic traffic drops significantly, to reverse manual actions, and when current SEO strategies aren’t yielding results.\n\nThis information complements Mueller’s advice by helping businesses recognize when professional intervention is necessary.\n\nFeatured Image: YouTube.com/GoogleSearchCentral"
    },
    {
        "title": "Google’s John Mueller On How To Verify An SEO Agency’s Work",
        "url": "https://www.searchenginejournal.com/googles-john-mueller-on-how-to-verify-an-seo-agencys-work/522449/",
        "content": "In a recent session of Google’s SEO office-hours Q&A, the Search Relations team addressed a common concern among business owners: how to determine if an SEO agency is actively optimizing your website.\n\nThe Business Owner’s Question\n\nThe discussion was prompted by a business owner who asked:\n\n“If I have an agency that is managing our organic SEO on a monthly basis, how can I tell if anyone has been actively optimizing? I have a suspicion that the agency has not been optimized out of site for years.”\n\nGoogle’s Response\n\nIn response, John Mueller, a Search Relations team member, shared his experience collaborating with an agency on Google’s Search Central content.\n\nKey Points from Mueller’s Advice\n\nRegular Meetings: Hold frequent discussions with the SEO agency to review their work. Progress Reports: Request reports that detail the site’s progress over time. Future Planning: Discussing upcoming work helps ensure the agency addresses your needs. Client Education: Clients should have a basic understanding of SEO work to better evaluate the agency’s efforts.\n\nWhile acknowledging that increased engagement requires additional time from both parties, Mueller believes it’s worth the effort.\n\nThis allows you to check if the SEO agency is meeting your needs. However, he notes that you need to have some trust in your relationship with the agency.\n\nResources For SEO Education\n\nTo assist businesses in managing their SEO efforts, Mueller pointed to two valuable resources:\n\nGoogle’s guide on hiring an SEO provides insights into the selection process. The SEO starter guide offers a foundational understanding of SEO principles.\n\nMueller’s Full Response\n\n“This is a great question. When we worked with an SEO agency for some of the Search Central content, we had regular meetings to discuss the work that they did, to look at reports about the site’s progress, and to discuss any upcoming work. This did require a bit more time, both from them and from us, but I found it very insightful. I think it helps to lightly understand the kind of work that an agency would do, so that you can confirm that they’re doing what you expect them to do, and even then there’s a component of trust involved. We have a page about hiring an SEO which has some insights, and there’s our SEO starter guide, which can explain a bit more. And also, perhaps some folks from the SEO industry can comment on how they’d help a client understand how they’re spending their time.”\n\nPrevious Discussions On SEO Hiring\n\nThis advice from Mueller echoes a similar discussion he initiated last year, where he sought recommendations on what businesses should look for when hiring SEO consultants.\n\nThe conversation among industry experts highlighted key factors such as experience, customization, transparency, and adherence to ethical practices.\n\nFor more insights on choosing the right SEO professional, refer to our previous coverage of that discussion.\n\nWhen To Seek Professional SEO Help\n\nFor businesses unsure about when to seek professional SEO help, here’s an article that outlines five critical situations that warrant hiring an SEO expert.\n\nSee: 5 Times You Absolutely Must Hire An SEO Pro\n\nThese include when Google isn’t indexing your site, during site migrations or redesigns, when organic traffic drops significantly, to reverse manual actions, and when current SEO strategies aren’t yielding results.\n\nThis information complements Mueller’s advice by helping businesses recognize when professional intervention is necessary.\n\nFeatured Image: YouTube.com/GoogleSearchCentral"
    },
    {
        "title": "Research Confirms Google AIO Keyword Trends",
        "url": "https://www.searchenginejournal.com/research-confirms-google-aio-keyword-trends/522365/",
        "content": "New research by enterprise search marketing company BrightEdge reveals dramatic changes to sites surfaced through Google’s AI Overviews search feature and though it maintains search market share, the data shows that AI search engine Perplexity is gaining ground at a remarkable pace.\n\nRapid & Dramatic Changes In AIO Triggers\n\nThe words that trigger AI Overviews are changing at an incredibly rapid pace. Some keyword trends in June may already changed in July.\n\nAI Overviews were triggered 50% more times for keywords with the word “best” in them. But Google may have reversed that behavior because those phrases, when applied to products, don’t appear to be triggering AIOs in July.\n\nOther AIO triggers for June 2024:\n\n“What Is” keywords increased by 20% more times\n\n“How to” queries increased by 15%\n\nQueries with the phrase “”symptoms of” increased by about 12%\n\nQueries with the word “treatment” increased by 10%\n\nA spokesperson from BrightEdge responded to my questions about ecommerce search queries:\n\n“AI’s prevalence in ecommerce is indeed increasing, with a nearly 20% rise in ecommerce keywords showing AI overviews since the beginning of July, and a dramatic 62.6% increase compared to the last week of June. Alongside this growth, we’re seeing a significant 66.67% uptick in product searches that contain both pros and cons from the AI overview. This dual trend indicates not only more prevalent use of AI in ecommerce search results but also more comprehensive and useful information being provided to consumers through features like the pros/cons modules.”\n\nGoogle Search And AI Trends\n\nBrightEdge used its proprietary BrightEdge Generative Parser™ (BGP) tool to identify key trends in search that may influence digital marketing for the rest of 2024. BGP is a tool that collects massive amounts of search trend data and turns it into actionable insights.\n\nTheir research estimates that each percentage point of search market share represents $1.2 billion, which means that gains as small as single digits are still incredibly valuable.\n\nJim Yu, founder and executive chairman of BrightEdge noted:\n\n“There is no doubt that Google’s dominance remains strong, and what it does in AI matters to every business and marketer across the planet.\n\nAt the same time, new players are laying new foundations as we enter an AI-led multi-search universe. AI is in a constant state of progress, so the most important thing marketers can do now is leverage the precision of insights to monitor, prepare for changes, and adapt accordingly.\n\nGoogle continues to be the most dominant source of search traffic, driving approximately 92% organic search referrals. A remarkable data point from the research is that AI competitors in all forms have not yet made a significant impact as a source of traffic, completely deflating speculation that AI competitors will cut into Google’s search traffic.\n\nMassive Decrease In Reddit & Quora Referrals\n\nBack in May 2024 Google Of interest to search marketers is that Google has followed through in reducing the amount of user generated content (UGC) surfaced through its AI Overviews search feature. UGC is responsible for many of the outrageously bad responses that generated negative press. BrightEdge’s research shows that referrals to Reddit and Quora from AI Overviews declined to “near zero” in the month of June.\n\nCitations to Quora from AI Overviews are reported to have decreased by 99.69%. Reddit fared marginally etter in June with an 85.71% decrease\n\nBrightEdge’s report noted:\n\n“Google is prioritizing established, expert content over user discussions and forums.”\n\nBing, Perplexity And Chatbot Impact\n\nMarket share for Bing continues to increase but only by fractions of a percentage point, growing from 4.2% to 4.5%. But as they say, it’s better to be moving forward than standing still.\n\nPerplexity on the other hand is growing at a monthly rate of 31%. Percentages however can be misleading because 31% of a relatively small number is still a relatively small number. Most publishers aren’t talking about all the traffic they’re getting from Perplexity so they still have a way to go. Nevertheless, a monthly growth rate of 31% is movement in the right direction.\n\nTraffic from Chatbots aren’t really a thing, so this comparison should be put into that perspective. Sending referral traffic to websites isn’t really what chatbots like Claude and ChatGPT are about (at this point in time). The data shows that both Claude and ChatGPT are not sending much traffic.\n\nOpenAI however is hiding referrals from the websites that it’s sending traffic to which makes it difficult to track it. Therefore a full understanding of the impact of LLM traffic, because ChatGPT uses a rel=noreferrer HTML attribute which hides all traffic originating from ChatGPT to websites. The use of the rel=noreferrer link attribute is not unusual though because it’s an industry standard for privacy and security.\n\nBrightEdge’s analysis looks at this from a long term perspective and anticipates that referral traffic from LLMs will become more prevalent and at some point will become a significant consideration for marketers.\n\nThis is the conclusion reached by BrightEdge:\n\n“The overall number of referrals from LLMs is small and expected to have little industry impact at this time. However, if this incremental growth continues, BrightEdge predicts it will influence where people search online and how brands approach optimizing for different engines.”\n\nBefore the iPhone existed, many scoffed at the idea of the Internet on mobile devices. So BrightEdge’s conclusions about what to expect from LLMs are not unreasonable.\n\nAIO trends have already changed in July, pointing to the importance of having fresh data for adapting to fast changing AIO keyword trends. BrightEdge delivers real-time data updated on a daily basis so that marketers can make better informed decisions.\n\nUnderstand AI Overview Trends:\n\nTen Observations On AI Overviews For June 2024\n\nFeatured Image by Shutterstock/Krakenimages.com"
    },
    {
        "title": "Research Confirms Google AIO Keyword Trends",
        "url": "https://www.searchenginejournal.com/research-confirms-google-aio-keyword-trends/522365/",
        "content": "New research by enterprise search marketing company BrightEdge reveals dramatic changes to sites surfaced through Google’s AI Overviews search feature and though it maintains search market share, the data shows that AI search engine Perplexity is gaining ground at a remarkable pace.\n\nRapid & Dramatic Changes In AIO Triggers\n\nThe words that trigger AI Overviews are changing at an incredibly rapid pace. Some keyword trends in June may already changed in July.\n\nAI Overviews were triggered 50% more times for keywords with the word “best” in them. But Google may have reversed that behavior because those phrases, when applied to products, don’t appear to be triggering AIOs in July.\n\nOther AIO triggers for June 2024:\n\n“What Is” keywords increased by 20% more times\n\n“How to” queries increased by 15%\n\nQueries with the phrase “”symptoms of” increased by about 12%\n\nQueries with the word “treatment” increased by 10%\n\nA spokesperson from BrightEdge responded to my questions about ecommerce search queries:\n\n“AI’s prevalence in ecommerce is indeed increasing, with a nearly 20% rise in ecommerce keywords showing AI overviews since the beginning of July, and a dramatic 62.6% increase compared to the last week of June. Alongside this growth, we’re seeing a significant 66.67% uptick in product searches that contain both pros and cons from the AI overview. This dual trend indicates not only more prevalent use of AI in ecommerce search results but also more comprehensive and useful information being provided to consumers through features like the pros/cons modules.”\n\nGoogle Search And AI Trends\n\nBrightEdge used its proprietary BrightEdge Generative Parser™ (BGP) tool to identify key trends in search that may influence digital marketing for the rest of 2024. BGP is a tool that collects massive amounts of search trend data and turns it into actionable insights.\n\nTheir research estimates that each percentage point of search market share represents $1.2 billion, which means that gains as small as single digits are still incredibly valuable.\n\nJim Yu, founder and executive chairman of BrightEdge noted:\n\n“There is no doubt that Google’s dominance remains strong, and what it does in AI matters to every business and marketer across the planet.\n\nAt the same time, new players are laying new foundations as we enter an AI-led multi-search universe. AI is in a constant state of progress, so the most important thing marketers can do now is leverage the precision of insights to monitor, prepare for changes, and adapt accordingly.\n\nGoogle continues to be the most dominant source of search traffic, driving approximately 92% organic search referrals. A remarkable data point from the research is that AI competitors in all forms have not yet made a significant impact as a source of traffic, completely deflating speculation that AI competitors will cut into Google’s search traffic.\n\nMassive Decrease In Reddit & Quora Referrals\n\nBack in May 2024 Google Of interest to search marketers is that Google has followed through in reducing the amount of user generated content (UGC) surfaced through its AI Overviews search feature. UGC is responsible for many of the outrageously bad responses that generated negative press. BrightEdge’s research shows that referrals to Reddit and Quora from AI Overviews declined to “near zero” in the month of June.\n\nCitations to Quora from AI Overviews are reported to have decreased by 99.69%. Reddit fared marginally etter in June with an 85.71% decrease\n\nBrightEdge’s report noted:\n\n“Google is prioritizing established, expert content over user discussions and forums.”\n\nBing, Perplexity And Chatbot Impact\n\nMarket share for Bing continues to increase but only by fractions of a percentage point, growing from 4.2% to 4.5%. But as they say, it’s better to be moving forward than standing still.\n\nPerplexity on the other hand is growing at a monthly rate of 31%. Percentages however can be misleading because 31% of a relatively small number is still a relatively small number. Most publishers aren’t talking about all the traffic they’re getting from Perplexity so they still have a way to go. Nevertheless, a monthly growth rate of 31% is movement in the right direction.\n\nTraffic from Chatbots aren’t really a thing, so this comparison should be put into that perspective. Sending referral traffic to websites isn’t really what chatbots like Claude and ChatGPT are about (at this point in time). The data shows that both Claude and ChatGPT are not sending much traffic.\n\nOpenAI however is hiding referrals from the websites that it’s sending traffic to which makes it difficult to track it. Therefore a full understanding of the impact of LLM traffic, because ChatGPT uses a rel=noreferrer HTML attribute which hides all traffic originating from ChatGPT to websites. The use of the rel=noreferrer link attribute is not unusual though because it’s an industry standard for privacy and security.\n\nBrightEdge’s analysis looks at this from a long term perspective and anticipates that referral traffic from LLMs will become more prevalent and at some point will become a significant consideration for marketers.\n\nThis is the conclusion reached by BrightEdge:\n\n“The overall number of referrals from LLMs is small and expected to have little industry impact at this time. However, if this incremental growth continues, BrightEdge predicts it will influence where people search online and how brands approach optimizing for different engines.”\n\nBefore the iPhone existed, many scoffed at the idea of the Internet on mobile devices. So BrightEdge’s conclusions about what to expect from LLMs are not unreasonable.\n\nAIO trends have already changed in July, pointing to the importance of having fresh data for adapting to fast changing AIO keyword trends. BrightEdge delivers real-time data updated on a daily basis so that marketers can make better informed decisions.\n\nUnderstand AI Overview Trends:\n\nTen Observations On AI Overviews For June 2024\n\nFeatured Image by Shutterstock/Krakenimages.com"
    },
    {
        "title": "Anthropic Announces Free Claude AI Chatbot For Android",
        "url": "https://www.searchenginejournal.com/anthropic-claude-for-android/522330/",
        "content": "Anthropic announced the release of a new Claude Android app that uses their powerful Claude 3.5 Sonnet language model. The app is available free (with usage limits) and also with paid plans.\n\nAnthropic Claude\n\nClaude is a powerful AI chatbot that offers advanced reasoning, can do real-time image analysis, and can translate languages in real-time. Claude 3.5 Sonnet is Anthropic’s most advanced language model, introduced in late June 2024.\n\nAccording to Anthropic:\n\n“Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet. Claude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval). It shows marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone.”\n\nClaude By Anthropic Android App\n\nThe Claude AI chatbot app is currently available for iOS and now it’s available from the Google Play store for Android users. Downloading and signing up is easy. Once signed in and verified users can start using Claude absolutely free. I downloaded it and gave it a try and was pleasantly surprised at its ability to help create a ramen recipe from scratch. A cool feature of the app is that it can continue chats from other devices.\n\nThe official announcement described various ways it’s useful:\n\n“Use Claude for work or for fun. Whether you’re drafting a business proposal between meetings, translating menus while traveling, brainstorming gift ideas while shopping, or composing a speech while waiting for a flight, Claude is ready to assist you.”\n\nDownload the Claude by Anthropic Android App from Google Play:\n\nClaude by Anthropic\n\nRead the official announcement:\n\nClaude Android app"
    },
    {
        "title": "Anthropic Announces Free Claude AI Chatbot For Android",
        "url": "https://www.searchenginejournal.com/anthropic-claude-for-android/522330/",
        "content": "Anthropic announced the release of a new Claude Android app that uses their powerful Claude 3.5 Sonnet language model. The app is available free (with usage limits) and also with paid plans.\n\nAnthropic Claude\n\nClaude is a powerful AI chatbot that offers advanced reasoning, can do real-time image analysis, and can translate languages in real-time. Claude 3.5 Sonnet is Anthropic’s most advanced language model, introduced in late June 2024.\n\nAccording to Anthropic:\n\n“Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet. Claude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval). It shows marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone.”\n\nClaude By Anthropic Android App\n\nThe Claude AI chatbot app is currently available for iOS and now it’s available from the Google Play store for Android users. Downloading and signing up is easy. Once signed in and verified users can start using Claude absolutely free. I downloaded it and gave it a try and was pleasantly surprised at its ability to help create a ramen recipe from scratch. A cool feature of the app is that it can continue chats from other devices.\n\nThe official announcement described various ways it’s useful:\n\n“Use Claude for work or for fun. Whether you’re drafting a business proposal between meetings, translating menus while traveling, brainstorming gift ideas while shopping, or composing a speech while waiting for a flight, Claude is ready to assist you.”\n\nDownload the Claude by Anthropic Android App from Google Play:\n\nClaude by Anthropic\n\nRead the official announcement:\n\nClaude Android app"
    },
    {
        "title": "CMOs Under Pressure: The Unseen Challenges In B2B Marketing",
        "url": "https://www.searchenginejournal.com/cmos-under-pressure-the-unseen-challenges-in-b2b-marketing/522319/",
        "content": "A recent study of 121 B2B CMOs and marketing leaders has uncovered current marketing industry challenges.\n\nThe study by Bospar, CMO Huddles, and Redpoint examines the concept of an “underground recession” in marketing departments and its implications for professionals in the field.\n\nKey findings include:\n\nBudget constraints and their impact on marketing strategies\n\nChanging deal cycles and their effects on revenue\n\nStaffing challenges and increased pressure on marketing teams\n\nEvolving CMO roles and job market trends\n\nRead on for a data-driven exploration of the current state of B2B marketing.\n\nMarketing Industry In A “Hidden” Recession\n\nDespite positive macroeconomic indicators, including a 9.34% increase in the S&P 500 since the beginning of 2024, marketing departments are experiencing a different reality.\n\nThe survey found that 69% of respondents believe their industry is in a recession, while 61% feel that the overall unemployment rate doesn’t accurately reflect the situation in their sector.\n\nKey Challenges Facing CMOs\n\nThe study identified four main trends making the job of marketing leaders increasingly difficult:\n\nBudget Cuts & Revenue Declines: 77% of marketing leaders reported flat or reduced budgets, with 38% experiencing cuts of at least 3%. Longer Deal Cycles: 54% of respondents noted extended sales cycles, impacting revenue timing and marketing budgets. Staffing Cuts & Layoffs: Half of the surveyed companies experienced layoffs, with 41% seeing cuts within their marketing departments. Pressure to Deliver More with Less: 69% of marketing leaders were asked to do more with reduced budgets in the past year.\n\nPersonal & Professional Toll on CMOs\n\nThe pressures of the current economic environment are reportedly taking a toll on marketing leaders.\n\n67% of respondents reported that the past year’s challenges have impacted their overall well-being.\n\nMany experienced adverse effects, including reduced exercise (80%), less time off (70%), and weight gain (40%).\n\nDeclining Job Prospects For CMOs\n\nThe study also highlighted a concerning trend in the job market for CMOs.\n\nLinkedIn data shows a 62% decrease in CMO job postings in the United States from February 2023 to February 2024.\n\nThis decline is partly attributed to companies consolidating marketing responsibilities under other C-suite roles.\n\nAdapting To The New Reality\n\nDespite these challenges, industry experts emphasize the need for CMOs to adapt and evolve their strategies.\n\nMarTech entrepreneur Jon Miller suggests that “the old playbooks just aren’t working anymore, and it’s time for a new playbook (and new technology) that aligns with modern buyers.”\n\nDrew Neisser of CMO Huddles recommends four key areas for CMOs to focus on:\n\nRole expansion beyond traditional marketing duties Metrics expansion to demonstrate marketing’s full value Idea concentration to maximize impact with limited resources AI implementation to drive innovation and efficiency\n\nWhy Does This Matter?\n\nThis study shows what’s happening in marketing beyond the rosy economic headlines.\n\nIt matters because:\n\nIt explains why your job might feel harder lately.\n\nIt shows we need to get creative with our strategies.\n\nIt highlights why proving marketing’s value is so important right now.\n\nWhat Does This Mean For You?\n\nHere’s what you should keep in mind:\n\nLearn skills that clearly show your worth, like data analysis.\n\nGet ready to do more with less – focus on what really matters.\n\nLook for ways to expand your role in the company.\n\nNetwork more – it could help you find new opportunities.\n\nKeep learning about new trends and tools.\n\nTake care of yourself – everyone’s feeling the pressure, not just you.\n\nFeatured Image: Ground Picture/Shutterstock"
    },
    {
        "title": "CMOs Under Pressure: The Unseen Challenges In B2B Marketing",
        "url": "https://www.searchenginejournal.com/cmos-under-pressure-the-unseen-challenges-in-b2b-marketing/522319/",
        "content": "A recent study of 121 B2B CMOs and marketing leaders has uncovered current marketing industry challenges.\n\nThe study by Bospar, CMO Huddles, and Redpoint examines the concept of an “underground recession” in marketing departments and its implications for professionals in the field.\n\nKey findings include:\n\nBudget constraints and their impact on marketing strategies\n\nChanging deal cycles and their effects on revenue\n\nStaffing challenges and increased pressure on marketing teams\n\nEvolving CMO roles and job market trends\n\nRead on for a data-driven exploration of the current state of B2B marketing.\n\nMarketing Industry In A “Hidden” Recession\n\nDespite positive macroeconomic indicators, including a 9.34% increase in the S&P 500 since the beginning of 2024, marketing departments are experiencing a different reality.\n\nThe survey found that 69% of respondents believe their industry is in a recession, while 61% feel that the overall unemployment rate doesn’t accurately reflect the situation in their sector.\n\nKey Challenges Facing CMOs\n\nThe study identified four main trends making the job of marketing leaders increasingly difficult:\n\nBudget Cuts & Revenue Declines: 77% of marketing leaders reported flat or reduced budgets, with 38% experiencing cuts of at least 3%. Longer Deal Cycles: 54% of respondents noted extended sales cycles, impacting revenue timing and marketing budgets. Staffing Cuts & Layoffs: Half of the surveyed companies experienced layoffs, with 41% seeing cuts within their marketing departments. Pressure to Deliver More with Less: 69% of marketing leaders were asked to do more with reduced budgets in the past year.\n\nPersonal & Professional Toll on CMOs\n\nThe pressures of the current economic environment are reportedly taking a toll on marketing leaders.\n\n67% of respondents reported that the past year’s challenges have impacted their overall well-being.\n\nMany experienced adverse effects, including reduced exercise (80%), less time off (70%), and weight gain (40%).\n\nDeclining Job Prospects For CMOs\n\nThe study also highlighted a concerning trend in the job market for CMOs.\n\nLinkedIn data shows a 62% decrease in CMO job postings in the United States from February 2023 to February 2024.\n\nThis decline is partly attributed to companies consolidating marketing responsibilities under other C-suite roles.\n\nAdapting To The New Reality\n\nDespite these challenges, industry experts emphasize the need for CMOs to adapt and evolve their strategies.\n\nMarTech entrepreneur Jon Miller suggests that “the old playbooks just aren’t working anymore, and it’s time for a new playbook (and new technology) that aligns with modern buyers.”\n\nDrew Neisser of CMO Huddles recommends four key areas for CMOs to focus on:\n\nRole expansion beyond traditional marketing duties Metrics expansion to demonstrate marketing’s full value Idea concentration to maximize impact with limited resources AI implementation to drive innovation and efficiency\n\nWhy Does This Matter?\n\nThis study shows what’s happening in marketing beyond the rosy economic headlines.\n\nIt matters because:\n\nIt explains why your job might feel harder lately.\n\nIt shows we need to get creative with our strategies.\n\nIt highlights why proving marketing’s value is so important right now.\n\nWhat Does This Mean For You?\n\nHere’s what you should keep in mind:\n\nLearn skills that clearly show your worth, like data analysis.\n\nGet ready to do more with less – focus on what really matters.\n\nLook for ways to expand your role in the company.\n\nNetwork more – it could help you find new opportunities.\n\nKeep learning about new trends and tools.\n\nTake care of yourself – everyone’s feeling the pressure, not just you.\n\nFeatured Image: Ground Picture/Shutterstock"
    },
    {
        "title": "Google’s Web Crawler Fakes Being “Idle” To Render JavaScript",
        "url": "https://www.searchenginejournal.com/googles-web-crawler-fakes-being-idle-to-render-javascript/522305/",
        "content": "In a recent episode of the Search Off The Record podcast, it was revealed that Google’s rendering system now pretends to be “idle” to trigger certain JavaScript events and improve webpage rendering.\n\nThe podcast features Zoe Clifford from Google’s rendering team, who discussed how the company’s web crawlers deal with JavaScript-based sites.\n\nThis revelation is insightful for web developers who use such methods to defer content loading.\n\nGoogle’s “Idle” Trick\n\nGooglebot simulates “idle” states during rendering, which triggers JavaScript events like requestIdleCallback.\n\nDevelopers use this function to defer loading less critical content until the browser is free from other tasks.\n\nBefore this change, Google’s rendering process was so efficient that the browser was always active, causing some websites to fail to load important content.\n\nClifford explained:\n\n“There was a certain popular video website which I won’t name…which deferred loading any of the page contents until after requestIdleCallback was fired.”\n\nSince the browser was never idle, this event wouldn’t fire, preventing much of the page from loading properly.\n\nFaking Idle Time To Improve Rendering\n\nGoogle implemented a system where the browser pretends to be idle periodically, even when it’s busy rendering pages.\n\nThis tweak ensures that idle callbacks are triggered correctly, allowing pages to fully load their content for indexing.\n\nImportance Of Error Handling\n\nClifford emphasized the importance of developers implementing graceful error handling in their JavaScript code.\n\nUnhandled errors can lead to blank pages, redirects, or missing content, negatively impacting indexing.\n\nShe advised:\n\n“If there is an error, I just try and handle it as gracefully as possible…web development is hard stuff.”\n\nWhat Does This Mean?\n\nImplications For Web Developers\n\nGraceful Error Handling : Implementing graceful error handling ensures pages load as intended, even if certain code elements fail.\n\n: Implementing graceful error handling ensures pages load as intended, even if certain code elements fail. Cautious Use of Idle Callbacks: While Google has adapted to handle idle callbacks, be wary of over-relying on these functions.\n\nImplications For SEO Professionals\n\nMonitoring & Testing : Implement regular website monitoring and testing to identify rendering issues that may impact search visibility.\n\n: Implement regular website monitoring and testing to identify rendering issues that may impact search visibility. Developer Collaboration : Collaborate with your development team to create user-friendly and search engine-friendly websites.\n\n: Collaborate with your development team to create user-friendly and search engine-friendly websites. Continuous Learning: Stay updated with the latest developments and best practices in how search engines handle JavaScript, render web pages, and evaluate content.\n\nSee also: Google Renders All Pages For Search, Including JavaScript-Heavy Sites\n\nOther Rendering-Related Topics Discussed\n\nThe discussion also touched on other rendering-related topics, such as the challenges posed by user agent detection and the handling of JavaScript redirects.\n\nThe whole podcast provides valuable insights into web rendering and the steps Google takes to assess pages accurately.\n\nSee also: Google Renders All Pages For Search, Including JavaScript-Heavy Sites\n\nFeatured Image: fizkes/Shutterstock"
    },
    {
        "title": "Google’s Web Crawler Fakes Being “Idle” To Render JavaScript",
        "url": "https://www.searchenginejournal.com/googles-web-crawler-fakes-being-idle-to-render-javascript/522305/",
        "content": "In a recent episode of the Search Off The Record podcast, it was revealed that Google’s rendering system now pretends to be “idle” to trigger certain JavaScript events and improve webpage rendering.\n\nThe podcast features Zoe Clifford from Google’s rendering team, who discussed how the company’s web crawlers deal with JavaScript-based sites.\n\nThis revelation is insightful for web developers who use such methods to defer content loading.\n\nGoogle’s “Idle” Trick\n\nGooglebot simulates “idle” states during rendering, which triggers JavaScript events like requestIdleCallback.\n\nDevelopers use this function to defer loading less critical content until the browser is free from other tasks.\n\nBefore this change, Google’s rendering process was so efficient that the browser was always active, causing some websites to fail to load important content.\n\nClifford explained:\n\n“There was a certain popular video website which I won’t name…which deferred loading any of the page contents until after requestIdleCallback was fired.”\n\nSince the browser was never idle, this event wouldn’t fire, preventing much of the page from loading properly.\n\nFaking Idle Time To Improve Rendering\n\nGoogle implemented a system where the browser pretends to be idle periodically, even when it’s busy rendering pages.\n\nThis tweak ensures that idle callbacks are triggered correctly, allowing pages to fully load their content for indexing.\n\nImportance Of Error Handling\n\nClifford emphasized the importance of developers implementing graceful error handling in their JavaScript code.\n\nUnhandled errors can lead to blank pages, redirects, or missing content, negatively impacting indexing.\n\nShe advised:\n\n“If there is an error, I just try and handle it as gracefully as possible…web development is hard stuff.”\n\nWhat Does This Mean?\n\nImplications For Web Developers\n\nGraceful Error Handling : Implementing graceful error handling ensures pages load as intended, even if certain code elements fail.\n\n: Implementing graceful error handling ensures pages load as intended, even if certain code elements fail. Cautious Use of Idle Callbacks: While Google has adapted to handle idle callbacks, be wary of over-relying on these functions.\n\nImplications For SEO Professionals\n\nMonitoring & Testing : Implement regular website monitoring and testing to identify rendering issues that may impact search visibility.\n\n: Implement regular website monitoring and testing to identify rendering issues that may impact search visibility. Developer Collaboration : Collaborate with your development team to create user-friendly and search engine-friendly websites.\n\n: Collaborate with your development team to create user-friendly and search engine-friendly websites. Continuous Learning: Stay updated with the latest developments and best practices in how search engines handle JavaScript, render web pages, and evaluate content.\n\nSee also: Google Renders All Pages For Search, Including JavaScript-Heavy Sites\n\nOther Rendering-Related Topics Discussed\n\nThe discussion also touched on other rendering-related topics, such as the challenges posed by user agent detection and the handling of JavaScript redirects.\n\nThe whole podcast provides valuable insights into web rendering and the steps Google takes to assess pages accurately.\n\nSee also: Google Renders All Pages For Search, Including JavaScript-Heavy Sites\n\nFeatured Image: fizkes/Shutterstock"
    },
    {
        "title": "CWV & Google Page Experience Ranking Factor Updated",
        "url": "https://www.searchenginejournal.com/cwv-google-page-experience-ranking-factor-updated/522279/",
        "content": "The June 2024 Chrome User Experience Report (CrUX) is out and it shows that websites in the real-world experienced an averaged across the board improvement in all Core Web Vitals (CWV) website performance scores. Some of the improvements are attributable to a change in how Interaction To Next Paint is measured, which will be good news to websites with dialog modals (popups).\n\nCrUX Dataset\n\nThe CrUX dataset consists of actual Core Web Vitals performance scores as measured in Chrome browsers when visiting websites. The data comes from browsers that were voluntarily opted in to report website performance metrics. The CrUX dataset is publicly available and is used by PageSpeed Insights, third party tools.\n\nCrUX Influences Page Experience Ranking Factor\n\nThe CrUX report is used for Google’s Page Experience Ranking Factor. The data is publicly available and can be used for evaluating performance, including competitor performance. CrUX is important because it is one of the only metrics that a website publishers can check that have something to do with a website ranking factor.\n\nAccording to Google’s overview documentation:\n\n“The data collected by CrUX is available publicly through a number of Google tools and third-party tools and is used by Google Search to inform the page experience ranking factor.”\n\nWhile the influence of the Page Experience Ranking Factor may be on the lower side, it’s still important for reasons outside of algorithms like improving conversions and ad clicks.\n\nJune 2024 Dataset\n\nThe dataset for June 2024 has been published and it shows that Core Web Vitals (CWV) website performance scores have incrementally risen across the board by modest percentages. This shows that website performance continues to be a focus for websites. Most of the popular content management systems are doing their best to improve, with WordPress making positive improvements with each new version that’s released.\n\nThe following scores are for origins. Origins are the entire website, which is different from Pages.\n\nThese are the average origin scores:\n\nLargest Contentful Paint (LCP)\n\nThis is a measurement of how fast the main content of a page loads. It specifically measures the largest image or content block that’s visible in a browser (viewport).\n\n63.4% (↑ 2.0%) had good LCP\n\nThis is a measurement of how fast the main content of a page loads. It specifically measures the largest image or content block that’s visible in a browser (viewport). 63.4% (↑ 2.0%) had good LCP Cumulative Layout Shift (CLS)\n\nMeasures how long it takes for web page layout to become stable without elements jumping and shifting on the page.\n\n77.8% (↑ 0.5%) had good CLS\n\nMeasures how long it takes for web page layout to become stable without elements jumping and shifting on the page. 77.8% (↑ 0.5%) had good CLS Interaction to Next Paint (INP)\n\nINP measures how long it takes for a web page to become responsive to user interactions\n\n84.1% (↑ 1.1%) had good INP\n\nINP measures how long it takes for a web page to become responsive to user interactions 84.1% (↑ 1.1%) had good INP Percentage Of Sites With Good CWV\n\nThis is the percentage of sites that had passing scores across all three Core Web Vitals metrics\n\n51.0% (↑ 2.3%) had good LCP, CLS and INP\n\nChanges To INP Measurements\n\nChrome made changes to how long it takes for a page to become interactive (Interaction to Next Paint – INP) is measured, making it more accurate. This may have helped to increase the scores of some sites that were inadvertently ranked lower for INP because the metric failed to account for some kinds of popups.\n\nThe Chrome team explained:\n\n“The Chrome team has been continuing work on improving efficiencies in Chrome’s handling of the Core Web Vitals metrics and recently launched some changes to INP which may have contributed to the positive trend this month. The most notable change is to better handle use of the basic modal dialogs (alert, confirm, print). While technically these are synchronous and block the main thread—and so are not recommended if there are alternatives—they do present user feedback for an interaction. They were previously not counted as presentation feedback for INP, which could result in very high INP values for sites that did use these. From Chrome 127 the presentation of the modal will mark the end measurement time for INP and so should lead to improved INP times for those sites.”\n\nRead the June 2024 CWV Announcement\n\nThe 202406 dataset is live\n\nFeatured Image by Shutterstock/Ivan Dudka"
    },
    {
        "title": "CWV & Google Page Experience Ranking Factor Updated",
        "url": "https://www.searchenginejournal.com/cwv-google-page-experience-ranking-factor-updated/522279/",
        "content": "The June 2024 Chrome User Experience Report (CrUX) is out and it shows that websites in the real-world experienced an averaged across the board improvement in all Core Web Vitals (CWV) website performance scores. Some of the improvements are attributable to a change in how Interaction To Next Paint is measured, which will be good news to websites with dialog modals (popups).\n\nCrUX Dataset\n\nThe CrUX dataset consists of actual Core Web Vitals performance scores as measured in Chrome browsers when visiting websites. The data comes from browsers that were voluntarily opted in to report website performance metrics. The CrUX dataset is publicly available and is used by PageSpeed Insights, third party tools.\n\nCrUX Influences Page Experience Ranking Factor\n\nThe CrUX report is used for Google’s Page Experience Ranking Factor. The data is publicly available and can be used for evaluating performance, including competitor performance. CrUX is important because it is one of the only metrics that a website publishers can check that have something to do with a website ranking factor.\n\nAccording to Google’s overview documentation:\n\n“The data collected by CrUX is available publicly through a number of Google tools and third-party tools and is used by Google Search to inform the page experience ranking factor.”\n\nWhile the influence of the Page Experience Ranking Factor may be on the lower side, it’s still important for reasons outside of algorithms like improving conversions and ad clicks.\n\nJune 2024 Dataset\n\nThe dataset for June 2024 has been published and it shows that Core Web Vitals (CWV) website performance scores have incrementally risen across the board by modest percentages. This shows that website performance continues to be a focus for websites. Most of the popular content management systems are doing their best to improve, with WordPress making positive improvements with each new version that’s released.\n\nThe following scores are for origins. Origins are the entire website, which is different from Pages.\n\nThese are the average origin scores:\n\nLargest Contentful Paint (LCP)\n\nThis is a measurement of how fast the main content of a page loads. It specifically measures the largest image or content block that’s visible in a browser (viewport).\n\n63.4% (↑ 2.0%) had good LCP\n\nThis is a measurement of how fast the main content of a page loads. It specifically measures the largest image or content block that’s visible in a browser (viewport). 63.4% (↑ 2.0%) had good LCP Cumulative Layout Shift (CLS)\n\nMeasures how long it takes for web page layout to become stable without elements jumping and shifting on the page.\n\n77.8% (↑ 0.5%) had good CLS\n\nMeasures how long it takes for web page layout to become stable without elements jumping and shifting on the page. 77.8% (↑ 0.5%) had good CLS Interaction to Next Paint (INP)\n\nINP measures how long it takes for a web page to become responsive to user interactions\n\n84.1% (↑ 1.1%) had good INP\n\nINP measures how long it takes for a web page to become responsive to user interactions 84.1% (↑ 1.1%) had good INP Percentage Of Sites With Good CWV\n\nThis is the percentage of sites that had passing scores across all three Core Web Vitals metrics\n\n51.0% (↑ 2.3%) had good LCP, CLS and INP\n\nChanges To INP Measurements\n\nChrome made changes to how long it takes for a page to become interactive (Interaction to Next Paint – INP) is measured, making it more accurate. This may have helped to increase the scores of some sites that were inadvertently ranked lower for INP because the metric failed to account for some kinds of popups.\n\nThe Chrome team explained:\n\n“The Chrome team has been continuing work on improving efficiencies in Chrome’s handling of the Core Web Vitals metrics and recently launched some changes to INP which may have contributed to the positive trend this month. The most notable change is to better handle use of the basic modal dialogs (alert, confirm, print). While technically these are synchronous and block the main thread—and so are not recommended if there are alternatives—they do present user feedback for an interaction. They were previously not counted as presentation feedback for INP, which could result in very high INP values for sites that did use these. From Chrome 127 the presentation of the modal will mark the end measurement time for INP and so should lead to improved INP times for those sites.”\n\nRead the June 2024 CWV Announcement\n\nThe 202406 dataset is live\n\nFeatured Image by Shutterstock/Ivan Dudka"
    },
    {
        "title": "The Reason Why Google Uses The Word “Creators”",
        "url": "https://www.searchenginejournal.com/the-reason-why-google-uses-the-word-creators/522200/",
        "content": "Google’s SearchLiaison responded to criticism over how they refer to website publishers with an answer that reflects not just changing times but also the practical reasons for doing so. The answer reflects how important it is for digital marketing to maintain the flexibility to bend with change.\n\nChange: There Isn’t Always A Motivation\n\nThe discussion began with a tweet by someone who objected to the use of the phrase “creators” instead of other terms like businesses or publishers because the word creators minimizes the fact that there are businesses behind the websites.\n\nThis is the tweet:\n\n“Notice the term “creators” in this piece. This is an example of Google’s successful effort to change the narrative. In the past they have used “publishers”, “businesses”, and just “web sites”. But “creators” minimizes business impact. And clearly some are falling for the trap.”\n\nNotice the term \"creators\" in this piece. This is an example of Google's successful effort to change the narrative. In the past they have used \"publishers\", \"businesses\", and just \"web sites\". But \"creators\" minimizes business impact. And clearly some are falling for the trap. https://t.co/KB2fIe34HV — Joe Hall ☕️ (@joehall) July 12, 2024\n\nKeeping Up With The Pace Of Change\n\nSearchLiaison’s response reflected something that is commonly misunderstood, which is that everything changes, including fashion, customs, norms and even speech. Those who lack self-awareness on this point will blink and miss it when the page turns on their generation and another one steps forward to take take their place at the center of the world.\n\nThis is especially true for SEO, where Google typically is a brand new search engine every five years.\n\nThis is SearchLiaison’s answer:\n\n“We used to say “webmasters” in the past, and that doesn’t really speak to so many people who have an interest in appearing in search results. That’s in part why we have tended to say “creators” more — though not exclusively — for years now. It’s not a particularly new thing. It’s also why Search Central got its new name in 2020, the whole “webmasters” isn’t really that inclusive (or used) term: https://developers.google.com/search/blog/2020/11/goodbye-google-webmasters “Publishers” tends to be heard by and used by those involved in news publishing. Businesses often just think of themselves as businesses. SEOs tend to be SEOs, and if you use that term, you exclude those who don’t think of SEOs but want to understand some of the things we share. So “creators” tends to be the catch-all term we used, as imperfect as it is, because sometime you really need one term rather than “Here’s what creators and SEO and businesses and brands and news publishers and etc etc should know about something….” All that said, I am seeing more of a need to use creators as less a catch-all and more to refer to people like Brandon who really do view themselves as content creators first-and-foremost. The work they do can be much different than an SEO, or a content marketer, or a local business and so on.”\n\nAnd in a follow up he continued:\n\n“We do say web sites when talking about web sites. But “web sites” isn’t a term that’s workable when addressing the people who are involved with web sites and have questions about their content appearing in search results.”\n\nEphemeral Quality Of Digital Marketing\n\nIt’s not just Google that changes, people change as well. Demand for certain products peak and then disappear. Ringtones used to be the hot affiliate product and then it was not. Technology drives change as well, as we’re currently seeing with AI.\n\nGoogle’s choice of the word creators is a small marker of change. You can roll with it or simply roll your own.\n\nFeatured Image by Shutterstock/Mix and Match Studio"
    },
    {
        "title": "The Reason Why Google Uses The Word “Creators”",
        "url": "https://www.searchenginejournal.com/the-reason-why-google-uses-the-word-creators/522200/",
        "content": "Google’s SearchLiaison responded to criticism over how they refer to website publishers with an answer that reflects not just changing times but also the practical reasons for doing so. The answer reflects how important it is for digital marketing to maintain the flexibility to bend with change.\n\nChange: There Isn’t Always A Motivation\n\nThe discussion began with a tweet by someone who objected to the use of the phrase “creators” instead of other terms like businesses or publishers because the word creators minimizes the fact that there are businesses behind the websites.\n\nThis is the tweet:\n\n“Notice the term “creators” in this piece. This is an example of Google’s successful effort to change the narrative. In the past they have used “publishers”, “businesses”, and just “web sites”. But “creators” minimizes business impact. And clearly some are falling for the trap.”\n\nNotice the term \"creators\" in this piece. This is an example of Google's successful effort to change the narrative. In the past they have used \"publishers\", \"businesses\", and just \"web sites\". But \"creators\" minimizes business impact. And clearly some are falling for the trap. https://t.co/KB2fIe34HV — Joe Hall ☕️ (@joehall) July 12, 2024\n\nKeeping Up With The Pace Of Change\n\nSearchLiaison’s response reflected something that is commonly misunderstood, which is that everything changes, including fashion, customs, norms and even speech. Those who lack self-awareness on this point will blink and miss it when the page turns on their generation and another one steps forward to take take their place at the center of the world.\n\nThis is especially true for SEO, where Google typically is a brand new search engine every five years.\n\nThis is SearchLiaison’s answer:\n\n“We used to say “webmasters” in the past, and that doesn’t really speak to so many people who have an interest in appearing in search results. That’s in part why we have tended to say “creators” more — though not exclusively — for years now. It’s not a particularly new thing. It’s also why Search Central got its new name in 2020, the whole “webmasters” isn’t really that inclusive (or used) term: https://developers.google.com/search/blog/2020/11/goodbye-google-webmasters “Publishers” tends to be heard by and used by those involved in news publishing. Businesses often just think of themselves as businesses. SEOs tend to be SEOs, and if you use that term, you exclude those who don’t think of SEOs but want to understand some of the things we share. So “creators” tends to be the catch-all term we used, as imperfect as it is, because sometime you really need one term rather than “Here’s what creators and SEO and businesses and brands and news publishers and etc etc should know about something….” All that said, I am seeing more of a need to use creators as less a catch-all and more to refer to people like Brandon who really do view themselves as content creators first-and-foremost. The work they do can be much different than an SEO, or a content marketer, or a local business and so on.”\n\nAnd in a follow up he continued:\n\n“We do say web sites when talking about web sites. But “web sites” isn’t a term that’s workable when addressing the people who are involved with web sites and have questions about their content appearing in search results.”\n\nEphemeral Quality Of Digital Marketing\n\nIt’s not just Google that changes, people change as well. Demand for certain products peak and then disappear. Ringtones used to be the hot affiliate product and then it was not. Technology drives change as well, as we’re currently seeing with AI.\n\nGoogle’s choice of the word creators is a small marker of change. You can roll with it or simply roll your own.\n\nFeatured Image by Shutterstock/Mix and Match Studio"
    },
    {
        "title": "Google’s Indifference To Site Publishers Explained",
        "url": "https://www.searchenginejournal.com/googles-indifference-to-site-publishers-explained/522143/",
        "content": "A publisher named Brandon Saltalamacchia interviewed Google’s SearchLiaison in which he offered hope that quality sites hit by Google’s algorithms may soon see their traffic levels bounce back. But that interview and a recent Google podcast reveal deeper issues that may explain why Google seems indifferent to publishers with every update.\n\nGoogle Search Relations\n\nGoogle has a team whose job is to communicate how site owners can do well on Google. So it’s not that Googlers themselves are indifferent to site publishers and creatives. Google provides a lot of feedback to publishers, especially through Google Search Console. The area in which Google is indifferent to publishers is directly in search at its most fundamental level.\n\nGoogle’s algorithms are built on the premise that it has to provide a good user experience and is internally evaluated to that standard. This creates the situation where from Google’s perspective the algorithm is working the way it should. But from the perspective of website publishers Google’s ranking algorithms are failing. Putting a finger on why that’s happening is what this article is about.\n\nPublishers Are Not Even An Afterthought To Google\n\nThe interview by Brandon Saltalamacchia comes against the background of many websites having lost traffic due to Google’s recent algorithm updates. From Google’s point of view their algorithms are working fine for users. But the steady feedback from website publishers is no, it’s not working. Google’s response for the past month is that they’re investigating how to improve.\n\nWhat all of this reveals is that there is a real disconnect between how Google measures how their algorithms are working and how website publishers experience it in the real world. It may surprise most people to learn that that this disconnect begins with Google’s mission statement to make information “universally accessible and useful” and ends with the rollout of an algorithm that is tested for metrics that take into account how users experience it but is 100% blind to how publishers experience it.\n\nSome of the complaints about Google’s algorithms:\n\nRanking algorithms for reviews, travel and other topics are favoring big brands over smaller publishers.\n\nGoogle’s decision to firehose traffic at Reddit contributes to the dismantling of the website publishing ecosystem.\n\nAI Overviews summarizes web pages and deprives websites of search traffic.\n\nThe stated goal for Google’s algorithm decisions is to increase user satisfaction but the problem with that approach is that website publishers are left out of that equation. Consider this: Google’s Search Quality Raters Guidelines says nothing about checking if big brands are dominating the search results. Zero.\n\nWebsite publishers aren’t even an afterthought for Google. Publishers are not not considered at any stage of the creation, testing and rollout of ranking algorithms.\n\nGoogle Historically Doesn’t Focus On Publishers\n\nA remark by Gary Illyes in a recent Search Off The Record indicated that in Gary’s opinion Google is all about the user experience because if search is good for the user then that’ll trickle down to the publishers and will be good for them too.\n\nIn the context of Gary explaining whether Google will announce that something is broken in search, Gary emphasized that search relations is focused on the search users and not the publishers who may be suffering from whatever is broken.\n\nJohn Mueller asked:\n\n“So, is the focus more on what users would see or what site owners would see? Because, as a Search Relations team, we would focus more on site owners. But it sounds like you’re saying, for these issues, we would look at what users would experience.”\n\nGary Illyes answered:\n\n“So it’s Search Relations, not Site Owners Relations, from Search perspective.”\n\nSee also: Google Search Liaison Gives Rare Glimpse Into Feedback Process\n\nGoogle’s Indifference To Publishers\n\nGoogle’s focus on satisfying search users can in practice turn into indifference toward publishers. If you read all the Google patents and research papers related to information retrieval (search technology) the one thing that becomes apparent is that the measure of success is always about the users. The impact to site publishers are consistently ignored. That’s why Google Search is perceived as indifferent to site publishers, because publishers have never been a part of the search satisfaction equation.\n\nThis is something that publishers and Google may not have wrapped their minds around just yet.\n\nLater on, in the Search Off The Record podcast, the Googlers specifically discuss how an update is deemed to be working well regardless if a (relatively) small amount of publishers are complaining that Google Search is broken, because what matters is if Google perceives that they are doing the right thing from Google’s perspective.\n\nJohn said:\n\n“…Sometimes we get feedback after big ranking updates, like core updates, where people are like, “Oh, everything is broken.”\n\nAt the 12:06 minute mark of the podcast Gary made light of that kind of feedback:\n\n“Do we? We get feedback like that?”\n\nMueller responded:\n\n“Well, yeah.”\n\nThen Mueller completed his thought:\n\n“I feel bad for them. I kind of understand that. I think those are the kind of situations where we would look at the examples and be like, “Oh, I see some sites are unhappy with this, but overall we’re doing the right thing from our perspective.”\n\nAnd Gary responded:\n\n“Right.”\n\nAnd John asks:\n\n“And then we wouldn’t see it as an issue, right?”\n\nGary affirmed that Google wouldn’t see it as an issue if a legit publisher loses traffic when overall the algorithm is working as they feel it should.\n\n“Yeah.”\n\nIt is precisely that shrugging indifference that a website publisher, Brandon Saltalamacchia, is concerned about and discussed with SearchLiaison in a recent blog post.\n\nLots of Questions\n\nSearchLiaison asked many questions about how Google could better support content creators, which is notable because Google has a long history of focusing on their user experience but seemingly not also considering what the impact on businesses with an online presence.\n\nThat’s a good sign from SearchLiaison but not entirely a surprise because unlike most Googlers, SearchLiaison (aka Danny Sullivan) has decades of experience as a publisher so he knows what it’s like on our side of the search box.\n\nIt will be interesting if SearchLiaison’s concern for publishers makes it back to Google in a more profound way so that there’s a better understanding that the Search Ecosystem is greater than Google’s users and encompasses website publishers, too. Algorithm updates should be about more than how they impact users, the updates should also be about how they impact publishers.\n\nHope For Sites That Lost Traffic\n\nPerhaps the most important news from the interview is that SearchLiaison expressed that there may be changes coming over the next few months that will benefit the publishers who have lost rankings over the past few months of updates.\n\nBrandon wrote:\n\n“One main take away from my conversation with Danny is that he did say to hang on, to keep doing what we are doing and that he’s hopeful that those of us building great websites will see some signs of recovery over the coming months.”\n\nYet despite those promises from Danny, Brandon didn’t come away with hope.\n\nBrandon wrote:\n\n“I got the sense things won’t change fast, nor anytime soon. “\n\nRead the entire interview:\n\nA Brief Meeting With Google After The Apocalypse\n\nListen to the Search Off The Record Podcast\n\nFeatured Image by Shutterstock/Roman Samborskyi"
    },
    {
        "title": "Google’s Indifference To Site Publishers Explained",
        "url": "https://www.searchenginejournal.com/googles-indifference-to-site-publishers-explained/522143/",
        "content": "A publisher named Brandon Saltalamacchia interviewed Google’s SearchLiaison in which he offered hope that quality sites hit by Google’s algorithms may soon see their traffic levels bounce back. But that interview and a recent Google podcast reveal deeper issues that may explain why Google seems indifferent to publishers with every update.\n\nGoogle Search Relations\n\nGoogle has a team whose job is to communicate how site owners can do well on Google. So it’s not that Googlers themselves are indifferent to site publishers and creatives. Google provides a lot of feedback to publishers, especially through Google Search Console. The area in which Google is indifferent to publishers is directly in search at its most fundamental level.\n\nGoogle’s algorithms are built on the premise that it has to provide a good user experience and is internally evaluated to that standard. This creates the situation where from Google’s perspective the algorithm is working the way it should. But from the perspective of website publishers Google’s ranking algorithms are failing. Putting a finger on why that’s happening is what this article is about.\n\nPublishers Are Not Even An Afterthought To Google\n\nThe interview by Brandon Saltalamacchia comes against the background of many websites having lost traffic due to Google’s recent algorithm updates. From Google’s point of view their algorithms are working fine for users. But the steady feedback from website publishers is no, it’s not working. Google’s response for the past month is that they’re investigating how to improve.\n\nWhat all of this reveals is that there is a real disconnect between how Google measures how their algorithms are working and how website publishers experience it in the real world. It may surprise most people to learn that that this disconnect begins with Google’s mission statement to make information “universally accessible and useful” and ends with the rollout of an algorithm that is tested for metrics that take into account how users experience it but is 100% blind to how publishers experience it.\n\nSome of the complaints about Google’s algorithms:\n\nRanking algorithms for reviews, travel and other topics are favoring big brands over smaller publishers.\n\nGoogle’s decision to firehose traffic at Reddit contributes to the dismantling of the website publishing ecosystem.\n\nAI Overviews summarizes web pages and deprives websites of search traffic.\n\nThe stated goal for Google’s algorithm decisions is to increase user satisfaction but the problem with that approach is that website publishers are left out of that equation. Consider this: Google’s Search Quality Raters Guidelines says nothing about checking if big brands are dominating the search results. Zero.\n\nWebsite publishers aren’t even an afterthought for Google. Publishers are not not considered at any stage of the creation, testing and rollout of ranking algorithms.\n\nGoogle Historically Doesn’t Focus On Publishers\n\nA remark by Gary Illyes in a recent Search Off The Record indicated that in Gary’s opinion Google is all about the user experience because if search is good for the user then that’ll trickle down to the publishers and will be good for them too.\n\nIn the context of Gary explaining whether Google will announce that something is broken in search, Gary emphasized that search relations is focused on the search users and not the publishers who may be suffering from whatever is broken.\n\nJohn Mueller asked:\n\n“So, is the focus more on what users would see or what site owners would see? Because, as a Search Relations team, we would focus more on site owners. But it sounds like you’re saying, for these issues, we would look at what users would experience.”\n\nGary Illyes answered:\n\n“So it’s Search Relations, not Site Owners Relations, from Search perspective.”\n\nSee also: Google Search Liaison Gives Rare Glimpse Into Feedback Process\n\nGoogle’s Indifference To Publishers\n\nGoogle’s focus on satisfying search users can in practice turn into indifference toward publishers. If you read all the Google patents and research papers related to information retrieval (search technology) the one thing that becomes apparent is that the measure of success is always about the users. The impact to site publishers are consistently ignored. That’s why Google Search is perceived as indifferent to site publishers, because publishers have never been a part of the search satisfaction equation.\n\nThis is something that publishers and Google may not have wrapped their minds around just yet.\n\nLater on, in the Search Off The Record podcast, the Googlers specifically discuss how an update is deemed to be working well regardless if a (relatively) small amount of publishers are complaining that Google Search is broken, because what matters is if Google perceives that they are doing the right thing from Google’s perspective.\n\nJohn said:\n\n“…Sometimes we get feedback after big ranking updates, like core updates, where people are like, “Oh, everything is broken.”\n\nAt the 12:06 minute mark of the podcast Gary made light of that kind of feedback:\n\n“Do we? We get feedback like that?”\n\nMueller responded:\n\n“Well, yeah.”\n\nThen Mueller completed his thought:\n\n“I feel bad for them. I kind of understand that. I think those are the kind of situations where we would look at the examples and be like, “Oh, I see some sites are unhappy with this, but overall we’re doing the right thing from our perspective.”\n\nAnd Gary responded:\n\n“Right.”\n\nAnd John asks:\n\n“And then we wouldn’t see it as an issue, right?”\n\nGary affirmed that Google wouldn’t see it as an issue if a legit publisher loses traffic when overall the algorithm is working as they feel it should.\n\n“Yeah.”\n\nIt is precisely that shrugging indifference that a website publisher, Brandon Saltalamacchia, is concerned about and discussed with SearchLiaison in a recent blog post.\n\nLots of Questions\n\nSearchLiaison asked many questions about how Google could better support content creators, which is notable because Google has a long history of focusing on their user experience but seemingly not also considering what the impact on businesses with an online presence.\n\nThat’s a good sign from SearchLiaison but not entirely a surprise because unlike most Googlers, SearchLiaison (aka Danny Sullivan) has decades of experience as a publisher so he knows what it’s like on our side of the search box.\n\nIt will be interesting if SearchLiaison’s concern for publishers makes it back to Google in a more profound way so that there’s a better understanding that the Search Ecosystem is greater than Google’s users and encompasses website publishers, too. Algorithm updates should be about more than how they impact users, the updates should also be about how they impact publishers.\n\nHope For Sites That Lost Traffic\n\nPerhaps the most important news from the interview is that SearchLiaison expressed that there may be changes coming over the next few months that will benefit the publishers who have lost rankings over the past few months of updates.\n\nBrandon wrote:\n\n“One main take away from my conversation with Danny is that he did say to hang on, to keep doing what we are doing and that he’s hopeful that those of us building great websites will see some signs of recovery over the coming months.”\n\nYet despite those promises from Danny, Brandon didn’t come away with hope.\n\nBrandon wrote:\n\n“I got the sense things won’t change fast, nor anytime soon. “\n\nRead the entire interview:\n\nA Brief Meeting With Google After The Apocalypse\n\nListen to the Search Off The Record Podcast\n\nFeatured Image by Shutterstock/Roman Samborskyi"
    },
    {
        "title": "Google Says There’s No Way To Block Content Only From Discover Feed",
        "url": "https://www.searchenginejournal.com/google-says-theres-no-way-to-block-content-from-discover-feed/522148/",
        "content": "Google officials confirmed on X (formerly Twitter) that there’s no way to block content from appearing in Google Discover, despite the ability to do so for Google News.\n\nThe conversation was initiated by Lily Ray, who raised concerns about a common challenge where certain content may not be suitable for Google News or Discover but performs well in organic search results.\n\nRay states:\n\n“We have experienced many situations with publisher clients where it would be helpful to prevent some content from being crawled/indexed specifically for Google News & Discover. However, this content often performs incredibly well in organic search, so it’s not a good idea to noindex it across the board. This content often falls in the grey area of what is forbidden in Google’s News & Discover guidelines, but still drives massive SEO traffic. We have noticed that having too much of this content appears to be detrimental to Discover performance over time. Outside of your guidelines for SafeSearch – has Google considered a mechanism to prevent individual pages from being considered for News/Discover?”\n\nGoogle’s Response\n\nIn response to Ray’s question, Google’s Search Liaison pointed to existing methods for blocking content from Google News.\n\nHowever, upon checking with John Mueller of Google’s Search Relations team, the Liaison confirmed these methods don’t extend to Google Discover.\n\nThe Search Liaison stated:\n\n“John [Mueller] and I pinged, and we’re pretty sure there’s not an option to just block content from Discover.”\n\nRecognizing the potential value of such a feature, he added:\n\n“That would seem useful, so we’ll pass it on.”\n\nJohn and I pinged, and we're pretty sure there's not an option to just block content from Discover. That would seem useful, so we'll pass it on. — Google SearchLiaison (@searchliaison) July 15, 2024\n\nWhat Does This Mean?\n\nThis admission from Google highlights a gap in publishers’ ability to control how Google crawls their content.\n\nWhile tools exist to manage content crawling for Google News and organic search results, the lack of similar controls for Discover presents a challenge.\n\nGoogle’s Search Liaison suggests there’s potential for more granular controls, though there are no immediate plans to introduce content blocking features for Discover.\n\nFeatured Image: Informa Plus/Shutterstock"
    },
    {
        "title": "Google Says There’s No Way To Block Content Only From Discover Feed",
        "url": "https://www.searchenginejournal.com/google-says-theres-no-way-to-block-content-from-discover-feed/522148/",
        "content": "Google officials confirmed on X (formerly Twitter) that there’s no way to block content from appearing in Google Discover, despite the ability to do so for Google News.\n\nThe conversation was initiated by Lily Ray, who raised concerns about a common challenge where certain content may not be suitable for Google News or Discover but performs well in organic search results.\n\nRay states:\n\n“We have experienced many situations with publisher clients where it would be helpful to prevent some content from being crawled/indexed specifically for Google News & Discover. However, this content often performs incredibly well in organic search, so it’s not a good idea to noindex it across the board. This content often falls in the grey area of what is forbidden in Google’s News & Discover guidelines, but still drives massive SEO traffic. We have noticed that having too much of this content appears to be detrimental to Discover performance over time. Outside of your guidelines for SafeSearch – has Google considered a mechanism to prevent individual pages from being considered for News/Discover?”\n\nGoogle’s Response\n\nIn response to Ray’s question, Google’s Search Liaison pointed to existing methods for blocking content from Google News.\n\nHowever, upon checking with John Mueller of Google’s Search Relations team, the Liaison confirmed these methods don’t extend to Google Discover.\n\nThe Search Liaison stated:\n\n“John [Mueller] and I pinged, and we’re pretty sure there’s not an option to just block content from Discover.”\n\nRecognizing the potential value of such a feature, he added:\n\n“That would seem useful, so we’ll pass it on.”\n\nJohn and I pinged, and we're pretty sure there's not an option to just block content from Discover. That would seem useful, so we'll pass it on. — Google SearchLiaison (@searchliaison) July 15, 2024\n\nWhat Does This Mean?\n\nThis admission from Google highlights a gap in publishers’ ability to control how Google crawls their content.\n\nWhile tools exist to manage content crawling for Google News and organic search results, the lack of similar controls for Discover presents a challenge.\n\nGoogle’s Search Liaison suggests there’s potential for more granular controls, though there are no immediate plans to introduce content blocking features for Discover.\n\nFeatured Image: Informa Plus/Shutterstock"
    },
    {
        "title": "YouTube Expands Shorts Toolkit: Auto Layout, Text-to-Speech, & More",
        "url": "https://www.searchenginejournal.com/youtube-expands-shorts-toolkit-auto-layout-text-to-speech-more/522117/",
        "content": "YouTube has introduced new tools for Shorts, designed to make content creation more accessible.\n\nThe news comes as part of the third installment of “Release Notes,” featuring YouTube’s Chief Product Officer Johanna Voolich and Creator Liaison Rene Ritchie.\n\nBreaking Down Barriers To Creation\n\nYouTube Shorts, which has garnered 70 billion daily views, is simplifying the creation process.\n\n“We want to inspire creation and introduce tools that make it easy on Shorts,” said Voolich during the announcement.\n\nNew Features Coming to Shorts\n\nAuto Layout: This Android-exclusive feature automatically tracks the main subject when converting longer videos into Shorts, ensuring optimal framing. Text-to-Speech Narration: Creators can add narration to their Shorts by selecting from four different voices to read out on-screen text. “Add Yours” Sticker: This interactive element allows you to encourage audience participation by inviting viewers to contribute related content, creating a chain of user-generated responses. Minecraft Effects: To celebrate Minecraft’s 15th anniversary, two new effects have been introduced: Minecraft Spring and Minecraft Rush. The latter is a playable mini-game. Enhanced Captions: Soon, creators can add, edit, and stylize auto-generated captions directly on their content. Remix Capabilities: Building on existing remixing tools, users can now “remix a remix.”\n\nCreator Feedback & Future Developments\n\nDuring the announcement, Voolich addressed questions from successful Shorts creators.\n\nGrowing An Audience With Shorts\n\nJenny Hoyos, who has amassed over one billion views on the platform, inquired about tools to help build community.\n\nVoolich suggested using the reply feature and going live to better connect with viewers.\n\nLonger Shorts On The Way?\n\nAnother creator raised the question of potentially extending Shorts beyond the current 60-second limit.\n\nWhile no immediate changes were announced, Voolich expressed interest in creator feedback on ideal video lengths.\n\nMore Creators Earning Money\n\nYouTube is celebrating the first anniversary of its Partner Program for Shorts, with Voolich noting that 25% of YPP creators are now earning money through the format.\n\nWhy Does This Matter?\n\nYouTube’s new Shorts features matter because:\n\nMore people can make videos : The new tools make it easier for anyone to create Shorts.\n\n: The new tools make it easier for anyone to create Shorts. Keeping up with TikTok : YouTube is trying to make Shorts popular by offering features similar to other short video apps.\n\n: YouTube is trying to make Shorts popular by offering features similar to other short video apps. Money for creators : More ways to make Shorts could mean more chances for creators to earn money.\n\n: More ways to make Shorts could mean more chances for creators to earn money. Fun for viewers : New features like stickers and effects make watching Shorts more entertaining and interactive.\n\n: New features like stickers and effects make watching Shorts more entertaining and interactive. Easier to understand: Better caption options help people enjoy videos even without sound.\n\nThese changes show that YouTube is paying attention to what people want in short videos.\n\nFeatured Image: Prathmesh T/Shutterstock"
    },
    {
        "title": "YouTube Expands Shorts Toolkit: Auto Layout, Text-to-Speech, & More",
        "url": "https://www.searchenginejournal.com/youtube-expands-shorts-toolkit-auto-layout-text-to-speech-more/522117/",
        "content": "YouTube has introduced new tools for Shorts, designed to make content creation more accessible.\n\nThe news comes as part of the third installment of “Release Notes,” featuring YouTube’s Chief Product Officer Johanna Voolich and Creator Liaison Rene Ritchie.\n\nBreaking Down Barriers To Creation\n\nYouTube Shorts, which has garnered 70 billion daily views, is simplifying the creation process.\n\n“We want to inspire creation and introduce tools that make it easy on Shorts,” said Voolich during the announcement.\n\nNew Features Coming to Shorts\n\nAuto Layout: This Android-exclusive feature automatically tracks the main subject when converting longer videos into Shorts, ensuring optimal framing. Text-to-Speech Narration: Creators can add narration to their Shorts by selecting from four different voices to read out on-screen text. “Add Yours” Sticker: This interactive element allows you to encourage audience participation by inviting viewers to contribute related content, creating a chain of user-generated responses. Minecraft Effects: To celebrate Minecraft’s 15th anniversary, two new effects have been introduced: Minecraft Spring and Minecraft Rush. The latter is a playable mini-game. Enhanced Captions: Soon, creators can add, edit, and stylize auto-generated captions directly on their content. Remix Capabilities: Building on existing remixing tools, users can now “remix a remix.”\n\nCreator Feedback & Future Developments\n\nDuring the announcement, Voolich addressed questions from successful Shorts creators.\n\nGrowing An Audience With Shorts\n\nJenny Hoyos, who has amassed over one billion views on the platform, inquired about tools to help build community.\n\nVoolich suggested using the reply feature and going live to better connect with viewers.\n\nLonger Shorts On The Way?\n\nAnother creator raised the question of potentially extending Shorts beyond the current 60-second limit.\n\nWhile no immediate changes were announced, Voolich expressed interest in creator feedback on ideal video lengths.\n\nMore Creators Earning Money\n\nYouTube is celebrating the first anniversary of its Partner Program for Shorts, with Voolich noting that 25% of YPP creators are now earning money through the format.\n\nWhy Does This Matter?\n\nYouTube’s new Shorts features matter because:\n\nMore people can make videos : The new tools make it easier for anyone to create Shorts.\n\n: The new tools make it easier for anyone to create Shorts. Keeping up with TikTok : YouTube is trying to make Shorts popular by offering features similar to other short video apps.\n\n: YouTube is trying to make Shorts popular by offering features similar to other short video apps. Money for creators : More ways to make Shorts could mean more chances for creators to earn money.\n\n: More ways to make Shorts could mean more chances for creators to earn money. Fun for viewers : New features like stickers and effects make watching Shorts more entertaining and interactive.\n\n: New features like stickers and effects make watching Shorts more entertaining and interactive. Easier to understand: Better caption options help people enjoy videos even without sound.\n\nThese changes show that YouTube is paying attention to what people want in short videos.\n\nFeatured Image: Prathmesh T/Shutterstock"
    },
    {
        "title": "Google Renders All Pages For Search, Including JavaScript-Heavy Sites",
        "url": "https://www.searchenginejournal.com/google-renders-all-pages-for-search-including-javascript-heavy-sites/522103/",
        "content": "In a recent episode of Google’s “Search Off The Record” podcast, Zoe Clifford from the rendering team joined Martin Splitt and John Mueller from Search Relations to discuss how Google handles JavaScript-heavy websites.\n\nGoogle affirms that it renders all websites in its search results, even if those sites rely on JavaScript.\n\nRendering Process Explained\n\nIn the context of Google Search, Clifford explained that rendering involves using a headless browser to process web pages.\n\nThis allows Google to index the content as a user would see it after JavaScript has executed and the page has fully loaded.\n\nClifford stated\n\n“We run a browser in the indexing pipeline so we can index the view of the web page as a user would see it after it has loaded and JavaScript has executed.”\n\nAll HTML Pages Rendered\n\nOne of the podcast’s most significant revelations was that Google renders all HTML pages, not just a select few. Despite the resource-intensive process, Google has committed to this approach to ensure comprehensive indexing.\n\nClifford confirmed:\n\n“We just render all of them, as long as they’re HTML and not other content types like PDFs.”\n\nShe acknowledged that while the process is expensive, accessing the full content of web pages, especially those relying heavily on JavaScript, is necessary.\n\nContinuous Browser Updates\n\nThe team also discussed Google’s shift to using the “Evergreen Googlebot” in 2019.\n\nThis update ensures that Googlebot, Google’s web crawling bot, stays current with the latest stable version of Chrome.\n\nThis change has improved Google’s ability to render and index modern websites.\n\nWhat This Means for Website Owners & Developers\n\nGood news for JavaScript: If your website uses a lot of JavaScript, Google will likely understand it. Speed still matters: Although Google can handle JavaScript better, having a fast-loading website is still important. Keep it simple when you can: While it’s okay to use JavaScript, try not to overdo it. Simpler websites are often easier for both Google and visitors to understand. Check your work: Use Google’s free tools, like Fetch As Google, to ensure search crawlers can render your site. Think about all users: Remember that some people might have slow internet or older devices. Ensure your main content works even if JavaScript doesn’t load perfectly.\n\nRelated:\n\nWrapping Up\n\nGoogle’s ability to handle JavaScript-heavy websites gives developers more freedom. However, it’s still smart to focus on creating fast, easy-to-use websites that work well for everyone.\n\nBy keeping these points in mind, you can keep your website in good shape for both Google and your visitors.\n\nListen to the full podcast episode below:"
    },
    {
        "title": "Google Renders All Pages For Search, Including JavaScript-Heavy Sites",
        "url": "https://www.searchenginejournal.com/google-renders-all-pages-for-search-including-javascript-heavy-sites/522103/",
        "content": "In a recent episode of Google’s “Search Off The Record” podcast, Zoe Clifford from the rendering team joined Martin Splitt and John Mueller from Search Relations to discuss how Google handles JavaScript-heavy websites.\n\nGoogle affirms that it renders all websites in its search results, even if those sites rely on JavaScript.\n\nRendering Process Explained\n\nIn the context of Google Search, Clifford explained that rendering involves using a headless browser to process web pages.\n\nThis allows Google to index the content as a user would see it after JavaScript has executed and the page has fully loaded.\n\nClifford stated\n\n“We run a browser in the indexing pipeline so we can index the view of the web page as a user would see it after it has loaded and JavaScript has executed.”\n\nAll HTML Pages Rendered\n\nOne of the podcast’s most significant revelations was that Google renders all HTML pages, not just a select few. Despite the resource-intensive process, Google has committed to this approach to ensure comprehensive indexing.\n\nClifford confirmed:\n\n“We just render all of them, as long as they’re HTML and not other content types like PDFs.”\n\nShe acknowledged that while the process is expensive, accessing the full content of web pages, especially those relying heavily on JavaScript, is necessary.\n\nContinuous Browser Updates\n\nThe team also discussed Google’s shift to using the “Evergreen Googlebot” in 2019.\n\nThis update ensures that Googlebot, Google’s web crawling bot, stays current with the latest stable version of Chrome.\n\nThis change has improved Google’s ability to render and index modern websites.\n\nWhat This Means for Website Owners & Developers\n\nGood news for JavaScript: If your website uses a lot of JavaScript, Google will likely understand it. Speed still matters: Although Google can handle JavaScript better, having a fast-loading website is still important. Keep it simple when you can: While it’s okay to use JavaScript, try not to overdo it. Simpler websites are often easier for both Google and visitors to understand. Check your work: Use Google’s free tools, like Fetch As Google, to ensure search crawlers can render your site. Think about all users: Remember that some people might have slow internet or older devices. Ensure your main content works even if JavaScript doesn’t load perfectly.\n\nRelated:\n\nWrapping Up\n\nGoogle’s ability to handle JavaScript-heavy websites gives developers more freedom. However, it’s still smart to focus on creating fast, easy-to-use websites that work well for everyone.\n\nBy keeping these points in mind, you can keep your website in good shape for both Google and your visitors.\n\nListen to the full podcast episode below:"
    },
    {
        "title": "Google Simplifies Adding Shipping & Return Policies For Online Stores",
        "url": "https://www.searchenginejournal.com/google-simplifies-adding-shipping-return-policies-for-online-stores/521923/",
        "content": "Google has announced a new feature allowing online stores to manage their shipping and return policies through Search Console.\n\nThis simplifies providing customers with vital information, potentially boosting retailers’ visibility and sales.\n\nWe're happy to announce an easier way for merchants to add their shipping and return information directly in Search Console. Check out our blog to learn more about it https://t.co/mPGhxHCgfR pic.twitter.com/9sp19F7JHb — Google Search Central (@googlesearchc) July 11, 2024\n\nAddressing Consumer Priorities\n\nRecent trends indicate that shoppers prioritize total price transparency when making online purchases. Shipping costs, delivery speed, and return policies impact consumer decision-making.\n\nRecognizing this, Google has made it easier for stores to communicate these details.\n\nNew Search Console Integration\n\nConfiguring shipping and return information in Google Search Console eliminates the need for complex markup on individual product pages.\n\nThis may be a boon to merchants who found the previous setup challenging.\n\nKey Features:\n\nDirect input of shipping and return policies in Search Console Automatic synchronization with Merchant Center accounts Option to set general return policies for entire businesses Precedence over website configurations and product-level markup\n\nPotential Benefits For Merchants\n\nOnline stores can enhance their product listings in Google Search by providing clear, up-to-date shipping and return information.\n\nThis improvement may increase visibility and traffic, particularly by highlighting services like free and fast shipping options alongside product information.\n\nHow To Implement\n\nGoogle has provided a straightforward process for adding and managing their shipping and return policies through Search Console.\n\nHere’s a step-by-step guide:\n\n1. Accessing The Feature\n\nOpen the Settings page for your Search Console property.\n\nNavigate to the “Shopping” section and select “Shipping and returns”.\n\n2. Adding Shipping Policies\n\nClick on the “Shipping” tab and select “Add shipping policy”.\n\nChoose the country the policy applies to.\n\nSet delivery times by specifying minimum and maximum days for delivery.\n\nConfigure shipping costs. Options include: Flat rate (with currency matching the selected country) Free shipping over a certain purchase amount Free shipping for all orders Other custom options\n\nSubmit the policy.\n\nRepeat for each country you ship to.\n\n3. Adding Return Policies\n\nSelect the “Returns” tab and click “Add return policy”.\n\nChoose the applicable country.\n\nEnter the URL of your return policy page.\n\nSpecify the return window (number of days allowed for returns or no returns).\n\nSet the return cost: Flat rate (matching the country’s currency) Free returns Percentage of the product price\n\nSubmit the policy.\n\nRepeat for each country where you offer returns.\n\n4. Editing Or Deleting Policies\n\nLocate the policy you wish to modify.\n\nClick the “more” icon next to the policy.\n\nChoose “Edit” to make changes or “Delete” to remove the policy.\n\nFollow the prompts to complete the action.\n\n5. Important Notes\n\nShipping details are automatically approved upon submission.\n\nReturn policies require manual verification, which can take 10-13 days.\n\nNewly submitted return policies will be marked as “pending” until verified.\n\nOnly owners or full users of the Search Console property associated with a Merchant Center account can access these settings.\n\nPolicies created in Merchant Center cannot be managed in Search Console.\n\nWhat Does This Mean?\n\nGoogle’s new tool for showing shipping and return info could affect the ecommerce experience in several ways.\n\nFor shoppers:\n\nEasier to see shipping costs and return policies right in search results\n\nSaves time comparing different stores\n\nBuilds trust by being upfront about costs and policies\n\nFor sellers:\n\nSmaller shops can now compete better with big ones\n\nStores with good shipping deals might stand out in searches\n\nIt’s now easier for all shops to share this info, not just tech-savvy ones\n\nFeatured Image: olgsera/Shutterstock"
    },
    {
        "title": "Google Simplifies Adding Shipping & Return Policies For Online Stores",
        "url": "https://www.searchenginejournal.com/google-simplifies-adding-shipping-return-policies-for-online-stores/521923/",
        "content": "Google has announced a new feature allowing online stores to manage their shipping and return policies through Search Console.\n\nThis simplifies providing customers with vital information, potentially boosting retailers’ visibility and sales.\n\nWe're happy to announce an easier way for merchants to add their shipping and return information directly in Search Console. Check out our blog to learn more about it https://t.co/mPGhxHCgfR pic.twitter.com/9sp19F7JHb — Google Search Central (@googlesearchc) July 11, 2024\n\nAddressing Consumer Priorities\n\nRecent trends indicate that shoppers prioritize total price transparency when making online purchases. Shipping costs, delivery speed, and return policies impact consumer decision-making.\n\nRecognizing this, Google has made it easier for stores to communicate these details.\n\nNew Search Console Integration\n\nConfiguring shipping and return information in Google Search Console eliminates the need for complex markup on individual product pages.\n\nThis may be a boon to merchants who found the previous setup challenging.\n\nKey Features:\n\nDirect input of shipping and return policies in Search Console Automatic synchronization with Merchant Center accounts Option to set general return policies for entire businesses Precedence over website configurations and product-level markup\n\nPotential Benefits For Merchants\n\nOnline stores can enhance their product listings in Google Search by providing clear, up-to-date shipping and return information.\n\nThis improvement may increase visibility and traffic, particularly by highlighting services like free and fast shipping options alongside product information.\n\nHow To Implement\n\nGoogle has provided a straightforward process for adding and managing their shipping and return policies through Search Console.\n\nHere’s a step-by-step guide:\n\n1. Accessing The Feature\n\nOpen the Settings page for your Search Console property.\n\nNavigate to the “Shopping” section and select “Shipping and returns”.\n\n2. Adding Shipping Policies\n\nClick on the “Shipping” tab and select “Add shipping policy”.\n\nChoose the country the policy applies to.\n\nSet delivery times by specifying minimum and maximum days for delivery.\n\nConfigure shipping costs. Options include: Flat rate (with currency matching the selected country) Free shipping over a certain purchase amount Free shipping for all orders Other custom options\n\nSubmit the policy.\n\nRepeat for each country you ship to.\n\n3. Adding Return Policies\n\nSelect the “Returns” tab and click “Add return policy”.\n\nChoose the applicable country.\n\nEnter the URL of your return policy page.\n\nSpecify the return window (number of days allowed for returns or no returns).\n\nSet the return cost: Flat rate (matching the country’s currency) Free returns Percentage of the product price\n\nSubmit the policy.\n\nRepeat for each country where you offer returns.\n\n4. Editing Or Deleting Policies\n\nLocate the policy you wish to modify.\n\nClick the “more” icon next to the policy.\n\nChoose “Edit” to make changes or “Delete” to remove the policy.\n\nFollow the prompts to complete the action.\n\n5. Important Notes\n\nShipping details are automatically approved upon submission.\n\nReturn policies require manual verification, which can take 10-13 days.\n\nNewly submitted return policies will be marked as “pending” until verified.\n\nOnly owners or full users of the Search Console property associated with a Merchant Center account can access these settings.\n\nPolicies created in Merchant Center cannot be managed in Search Console.\n\nWhat Does This Mean?\n\nGoogle’s new tool for showing shipping and return info could affect the ecommerce experience in several ways.\n\nFor shoppers:\n\nEasier to see shipping costs and return policies right in search results\n\nSaves time comparing different stores\n\nBuilds trust by being upfront about costs and policies\n\nFor sellers:\n\nSmaller shops can now compete better with big ones\n\nStores with good shipping deals might stand out in searches\n\nIt’s now easier for all shops to share this info, not just tech-savvy ones\n\nFeatured Image: olgsera/Shutterstock"
    },
    {
        "title": "Google’s Now Translating SERPs Into More Languages",
        "url": "https://www.searchenginejournal.com/googles-now-translating-serps-into-more-languages/521843/",
        "content": "Google updated their documentation to reflect that it added eight new languages to its translated results feature, broadening the reach of publishers to an increasingly global scale, with automatic translations to a site visitor’s native language.\n\nGoogle Translated Results\n\nTranslated Results is a Google Search feature that will automatically translate the title link and meta description into the local language of a user, making a website published in one language available to a searcher in another language. If the searcher clicks on the link of a translated result the web page itself will also be automatically translated.\n\nAccording to Google’s documentation for this feature:\n\n“Google doesn’t host any translated pages. Opening a page through a translated result is no different than opening the original search result through Google Translate or using Chrome in-browser translation. This means that JavaScript on the page is usually supported, as well as embedded images and other page features.”\n\nThis feature benefits publishers because it makes their website available to a larger audience.\n\nSearch Feature Available In More Languages\n\nGoogle’s documentation for this feature was updated to reflect that it is now available in eight more languages.\n\nUsers who speak the following languages will now have automatic access to a broader range of websites.\n\nList Of Added Languages\n\nArabic\n\nGujarati\n\nKorean\n\nPersian\n\nThai\n\nTurkish\n\nUrdu\n\nVietnamese\n\nWhy Did It Take So Long?\n\nIt seems odd that Google didn’t already translate results into so many major languages like Turkish, Arabic or Korean. So I asked international SEO expert Christopher Shin (LinkedIn profile) about why it might have taken so long for Google to do this in the Korean language.\n\nChristopher shared:\n\nGoogle was always facing difficulties in the South Korean market as a search engine, and that has to do mainly with Naver and Kakao, formerly known as Daum. But the whole paradigm shift to Google began when more and more students that went abroad to where Google is the dominant search engine came back to South Korea. When more and more students, travelers abroad etc., returned to Korea, they started to realize the strengths and weaknesses of the local search portals and the information capabilities these local portals provided. Laterally, more and more businesses in South Korea like Samsung, Hyundai etc., started to also shift marketing and sales to global markets, so the importance of Google as a tool for companies was also becoming more important with the domestic population. Naver is still the dominant search portal, but not to retrieve answers to specific queries, rather for the purpose of shopping, reviews etc. So I believe that market prioritization may be a big part as to the delayed introduction of Translated Google Search Results. And in terms of numbers, Korea is smaller with only roughly 52M nationwide and continues to decline due to poor birth rates. Another big factor as I see it, has to do with the complexity of the Korean language which would make it more challenging to build out a translation tool that only replicates a simple English version. We use the modern Korean Hangeul but also the country uses Hanja, which are words from the Chinese origin. I used to have my team use Google Translate until all of them complained that Naver’s Papago does a better job, but with the introduction of ChatGPT, the competitiveness offered by Google was slim.”\n\nTakeaway\n\nIt’s not an understatement to say that 2024 has not been a good year for publishers, from the introduction of AI Overviews to the 2024 Core Algorithm Update, and missing image thumbnails on recipe blogger sites, there hasn’t been much good news coming out of Google. But this news is different because it creates the opportunity for publisher content to be shown in even more languages than ever.\n\nRead the updated documentation here:\n\nTranslated results in Google Search\n\nFeatured Image by Shutterstock/baranq"
    },
    {
        "title": "Google’s Now Translating SERPs Into More Languages",
        "url": "https://www.searchenginejournal.com/googles-now-translating-serps-into-more-languages/521843/",
        "content": "Google updated their documentation to reflect that it added eight new languages to its translated results feature, broadening the reach of publishers to an increasingly global scale, with automatic translations to a site visitor’s native language.\n\nGoogle Translated Results\n\nTranslated Results is a Google Search feature that will automatically translate the title link and meta description into the local language of a user, making a website published in one language available to a searcher in another language. If the searcher clicks on the link of a translated result the web page itself will also be automatically translated.\n\nAccording to Google’s documentation for this feature:\n\n“Google doesn’t host any translated pages. Opening a page through a translated result is no different than opening the original search result through Google Translate or using Chrome in-browser translation. This means that JavaScript on the page is usually supported, as well as embedded images and other page features.”\n\nThis feature benefits publishers because it makes their website available to a larger audience.\n\nSearch Feature Available In More Languages\n\nGoogle’s documentation for this feature was updated to reflect that it is now available in eight more languages.\n\nUsers who speak the following languages will now have automatic access to a broader range of websites.\n\nList Of Added Languages\n\nArabic\n\nGujarati\n\nKorean\n\nPersian\n\nThai\n\nTurkish\n\nUrdu\n\nVietnamese\n\nWhy Did It Take So Long?\n\nIt seems odd that Google didn’t already translate results into so many major languages like Turkish, Arabic or Korean. So I asked international SEO expert Christopher Shin (LinkedIn profile) about why it might have taken so long for Google to do this in the Korean language.\n\nChristopher shared:\n\nGoogle was always facing difficulties in the South Korean market as a search engine, and that has to do mainly with Naver and Kakao, formerly known as Daum. But the whole paradigm shift to Google began when more and more students that went abroad to where Google is the dominant search engine came back to South Korea. When more and more students, travelers abroad etc., returned to Korea, they started to realize the strengths and weaknesses of the local search portals and the information capabilities these local portals provided. Laterally, more and more businesses in South Korea like Samsung, Hyundai etc., started to also shift marketing and sales to global markets, so the importance of Google as a tool for companies was also becoming more important with the domestic population. Naver is still the dominant search portal, but not to retrieve answers to specific queries, rather for the purpose of shopping, reviews etc. So I believe that market prioritization may be a big part as to the delayed introduction of Translated Google Search Results. And in terms of numbers, Korea is smaller with only roughly 52M nationwide and continues to decline due to poor birth rates. Another big factor as I see it, has to do with the complexity of the Korean language which would make it more challenging to build out a translation tool that only replicates a simple English version. We use the modern Korean Hangeul but also the country uses Hanja, which are words from the Chinese origin. I used to have my team use Google Translate until all of them complained that Naver’s Papago does a better job, but with the introduction of ChatGPT, the competitiveness offered by Google was slim.”\n\nTakeaway\n\nIt’s not an understatement to say that 2024 has not been a good year for publishers, from the introduction of AI Overviews to the 2024 Core Algorithm Update, and missing image thumbnails on recipe blogger sites, there hasn’t been much good news coming out of Google. But this news is different because it creates the opportunity for publisher content to be shown in even more languages than ever.\n\nRead the updated documentation here:\n\nTranslated results in Google Search\n\nFeatured Image by Shutterstock/baranq"
    },
    {
        "title": "Google’s Response To Experts Outranked By Redditors",
        "url": "https://www.searchenginejournal.com/googles-response-to-experts-outranked-by-redditors/521801/",
        "content": "An SEO asked on LinkedIn why an anonymous user on Reddit could outrank a credible website with a named author. Google’s answer gives a peek at what’s going on with search rankings and why Reddit can outrank expert articles.\n\nWhy Do Anonymous Redditors Outrank Experts?\n\nThe person asking the question wanted to know why an anonymous author on Reddit can outrank an actual author that has “credibility” such as in a brand name site like PCMag.\n\nThe person wrote:\n\n“I was referring to how important the credibility of the writer is now. If we search for ‘best product under X amount,’ we see, let’s say, PCMag and Reddit both on the first page. PCMag is a reliable source for that product, while Reddit has UGC and surely doesn’t guarantee authenticity. Where do you see this in terms of credibility? In my opinion, Google must be focusing more on this, especially after the AI boom, where misinformation can be easily and massively spread. Do you think this is an important factor in rankings anymore?”\n\nThis is their question that points out what the SEO feels is wrong with Google’s search results:\n\n“As we can see, Reddit, popular for anonymous use, ranks much higher than many other websites. This means that content from anonymous users is acceptable. Can I conclude that a blog without any ‘about’ page or ‘author profile’ can also perform as well?”\n\nRelevance And Usefulness Versus Credibility\n\nGoogle’s John Mueller answered the question by pointing out that there are multiple kinds of websites, not just sites that are perceived to be credible and everyone else. The idea of credibility is one dimension of what a site can be, one quality of a website. Mueller’s answer reminded that search (and SEO) is multidimensional.\n\nGoogle’s John Mueller answered:\n\n“Both are websites, but also, they’re quite different, right? Finding the right tools for your needs, the right platforms for your audience and for your messages – it’s worth looking at more than just a simplification like that. Google aims to provide search results that are relevant & useful for users, and there’s a lot involved in that. I feel this might fit, perhaps you have seen it before -“\n\nDoes Reddit Lack Credibility?\n\nWhen it comes to recipes , in my opinion, Reddit users lack a lot of credibility in some contexts. When it comes to recipes, I’ll take the opinions of a recipe blogger or Serious Eats over what a random Redditor “thinks” a recipe should be.\n\nThe person asking the question mentioned product reviews as a topic that Reddit fails at credibility and ironically that’s actually a topic where Reddit actually shines. A person on Reddit who is sharing their hands-on experience using a brand of air fryer or mobile phone is the epitome of what Google is trying to rank for reviews because it’s the opinion of someone with days, weeks, months, years of actual experience with a product.\n\nSaying that UGC product reviews are useful doesn’t invalidate the professional product reviews. It’s possible that both UGC and professional reviews have value, right? And I think that’s the point that John Mueller was trying to get across about not simplifying search to one ranking criteria, one dimension.\n\nThis a dimension of search that the person asking the question overlooked, the hands-on experience of the reviewer and it illustrates what Mueller means when he says that “it’s worth looking at more than just a simplification” of what’s ranking in the search results.\n\nOTOH… Feels Like A Slap In The Face\n\nThere are many high quality sites with original photos, actual reviews and content based on real experience that are no longer ranking the search results. I know because I have seen many of these sites that in my opinion should be ranking but are not. Googlers have expressed the possibility that a future update will help more quality sites bounce back and many expert publishers are counting on that.\n\nNevertheless, it must be acknowledged that it must feel like a slap in the face for an expert author to see an anonymous Redditor outranking them in Google’s search results.\n\nMultidimensional Approach To SEO\n\nA common issue I see in how some digital marketers and bloggers debug the search engine results pages (SERPs) is that they see it through one, two, or three dimensions such as:\n\nKeywords,\n\nExpertise\n\nCredibility\n\nLinks\n\nReviewing the SERPs to understand why Google is ranking something is a good idea. But reviewing it with just a handful of dimensions, a limited amount of “signals” can be frustrating and counterproductive.\n\nIt was only a few years ago that SEOs convinced themselves that “author signals” were a critical part of ranking and now almost everyone (finally) understands that this was all a misinterpretation of what Google and Googlers said (despite the Googlers consistently denying that authorship was a ranking signal).\n\nThe “authorship” SEO trend is an example of a one dimensional approach to SEO that overlooked the multidimensional quality of how Google ranks web pages.\n\nThere are thousands of contexts that contribute to what is ranked, like solving a problem from the user perspective, interpreting user needs, adapting to cultural and language nuances, nationwide trends, local trends, and so on. There are also ranking contexts (dimensions) that are related to Google’s Core Topicality Systems which are used to understand search queries and web pages.\n\nRanking web pages, from Google’s perspective, is a multidimensional problem. What that means is that reducing a search ranking problem to one dimension, like the anonymity of User Generated Content, inevitably leads to frustration. Broadening the perspective leads to better SEO.\n\nRead the discussion on LinkedIn:\n\nCan I conclude that a blog without any ‘about’ page or ‘author profile’ can also perform as well?\n\nFeatured Image by Shutterstock/Master1305"
    },
    {
        "title": "Google’s Response To Experts Outranked By Redditors",
        "url": "https://www.searchenginejournal.com/googles-response-to-experts-outranked-by-redditors/521801/",
        "content": "An SEO asked on LinkedIn why an anonymous user on Reddit could outrank a credible website with a named author. Google’s answer gives a peek at what’s going on with search rankings and why Reddit can outrank expert articles.\n\nWhy Do Anonymous Redditors Outrank Experts?\n\nThe person asking the question wanted to know why an anonymous author on Reddit can outrank an actual author that has “credibility” such as in a brand name site like PCMag.\n\nThe person wrote:\n\n“I was referring to how important the credibility of the writer is now. If we search for ‘best product under X amount,’ we see, let’s say, PCMag and Reddit both on the first page. PCMag is a reliable source for that product, while Reddit has UGC and surely doesn’t guarantee authenticity. Where do you see this in terms of credibility? In my opinion, Google must be focusing more on this, especially after the AI boom, where misinformation can be easily and massively spread. Do you think this is an important factor in rankings anymore?”\n\nThis is their question that points out what the SEO feels is wrong with Google’s search results:\n\n“As we can see, Reddit, popular for anonymous use, ranks much higher than many other websites. This means that content from anonymous users is acceptable. Can I conclude that a blog without any ‘about’ page or ‘author profile’ can also perform as well?”\n\nRelevance And Usefulness Versus Credibility\n\nGoogle’s John Mueller answered the question by pointing out that there are multiple kinds of websites, not just sites that are perceived to be credible and everyone else. The idea of credibility is one dimension of what a site can be, one quality of a website. Mueller’s answer reminded that search (and SEO) is multidimensional.\n\nGoogle’s John Mueller answered:\n\n“Both are websites, but also, they’re quite different, right? Finding the right tools for your needs, the right platforms for your audience and for your messages – it’s worth looking at more than just a simplification like that. Google aims to provide search results that are relevant & useful for users, and there’s a lot involved in that. I feel this might fit, perhaps you have seen it before -“\n\nDoes Reddit Lack Credibility?\n\nWhen it comes to recipes , in my opinion, Reddit users lack a lot of credibility in some contexts. When it comes to recipes, I’ll take the opinions of a recipe blogger or Serious Eats over what a random Redditor “thinks” a recipe should be.\n\nThe person asking the question mentioned product reviews as a topic that Reddit fails at credibility and ironically that’s actually a topic where Reddit actually shines. A person on Reddit who is sharing their hands-on experience using a brand of air fryer or mobile phone is the epitome of what Google is trying to rank for reviews because it’s the opinion of someone with days, weeks, months, years of actual experience with a product.\n\nSaying that UGC product reviews are useful doesn’t invalidate the professional product reviews. It’s possible that both UGC and professional reviews have value, right? And I think that’s the point that John Mueller was trying to get across about not simplifying search to one ranking criteria, one dimension.\n\nThis a dimension of search that the person asking the question overlooked, the hands-on experience of the reviewer and it illustrates what Mueller means when he says that “it’s worth looking at more than just a simplification” of what’s ranking in the search results.\n\nOTOH… Feels Like A Slap In The Face\n\nThere are many high quality sites with original photos, actual reviews and content based on real experience that are no longer ranking the search results. I know because I have seen many of these sites that in my opinion should be ranking but are not. Googlers have expressed the possibility that a future update will help more quality sites bounce back and many expert publishers are counting on that.\n\nNevertheless, it must be acknowledged that it must feel like a slap in the face for an expert author to see an anonymous Redditor outranking them in Google’s search results.\n\nMultidimensional Approach To SEO\n\nA common issue I see in how some digital marketers and bloggers debug the search engine results pages (SERPs) is that they see it through one, two, or three dimensions such as:\n\nKeywords,\n\nExpertise\n\nCredibility\n\nLinks\n\nReviewing the SERPs to understand why Google is ranking something is a good idea. But reviewing it with just a handful of dimensions, a limited amount of “signals” can be frustrating and counterproductive.\n\nIt was only a few years ago that SEOs convinced themselves that “author signals” were a critical part of ranking and now almost everyone (finally) understands that this was all a misinterpretation of what Google and Googlers said (despite the Googlers consistently denying that authorship was a ranking signal).\n\nThe “authorship” SEO trend is an example of a one dimensional approach to SEO that overlooked the multidimensional quality of how Google ranks web pages.\n\nThere are thousands of contexts that contribute to what is ranked, like solving a problem from the user perspective, interpreting user needs, adapting to cultural and language nuances, nationwide trends, local trends, and so on. There are also ranking contexts (dimensions) that are related to Google’s Core Topicality Systems which are used to understand search queries and web pages.\n\nRanking web pages, from Google’s perspective, is a multidimensional problem. What that means is that reducing a search ranking problem to one dimension, like the anonymity of User Generated Content, inevitably leads to frustration. Broadening the perspective leads to better SEO.\n\nRead the discussion on LinkedIn:\n\nCan I conclude that a blog without any ‘about’ page or ‘author profile’ can also perform as well?\n\nFeatured Image by Shutterstock/Master1305"
    },
    {
        "title": "New YouTube Feature Aims To Spark Viral Trends In Shorts",
        "url": "https://www.searchenginejournal.com/new-youtube-feature-aims-to-spark-viral-trends-in-shorts/521710/",
        "content": "YouTube is rolling out an experimental feature to boost trend participation among YouTube Shorts creators.\n\nThe new “Add Yours” sticker, currently being tested with select channels, provides a way to initiate and join trending topics.\n\nYouTube announced the feature in its weekly news update for video creators.\n\nThe experiment introduces a novel way to prompt responses from your audience, potentially catalyzing viral trends and challenges.\n\nHow It Works\n\nCreators in the test group will find an “Add yours” option in the sticker picker when creating a Short. After recording a Short, tap “Next,” then the sticker icon, and select “Add yours.” Type a prompt for other users to respond to. The sticker will appear as an overlay on the published Short.\n\nViewer Interaction\n\nViewers watching Shorts with the “Add yours” sticker can participate by clicking on the sticker and creating a response video.\n\nChannels can track engagement by tapping on the sticker. This lets you see all the Shorts created responding to your prompt, providing potential feedback and insights.\n\nWhat Does This Mean For You?\n\nTo illustrate the potential of YouTube’s new “Add Yours” sticker for Shorts, let’s consider how a creator in the SEO community might use this feature.\n\nExample: “30-Second SEO Tip”\n\nInitial Short : Demonstrates a quick SEO trick, like using Google Trends for keyword research. Sticker : “Share your best 30-second SEO tip!” Caption : “Boost your SEO game! What’s your quickest optimization hack? #30SecSEOTip ” Result : Other creators contribute their SEO tips.\n\n: Demonstrates a quick SEO trick, like using Google Trends for keyword research.\n\nThis concise format encourages participation, provides quick value, and can potentially grow your YouTube audience.\n\nLimited Rollout\n\nYouTube is making the feature available only to a small number of Shorts creators on mobile devices.\n\nThis limited release allows YouTube to gather feedback and refine the feature before a potential wider launch.\n\nLooking Ahead\n\nThe introduction of the “Add yours” sticker aligns with YouTube’s efforts to compete in the short-form video market, dominated by platforms like TikTok and Instagram Reels.\n\nBy facilitating easier trend creation and participation, YouTube aims to amplify the creation of short videos.\n\nFeatured Image: photosince/shutterstock"
    },
    {
        "title": "New YouTube Feature Aims To Spark Viral Trends In Shorts",
        "url": "https://www.searchenginejournal.com/new-youtube-feature-aims-to-spark-viral-trends-in-shorts/521710/",
        "content": "YouTube is rolling out an experimental feature to boost trend participation among YouTube Shorts creators.\n\nThe new “Add Yours” sticker, currently being tested with select channels, provides a way to initiate and join trending topics.\n\nYouTube announced the feature in its weekly news update for video creators.\n\nThe experiment introduces a novel way to prompt responses from your audience, potentially catalyzing viral trends and challenges.\n\nHow It Works\n\nCreators in the test group will find an “Add yours” option in the sticker picker when creating a Short. After recording a Short, tap “Next,” then the sticker icon, and select “Add yours.” Type a prompt for other users to respond to. The sticker will appear as an overlay on the published Short.\n\nViewer Interaction\n\nViewers watching Shorts with the “Add yours” sticker can participate by clicking on the sticker and creating a response video.\n\nChannels can track engagement by tapping on the sticker. This lets you see all the Shorts created responding to your prompt, providing potential feedback and insights.\n\nWhat Does This Mean For You?\n\nTo illustrate the potential of YouTube’s new “Add Yours” sticker for Shorts, let’s consider how a creator in the SEO community might use this feature.\n\nExample: “30-Second SEO Tip”\n\nInitial Short : Demonstrates a quick SEO trick, like using Google Trends for keyword research. Sticker : “Share your best 30-second SEO tip!” Caption : “Boost your SEO game! What’s your quickest optimization hack? #30SecSEOTip ” Result : Other creators contribute their SEO tips.\n\n: Demonstrates a quick SEO trick, like using Google Trends for keyword research.\n\nThis concise format encourages participation, provides quick value, and can potentially grow your YouTube audience.\n\nLimited Rollout\n\nYouTube is making the feature available only to a small number of Shorts creators on mobile devices.\n\nThis limited release allows YouTube to gather feedback and refine the feature before a potential wider launch.\n\nLooking Ahead\n\nThe introduction of the “Add yours” sticker aligns with YouTube’s efforts to compete in the short-form video market, dominated by platforms like TikTok and Instagram Reels.\n\nBy facilitating easier trend creation and participation, YouTube aims to amplify the creation of short videos.\n\nFeatured Image: photosince/shutterstock"
    },
    {
        "title": "Google Says These Are Not Good Signals",
        "url": "https://www.searchenginejournal.com/google-says-these-are-not-good-signals/521697/",
        "content": "Google’s Gary Illyes’ answer about authorship shared insights about why Google has less trust for signals that are under direct control of site owners and SEOs and provides a better understanding about what site owners and SEOs should focus on when optimizing a website.\n\nThe question that Illyes answered was in the context of a live interview at a search conference in May 2024. The interview went largely unnoticed but it’s full of great information related to digital marketing and how Google ranks web pages.\n\nAuthorship Signals\n\nSomeone asked the question about whether Google would bring back authorship signals. Authorship has been a fixation by some SEOs based on Google’s encouragement that SEOs and site owners review the Search Quality Raters Guidelines to understand what Google aspires to rank. SEOs however took the encouragement too literally and started to parse the document for ranking signal ideas instead.\n\nDigital marketers came to see the concept of EEAT (Expertise, Experience, Authoritativeness, and Trustworthiness) as actual signals that Google’s algorithms were looking for and from there came the idea that authorship signals were important for ranking.\n\nThe idea of authorship signals is not far-fetched because Google at one time created a way for site owners and SEOs pass along metadata about webpage authorship but Google eventually abandoned that idea.\n\nSEO-Controlled Markup Is Untrustworthy\n\nGoogle’s Gary Illyes answered the question about authorship signals and very quickly, within the same sentence, shared that Google’s experience with SEO-controlled data on the web page (markup) tends to become spammy (implying that it’s untrustworthy).\n\nThis is the question as relayed by the interviewer:\n\n“Are Google planning to release some authorship sooner or later, something that goes back to that old authorship?”\n\nGoogle’s Gary Illyes answered:\n\n“Uhm… I don’t know of such plans and honestly I’m not very excited about anything along those lines, especially not one that is similar to what we had back in 2011 to 2013 because pretty much any markup that SEOs and site owners have access to will be in some form spam.”\n\nGary next went into greater detail by saying that SEO and author controlled markup are not good signals.\n\nHere is how he explained it:\n\n“And generally they are not good signals. That’s why rel-canonical, for example is not a directive but a hint. And that’s why Meta description is not a directive, but something that we might consider and so on. Having something similar for authorship, I think would be a mistake.”\n\nThe concept of SEO-controlled data not being a good signal is important to understand because many in search marketing believe that they can manipulate Google by spoofing authorship signals with fake author profiles, with reviews that pretend to be hands-on, and with metadata (like titles and meta descriptions) that is specifically crafted to rank for keywords.\n\nWhat About Algorithmically Determined Authorship?\n\nGary then turned to the idea of algorithmically determined authorship signals and it may surprise some that Gary describes those siganls as lacking in value. This may come as a blow to SEOs and site owners who have spent significant amounts of time updating their web pages to improve their authorship data.\n\nThe concept of the importance of “authorship signals” for ranking is something that some SEOs created all by themselves, it’s not an idea that Google encouraged. In fact, Googlers like John Mueller and SearchLiaison have consistently downplayed the necessity of author profiles for years.\n\nGary explained about algorithmically determined authorship signals:\n\n“Having something similar for authorship, I think would be a mistake. If it’s algorithmically determined, then perhaps it would be more accurate or could be higher accuracy, but honestly I don’t necessarily see the value in it.”\n\nThe interviewer commented about rel-canonicals sometimes being a poor source of information:\n\n“I’ve seen canonical done badly a lot of times myself, so I’m glad to hear that it is only a suggestion rather than a rule.”\n\nGary’s response to the observation about poor canonicals is interesting because he doesn’t downplay the importance of “suggestions” but implies that some of them are stronger although still falling short of a directive. A directive is something that Google is obligated to obey, like a noindex meta tag.\n\nGary explained about rel-canonicals being a strong suggestion:\n\n“I mean it’s it’s a strong suggestion, but still it’s a suggestion.”\n\nGary affirmed that even though rel=canonicals is a suggestion, it’s a strong suggestion. That implies a relative scale of how much Google trusts certain inputs that publishers make. In the case of a canonical, Google’s stronger trust in rel-canonical is probably a reflection of the fact that it’s in a publisher’s best interest to get it right, whereas other data like authorship could be prone to exaggeration or outright deception and therefore less trustworthy.\n\nWhat Does It All Mean?\n\nGary’s comments should give a foundation for setting the correct course on what to focus on when optimizing a web page. Gary (and other Googlers) have said multiple times that authorship is not really something that Google is looking for. That’s something that SEOs invented, not something that Google encouraged.\n\nThis also provides guidance on not overestimating the importance of metadata that is controlled by a site owner or SEO.\n\nWatch the interview starting at about the two minute mark:\n\nFeatured Image by Shutterstock/Asier Romero"
    },
    {
        "title": "Google Says These Are Not Good Signals",
        "url": "https://www.searchenginejournal.com/google-says-these-are-not-good-signals/521697/",
        "content": "Google’s Gary Illyes’ answer about authorship shared insights about why Google has less trust for signals that are under direct control of site owners and SEOs and provides a better understanding about what site owners and SEOs should focus on when optimizing a website.\n\nThe question that Illyes answered was in the context of a live interview at a search conference in May 2024. The interview went largely unnoticed but it’s full of great information related to digital marketing and how Google ranks web pages.\n\nAuthorship Signals\n\nSomeone asked the question about whether Google would bring back authorship signals. Authorship has been a fixation by some SEOs based on Google’s encouragement that SEOs and site owners review the Search Quality Raters Guidelines to understand what Google aspires to rank. SEOs however took the encouragement too literally and started to parse the document for ranking signal ideas instead.\n\nDigital marketers came to see the concept of EEAT (Expertise, Experience, Authoritativeness, and Trustworthiness) as actual signals that Google’s algorithms were looking for and from there came the idea that authorship signals were important for ranking.\n\nThe idea of authorship signals is not far-fetched because Google at one time created a way for site owners and SEOs pass along metadata about webpage authorship but Google eventually abandoned that idea.\n\nSEO-Controlled Markup Is Untrustworthy\n\nGoogle’s Gary Illyes answered the question about authorship signals and very quickly, within the same sentence, shared that Google’s experience with SEO-controlled data on the web page (markup) tends to become spammy (implying that it’s untrustworthy).\n\nThis is the question as relayed by the interviewer:\n\n“Are Google planning to release some authorship sooner or later, something that goes back to that old authorship?”\n\nGoogle’s Gary Illyes answered:\n\n“Uhm… I don’t know of such plans and honestly I’m not very excited about anything along those lines, especially not one that is similar to what we had back in 2011 to 2013 because pretty much any markup that SEOs and site owners have access to will be in some form spam.”\n\nGary next went into greater detail by saying that SEO and author controlled markup are not good signals.\n\nHere is how he explained it:\n\n“And generally they are not good signals. That’s why rel-canonical, for example is not a directive but a hint. And that’s why Meta description is not a directive, but something that we might consider and so on. Having something similar for authorship, I think would be a mistake.”\n\nThe concept of SEO-controlled data not being a good signal is important to understand because many in search marketing believe that they can manipulate Google by spoofing authorship signals with fake author profiles, with reviews that pretend to be hands-on, and with metadata (like titles and meta descriptions) that is specifically crafted to rank for keywords.\n\nWhat About Algorithmically Determined Authorship?\n\nGary then turned to the idea of algorithmically determined authorship signals and it may surprise some that Gary describes those siganls as lacking in value. This may come as a blow to SEOs and site owners who have spent significant amounts of time updating their web pages to improve their authorship data.\n\nThe concept of the importance of “authorship signals” for ranking is something that some SEOs created all by themselves, it’s not an idea that Google encouraged. In fact, Googlers like John Mueller and SearchLiaison have consistently downplayed the necessity of author profiles for years.\n\nGary explained about algorithmically determined authorship signals:\n\n“Having something similar for authorship, I think would be a mistake. If it’s algorithmically determined, then perhaps it would be more accurate or could be higher accuracy, but honestly I don’t necessarily see the value in it.”\n\nThe interviewer commented about rel-canonicals sometimes being a poor source of information:\n\n“I’ve seen canonical done badly a lot of times myself, so I’m glad to hear that it is only a suggestion rather than a rule.”\n\nGary’s response to the observation about poor canonicals is interesting because he doesn’t downplay the importance of “suggestions” but implies that some of them are stronger although still falling short of a directive. A directive is something that Google is obligated to obey, like a noindex meta tag.\n\nGary explained about rel-canonicals being a strong suggestion:\n\n“I mean it’s it’s a strong suggestion, but still it’s a suggestion.”\n\nGary affirmed that even though rel=canonicals is a suggestion, it’s a strong suggestion. That implies a relative scale of how much Google trusts certain inputs that publishers make. In the case of a canonical, Google’s stronger trust in rel-canonical is probably a reflection of the fact that it’s in a publisher’s best interest to get it right, whereas other data like authorship could be prone to exaggeration or outright deception and therefore less trustworthy.\n\nWhat Does It All Mean?\n\nGary’s comments should give a foundation for setting the correct course on what to focus on when optimizing a web page. Gary (and other Googlers) have said multiple times that authorship is not really something that Google is looking for. That’s something that SEOs invented, not something that Google encouraged.\n\nThis also provides guidance on not overestimating the importance of metadata that is controlled by a site owner or SEO.\n\nWatch the interview starting at about the two minute mark:\n\nFeatured Image by Shutterstock/Asier Romero"
    },
    {
        "title": "Google Search Now Supports Labeling AI Generated Or Manipulated Images",
        "url": "https://www.searchenginejournal.com/google-search-now-supports-labeling-ai-generated-or-manipulated-images/521675/",
        "content": "Google Search Central updated their documentation to reflect support for labeling images that were extended or manipulated with AI. Google also quietly removed the “AI generated” metadata from Beta status, indicating that the “AI Generated” label is now fully supported in search.\n\nIPTC Photo Metadata\n\nThe International Press Telecommunications Council (IPTC) is a standards making body that among other things creates standards for photo metadata. Photo metadata enables a photograph to be labeled with information about the photo, like information about copyright, licensing and image descriptions.\n\nAlthough the standards is made for by an international press standards organization the meta data standards they curate are used by Google Images in a context outside of Google News. The metadata allows Google Images to show additional information about the image.\n\nGoogle’s documentation explains the use case and benefit of the metadata:\n\n“When you specify image metadata, Google Images can show more details about the image, such as who the creator is, how people can use an image, and credit information. For example, providing licensing information can make the image eligible for the Licensable badge, which provides a link to the license and more detail on how someone can use the image.”\n\nAI Image Manipulation Metadata\n\nGoogle quietly adopted the metadata standards pertaining to images that were manipulated with AI algorithms that are typically used to manipulate images, like convolutional neural networks (CNNs) and generative adversarial networks (GANs).\n\nThere are two forms of AI image manipulation that are covered by the new metadata:\n\nInpainting\n\nOutpainting\n\nInpainting\n\nInpainting is generally conceived as enhancing an image for the purpose of restoring or reconstructing it, to fill in the missing parts. But inpainting is also any algorithm manipulation that adds to an image.\n\nOutpainting\n\nOutpainting is the algorithm process of adding to an image, extending it beyond the borders of the original photograph, adding more to it than what was in the original image.\n\nGoogle now supports labeling images that were manipulated in both those ways with a new metadata property of the Digital Source Type that’s called compositeWithTrainedAlgorithmicMedia.\n\ncompositeWithTrainedAlgorithmicMedia\n\nWhile the new property looks like structured data, it’s not Schema structured data. It’s metadata that’s embedded in a digital image.\n\nThis is what was added to Google’s documentation:\n\n“Digital Source Type compositeWithTrainedAlgorithmicMedia: The image is a composite of trained algorithmic media with some other media, such as with inpainting or outpainting operations.”\n\nLabel For “AI Generated” – algorithmicMedia Metadata\n\nGoogle also lifted the Beta status of the algorithmicMedia metadata specifications, which means that images that are created with AI can now be labeled as AI Generated if the algorithmicMedia metadata is embedded within an image.\n\nThis is the documentation before the change:\n\n“algorithmicMedia: The image was created purely by an algorithm not based on any sampled training data (for example, an image created by software using a mathematical formula). Beta: Currently, this property is in beta and only available for IPTC photo metadata. Adding this property makes your image eligible for display with an AI-generated label, but you may not see the label in Google Images right away, as we’re still actively developing it.”\n\nThe change in the documentation was to remove the entirety of the second paragraph to remove any mention of Beta status. Curiously, this change is not reflected in Google’s changelog.\n\nGoogle’s Search Central documentation changelog noted:\n\n“Supporting a new IPTC digital source type\n\nWhat: Added compositeWithTrainedAlgorithmicMedia to the IPTC photo metadata documentation. Why: Google can now extract the compositeWithTrainedAlgorithmicMedia IPTC NewsCode.”\n\nRead Google’s updated documentation:\n\nImage metadata in Google Images\n\nFeatured Image by Shutterstock/Roman Samborskyi"
    },
    {
        "title": "Google Search Now Supports Labeling AI Generated Or Manipulated Images",
        "url": "https://www.searchenginejournal.com/google-search-now-supports-labeling-ai-generated-or-manipulated-images/521675/",
        "content": "Google Search Central updated their documentation to reflect support for labeling images that were extended or manipulated with AI. Google also quietly removed the “AI generated” metadata from Beta status, indicating that the “AI Generated” label is now fully supported in search.\n\nIPTC Photo Metadata\n\nThe International Press Telecommunications Council (IPTC) is a standards making body that among other things creates standards for photo metadata. Photo metadata enables a photograph to be labeled with information about the photo, like information about copyright, licensing and image descriptions.\n\nAlthough the standards is made for by an international press standards organization the meta data standards they curate are used by Google Images in a context outside of Google News. The metadata allows Google Images to show additional information about the image.\n\nGoogle’s documentation explains the use case and benefit of the metadata:\n\n“When you specify image metadata, Google Images can show more details about the image, such as who the creator is, how people can use an image, and credit information. For example, providing licensing information can make the image eligible for the Licensable badge, which provides a link to the license and more detail on how someone can use the image.”\n\nAI Image Manipulation Metadata\n\nGoogle quietly adopted the metadata standards pertaining to images that were manipulated with AI algorithms that are typically used to manipulate images, like convolutional neural networks (CNNs) and generative adversarial networks (GANs).\n\nThere are two forms of AI image manipulation that are covered by the new metadata:\n\nInpainting\n\nOutpainting\n\nInpainting\n\nInpainting is generally conceived as enhancing an image for the purpose of restoring or reconstructing it, to fill in the missing parts. But inpainting is also any algorithm manipulation that adds to an image.\n\nOutpainting\n\nOutpainting is the algorithm process of adding to an image, extending it beyond the borders of the original photograph, adding more to it than what was in the original image.\n\nGoogle now supports labeling images that were manipulated in both those ways with a new metadata property of the Digital Source Type that’s called compositeWithTrainedAlgorithmicMedia.\n\ncompositeWithTrainedAlgorithmicMedia\n\nWhile the new property looks like structured data, it’s not Schema structured data. It’s metadata that’s embedded in a digital image.\n\nThis is what was added to Google’s documentation:\n\n“Digital Source Type compositeWithTrainedAlgorithmicMedia: The image is a composite of trained algorithmic media with some other media, such as with inpainting or outpainting operations.”\n\nLabel For “AI Generated” – algorithmicMedia Metadata\n\nGoogle also lifted the Beta status of the algorithmicMedia metadata specifications, which means that images that are created with AI can now be labeled as AI Generated if the algorithmicMedia metadata is embedded within an image.\n\nThis is the documentation before the change:\n\n“algorithmicMedia: The image was created purely by an algorithm not based on any sampled training data (for example, an image created by software using a mathematical formula). Beta: Currently, this property is in beta and only available for IPTC photo metadata. Adding this property makes your image eligible for display with an AI-generated label, but you may not see the label in Google Images right away, as we’re still actively developing it.”\n\nThe change in the documentation was to remove the entirety of the second paragraph to remove any mention of Beta status. Curiously, this change is not reflected in Google’s changelog.\n\nGoogle’s Search Central documentation changelog noted:\n\n“Supporting a new IPTC digital source type\n\nWhat: Added compositeWithTrainedAlgorithmicMedia to the IPTC photo metadata documentation. Why: Google can now extract the compositeWithTrainedAlgorithmicMedia IPTC NewsCode.”\n\nRead Google’s updated documentation:\n\nImage metadata in Google Images\n\nFeatured Image by Shutterstock/Roman Samborskyi"
    },
    {
        "title": "Google Struggles To Boost Search Traffic On Its iPhone Apps",
        "url": "https://www.searchenginejournal.com/google-struggles-to-boost-search-traffic-on-its-iphone-apps/521626/",
        "content": "According to a report by The Information, Google is working to reduce its reliance on Apple’s Safari browser, but progress has been slower than anticipated.\n\nAs Google awaits a ruling on the U.S. Department of Justice’s antitrust lawsuit, its arrangement with Apple is threatened.\n\nThe current agreement, which makes Google the default search engine on Safari for iPhones, could be in jeopardy if the judge rules against Google.\n\nTo mitigate this risk, Google encourages iPhone users to switch to its Google Search or Chrome apps for browsing. However, these efforts have yielded limited success.\n\nModest Gains In App Adoption\n\nOver the past five years, Google has increased the percentage of iPhone searches conducted through its apps from 25% to the low 30s.\n\nWhile this represents progress, it falls short of Google’s internal target of 50% by 2030.\n\nThe company has employed various marketing strategies, including campaigns showcasing features like Lens image search and improvements to the Discover feed.\n\nDespite these efforts, Safari’s preinstalled status on iPhones remains an obstacle.\n\nFinancial Stakes & Market Dynamics\n\nThe financial implications of this struggle are considerable for both Google and Apple.\n\nIn 2023, Google reportedly paid over $20 billion to Apple to maintain its status as the default search engine on Safari.\n\nBy shifting more users to its apps, Google aims to reduce these payments and gain leverage in future negotiations.\n\nAntitrust Lawsuit & Potential Consequences\n\nThe ongoing antitrust lawsuit threatens Google’s business model.\n\nIf Google loses the case, it could potentially lose access to approximately 70% of searches conducted on iPhones, which account for about half of the smartphones in the U.S.\n\nThis outcome could impact Google’s mobile search advertising revenue, which exceeded $207 billion in 2023.\n\nNew Initiatives & Leadership\n\nTo address these challenges, Google has brought in new talent, including former Instagram and Yahoo product executive Robby Stein.\n\nStein is now tasked with leading efforts to shift iPhone users to Google’s mobile apps, exploring ways to make the apps more compelling, including the potential use of generative AI.\n\nLooking Ahead\n\nWith the antitrust ruling on the horizon, Google’s ability to attract users to its apps will determine whether it maintains its search market share.\n\nWe’ll be watching closely to see how Google navigates these challenges and if it can reduce its reliance on Safari.\n\nFeatured Image: photosince/shutterstock"
    },
    {
        "title": "Google Struggles To Boost Search Traffic On Its iPhone Apps",
        "url": "https://www.searchenginejournal.com/google-struggles-to-boost-search-traffic-on-its-iphone-apps/521626/",
        "content": "According to a report by The Information, Google is working to reduce its reliance on Apple’s Safari browser, but progress has been slower than anticipated.\n\nAs Google awaits a ruling on the U.S. Department of Justice’s antitrust lawsuit, its arrangement with Apple is threatened.\n\nThe current agreement, which makes Google the default search engine on Safari for iPhones, could be in jeopardy if the judge rules against Google.\n\nTo mitigate this risk, Google encourages iPhone users to switch to its Google Search or Chrome apps for browsing. However, these efforts have yielded limited success.\n\nModest Gains In App Adoption\n\nOver the past five years, Google has increased the percentage of iPhone searches conducted through its apps from 25% to the low 30s.\n\nWhile this represents progress, it falls short of Google’s internal target of 50% by 2030.\n\nThe company has employed various marketing strategies, including campaigns showcasing features like Lens image search and improvements to the Discover feed.\n\nDespite these efforts, Safari’s preinstalled status on iPhones remains an obstacle.\n\nFinancial Stakes & Market Dynamics\n\nThe financial implications of this struggle are considerable for both Google and Apple.\n\nIn 2023, Google reportedly paid over $20 billion to Apple to maintain its status as the default search engine on Safari.\n\nBy shifting more users to its apps, Google aims to reduce these payments and gain leverage in future negotiations.\n\nAntitrust Lawsuit & Potential Consequences\n\nThe ongoing antitrust lawsuit threatens Google’s business model.\n\nIf Google loses the case, it could potentially lose access to approximately 70% of searches conducted on iPhones, which account for about half of the smartphones in the U.S.\n\nThis outcome could impact Google’s mobile search advertising revenue, which exceeded $207 billion in 2023.\n\nNew Initiatives & Leadership\n\nTo address these challenges, Google has brought in new talent, including former Instagram and Yahoo product executive Robby Stein.\n\nStein is now tasked with leading efforts to shift iPhone users to Google’s mobile apps, exploring ways to make the apps more compelling, including the potential use of generative AI.\n\nLooking Ahead\n\nWith the antitrust ruling on the horizon, Google’s ability to attract users to its apps will determine whether it maintains its search market share.\n\nWe’ll be watching closely to see how Google navigates these challenges and if it can reduce its reliance on Safari.\n\nFeatured Image: photosince/shutterstock"
    },
    {
        "title": "Meta Pushes GA4 Integration, Boasts 22% Conversion Boost",
        "url": "https://www.searchenginejournal.com/meta-pushes-ga4-integration-boasts-22-conversion-boost/521601/",
        "content": "Meta is encouraging advertisers to integrate their Google Analytics 4 (GA4) data with their advertising accounts.\n\nBased on recent experimental data, the company claims this integration could improve campaign performance, citing a 22% conversion increase.\n\nUpon logging in with your advertiser account, a new pop-up will appear. Click on the ‘Connect Google Analytics’ button to initiate the integration process.\n\nInteresting new pop up in Meta Ads this morning, now allowing advertisers to connect their GA account. Have you seen it yet and connected yours? pic.twitter.com/stCfSQYqwl — Akvile DeFazio (@AkvileDeFazio) July 5, 2024\n\nStreamlined Integration Process\n\nKey Steps for Integration:\n\nUse URL Campaign Builder: Use Google’s Campaign URL Builder to generate trackable links for your meta ads. This tool allows you to add UTM parameters to your URLs, providing context for traffic sources in GA4. Define Source, Medium, and Campaign Variables: Set up your UTM parameters with ‘facebook’ as the source, ‘paid’ as the medium, and your specific campaign name. This ensures accurate attribution in GA4. Add Trackable Links to Meta Ads: Incorporate your UTM-tagged links into your Meta ad campaigns in the “Website URL” field or within the ad copy. Measure in Google Analytics 4: Once set up, you can view Meta ad performance in GA4 under Acquisition > Traffic Acquisition, with options to analyze by session medium, source, campaign, and more.\n\nBenefits Of Integration\n\nBy connecting GA4 data to Meta advertising accounts, marketers can gain insights into:\n\nHow Meta ads influence website traffic and user behavior\n\nCross-platform user journeys\n\nMore accurate conversion attribution\n\nIndustry Implications\n\nMeta’s push for GA4 integration signals a trend towards more holistic, cross-platform analytics in digital advertising.\n\nAs the industry adapts to the deprecation of third-party cookies, such integrations may become vital for effective campaign measurement and optimization.\n\nLimitations\n\nWhile GA4 integration offers valuable insights, it has limitations.\n\nDue to privacy regulations, GA4 cannot track individual users, and its attribution models have constraints, particularly for longer sales cycles.\n\nAlso, GA4 doesn’t account for ad impressions, potentially undervaluing upper-funnel campaigns.\n\nSome advertisers are turning to specialized (paid) attribution tools for more comprehensive tracking.\n\nFAQ How can GA4 integration improve campaign performance for Meta ads? Connecting GA4 to Meta ads can improve your campaigns. Meta claims this could increase sales or sign-ups by 22%. Using special tags in your links lets you see how your Facebook and Instagram ads affect your website visits and what people do there. This gives you a clearer picture of how well your ads work, helping you make smarter decisions about your ad money and get better results. What are the limitations of integrating GA4 with Meta ads? GA4, while helpful, has some downsides. Because of privacy laws, it can’t follow individual users, making it hard to tell precisely which ads led to sales, especially for products that take a while to sell. Also, GA4 doesn’t count the number of times an ad is shown, which might make early-stage advertising seem less valuable than it really is. Because of these issues, some companies pay for other tools to track their ads more thoroughly. Why is Meta encouraging advertisers to integrate GA4 with their ad accounts? Meta is pushing for this because the old ways of tracking people online (like using cookies) are becoming less useful. By working with GA4, Meta is trying to give advertisers a clearer picture of how their ads perform across various platforms. This detailed information is becoming increasingly important for businesses to plan and improve their online advertising strategies.\n\nFeatured Image: Wirestock Creators/Shutterstock"
    },
    {
        "title": "Meta Pushes GA4 Integration, Boasts 22% Conversion Boost",
        "url": "https://www.searchenginejournal.com/meta-pushes-ga4-integration-boasts-22-conversion-boost/521601/",
        "content": "Meta is encouraging advertisers to integrate their Google Analytics 4 (GA4) data with their advertising accounts.\n\nBased on recent experimental data, the company claims this integration could improve campaign performance, citing a 22% conversion increase.\n\nUpon logging in with your advertiser account, a new pop-up will appear. Click on the ‘Connect Google Analytics’ button to initiate the integration process.\n\nInteresting new pop up in Meta Ads this morning, now allowing advertisers to connect their GA account. Have you seen it yet and connected yours? pic.twitter.com/stCfSQYqwl — Akvile DeFazio (@AkvileDeFazio) July 5, 2024\n\nStreamlined Integration Process\n\nKey Steps for Integration:\n\nUse URL Campaign Builder: Use Google’s Campaign URL Builder to generate trackable links for your meta ads. This tool allows you to add UTM parameters to your URLs, providing context for traffic sources in GA4. Define Source, Medium, and Campaign Variables: Set up your UTM parameters with ‘facebook’ as the source, ‘paid’ as the medium, and your specific campaign name. This ensures accurate attribution in GA4. Add Trackable Links to Meta Ads: Incorporate your UTM-tagged links into your Meta ad campaigns in the “Website URL” field or within the ad copy. Measure in Google Analytics 4: Once set up, you can view Meta ad performance in GA4 under Acquisition > Traffic Acquisition, with options to analyze by session medium, source, campaign, and more.\n\nBenefits Of Integration\n\nBy connecting GA4 data to Meta advertising accounts, marketers can gain insights into:\n\nHow Meta ads influence website traffic and user behavior\n\nCross-platform user journeys\n\nMore accurate conversion attribution\n\nIndustry Implications\n\nMeta’s push for GA4 integration signals a trend towards more holistic, cross-platform analytics in digital advertising.\n\nAs the industry adapts to the deprecation of third-party cookies, such integrations may become vital for effective campaign measurement and optimization.\n\nLimitations\n\nWhile GA4 integration offers valuable insights, it has limitations.\n\nDue to privacy regulations, GA4 cannot track individual users, and its attribution models have constraints, particularly for longer sales cycles.\n\nAlso, GA4 doesn’t account for ad impressions, potentially undervaluing upper-funnel campaigns.\n\nSome advertisers are turning to specialized (paid) attribution tools for more comprehensive tracking.\n\nFAQ How can GA4 integration improve campaign performance for Meta ads? Connecting GA4 to Meta ads can improve your campaigns. Meta claims this could increase sales or sign-ups by 22%. Using special tags in your links lets you see how your Facebook and Instagram ads affect your website visits and what people do there. This gives you a clearer picture of how well your ads work, helping you make smarter decisions about your ad money and get better results. What are the limitations of integrating GA4 with Meta ads? GA4, while helpful, has some downsides. Because of privacy laws, it can’t follow individual users, making it hard to tell precisely which ads led to sales, especially for products that take a while to sell. Also, GA4 doesn’t count the number of times an ad is shown, which might make early-stage advertising seem less valuable than it really is. Because of these issues, some companies pay for other tools to track their ads more thoroughly. Why is Meta encouraging advertisers to integrate GA4 with their ad accounts? Meta is pushing for this because the old ways of tracking people online (like using cookies) are becoming less useful. By working with GA4, Meta is trying to give advertisers a clearer picture of how their ads perform across various platforms. This detailed information is becoming increasingly important for businesses to plan and improve their online advertising strategies.\n\nFeatured Image: Wirestock Creators/Shutterstock"
    },
    {
        "title": "WordPress Nested Pages Plugin High Severity Vulnerability",
        "url": "https://www.searchenginejournal.com/wordpress-nested-pages-plugin-vulnerability/521550/",
        "content": "The U.S. National Vulnerability Database (NVD) and Wordfence published a security advisory of a high severity Cross Site Request Forgery (CSRF) vulnerability affecting the Nested Pages WordPress plugin affecting up to +100,000 installations. The vulnerability received a Common Vulnerability Scoring System (CVSS) rating of 8.8 on a scale of 1 – 10, with ten representing the highest level severity.\n\nCross Site Request Forgery (CSRF)\n\nThe Cross Site Request Forgery (CSRF) is a type of attack that takes advantage of a security flaw in the Nested Pages plugin that allows unauthenticated attackers to call (execute) PHP files, which are the code level files of WordPress.\n\nThere is a missing or incorrect nonce validation, which is a common security feature used in WordPress plugins to secure forms and URLs. A second flaw in the plugin is a missing security feature called sanitization. Sanitization is a method of securing data that’s input or output which is also common to WordPress plugins but in this case is missing.\n\nAccording to Wordfence:\n\n“This is due to missing or incorrect nonce validation on the ‘settingsPage’ function and missing santization of the ‘tab’ parameter.”\n\nThe CSRF attack relies on getting a signed in WordPress user (like an Administrator) to click a link which in turn allows the attacker to complete the attack. This vulnerability is rated 8.8 which makes it a high severity threat. To put that into perspective, a score of 8.9 is a critical level threat which is an even higher level. So at 8.8 it is just short of a critical level threat.\n\nThis vulnerability affects all versions of the Nested Pages plugin up to and including version 3.2.7. The developers of the plugin released a security fix in version 3.2.8 and responsibly published the details of the security update in their changelog.\n\nThe official changelog documents the security fix:\n\n“Security update addressing CSRF issue in plugin settings”\n\nRead the advisory at Wordfence:\n\nNested Pages <= 3.2.7 – Cross-Site Request Forgery to Local File Inclusion\n\nRead the advisory at the NVD:\n\nCVE-2024-5943 Detail\n\nFeatured Image by Shutterstock/Dean Drobot"
    },
    {
        "title": "WordPress Nested Pages Plugin High Severity Vulnerability",
        "url": "https://www.searchenginejournal.com/wordpress-nested-pages-plugin-vulnerability/521550/",
        "content": "The U.S. National Vulnerability Database (NVD) and Wordfence published a security advisory of a high severity Cross Site Request Forgery (CSRF) vulnerability affecting the Nested Pages WordPress plugin affecting up to +100,000 installations. The vulnerability received a Common Vulnerability Scoring System (CVSS) rating of 8.8 on a scale of 1 – 10, with ten representing the highest level severity.\n\nCross Site Request Forgery (CSRF)\n\nThe Cross Site Request Forgery (CSRF) is a type of attack that takes advantage of a security flaw in the Nested Pages plugin that allows unauthenticated attackers to call (execute) PHP files, which are the code level files of WordPress.\n\nThere is a missing or incorrect nonce validation, which is a common security feature used in WordPress plugins to secure forms and URLs. A second flaw in the plugin is a missing security feature called sanitization. Sanitization is a method of securing data that’s input or output which is also common to WordPress plugins but in this case is missing.\n\nAccording to Wordfence:\n\n“This is due to missing or incorrect nonce validation on the ‘settingsPage’ function and missing santization of the ‘tab’ parameter.”\n\nThe CSRF attack relies on getting a signed in WordPress user (like an Administrator) to click a link which in turn allows the attacker to complete the attack. This vulnerability is rated 8.8 which makes it a high severity threat. To put that into perspective, a score of 8.9 is a critical level threat which is an even higher level. So at 8.8 it is just short of a critical level threat.\n\nThis vulnerability affects all versions of the Nested Pages plugin up to and including version 3.2.7. The developers of the plugin released a security fix in version 3.2.8 and responsibly published the details of the security update in their changelog.\n\nThe official changelog documents the security fix:\n\n“Security update addressing CSRF issue in plugin settings”\n\nRead the advisory at Wordfence:\n\nNested Pages <= 3.2.7 – Cross-Site Request Forgery to Local File Inclusion\n\nRead the advisory at the NVD:\n\nCVE-2024-5943 Detail\n\nFeatured Image by Shutterstock/Dean Drobot"
    },
    {
        "title": "Google Gives Exact Reason Why Negative SEO Doesn’t Work",
        "url": "https://www.searchenginejournal.com/google-explains-why-negative-seo-is-ineffective/521430/",
        "content": "Google’s Gary Illyes answered a question about negative SEO provides useful insights into the technical details of how Google prevents low quality spam links from affecting normal websites.\n\nThe answer about negative SEO was given in an interview in May and has gone unnoticed until now.\n\nNegative SEO\n\nNegative SEO is the practice of sabotaging a competitor with an avalanche of low quality links. The idea is that Google will assume that the competitor is spamming and knock them out of the search engine results pages (SERPs).\n\nThe practice of negative SEO originated in the online gambling space where the rewards for top ranking are high and the competition is fierce. I first heard of it around the mid-2000s (probably before 2010) when someone involved in the gambling space told me about it.\n\nVirtually all websites that rank for meaningful search queries attract low quality links and there is nothing unusual about, it’s always been this way. The concept of negative SEO became more prominent after the Penguin link spam update caused site owners to become more aware of the state of their inbound links.\n\nDoes Negative SEO Cause Harm?\n\nThe person interviewing Gary Illyes was taking questions from the audience.\n\nShe asked:\n\n“Does negative SEO via spammy link building, a competitor throwing tens of thousands of links at another competitor, does that kind of thing still harm people or has Google kind of pushed that off to the side?\n\nGoogle’s Gary Illyes answered the question by first asking the interviewer if she remembered the Penguin update to which she answered yes.\n\nHe then explained his experience reviewing examples of negative SEO that site owners and SEOs had sent him. He said that out of hundreds of cases he reviewed there was only one case that might have actually been negative SEO but that the web spam team wasn’t 100% sure.\n\nGary explained:\n\n“Around the time we released Penguin, there was tons and tons of tons of complaints about negative SEO, specifically link based negative SEO and then very un-smartly, I requested examples like show me examples, like show me how it works and show me that it worked. And then I got hundreds, literally hundreds of examples of alleged negative SEO and all of them were not negative SEO. It was always something that was so far away from negative SEO that I didn’t even bother looking further, except one that I sent to the web spam team for double checking and that we haven’t made up our mind about it, but it could have been negative SEO. With this, I want to say that the fear about negative SEO is much bigger than or much larger than it needs to be, we disable insane numbers of links…”\n\nThe above is Gary’s experience of negative SEO. Next he explains the exact reason why “negative SEO links” have no effect.\n\nLinks From Irrelevant Topics Are Not Counted\n\nAt about the 30 minute mark of the interview, Gary confirmed something interesting about how links evaluated that is important to understand. Google has, for a very long time, examined the context of the site that’s linking out to match it to the site that’s being linked to, and if they don’t match up then Google wouldn’t pass the PageRank signal.\n\nGary continued his answer:\n\n“If you see links from completely irrelevant sites, be that p–n sites or or pure spam sites or whatever, you can safely assume that we disabled the links from those sites because, one of the things is that we try to match the the topic of the target page plus whoever is linking out, and if they don’t match then why on Earth would we use those links? Like for example if someone is linking to your flower page from a Canadian casino that sells Viagra without prescription, then why would we trust that link? I would say that I would not worry about it. Like, find something else to worry about.”\n\nGoogle Matches Topics From Page To Page\n\nThere was a time, in the early days of SEO, when thousands of links from non-matching topics could boost a site to the top of Google’s search results. Some link builders used to offer “free” traffic counter widgets to universities that when placed in the footer would contain a link back to their client sites and they used to work. But Google tightened up on those kinds of links.\n\nWhat Gary said about links having to be relevant matches up with what link builders have known for at least twenty years. The concept of off topic links not being counted by Google was understood way in the days when people did reciprocal links.\n\nAlthough I can’t remember everything every Googler has ever said about negative SEO, this seems to be one of the rare occasions that a Googler offered a detailed reason why negative SEO doesn’t work.\n\nWatch Gary Illyes answer the question at the 26 minute mark:\n\nFeatured Image by Shutterstock/MDV Edwards"
    },
    {
        "title": "Google Gives Exact Reason Why Negative SEO Doesn’t Work",
        "url": "https://www.searchenginejournal.com/google-explains-why-negative-seo-is-ineffective/521430/",
        "content": "Google’s Gary Illyes answered a question about negative SEO provides useful insights into the technical details of how Google prevents low quality spam links from affecting normal websites.\n\nThe answer about negative SEO was given in an interview in May and has gone unnoticed until now.\n\nNegative SEO\n\nNegative SEO is the practice of sabotaging a competitor with an avalanche of low quality links. The idea is that Google will assume that the competitor is spamming and knock them out of the search engine results pages (SERPs).\n\nThe practice of negative SEO originated in the online gambling space where the rewards for top ranking are high and the competition is fierce. I first heard of it around the mid-2000s (probably before 2010) when someone involved in the gambling space told me about it.\n\nVirtually all websites that rank for meaningful search queries attract low quality links and there is nothing unusual about, it’s always been this way. The concept of negative SEO became more prominent after the Penguin link spam update caused site owners to become more aware of the state of their inbound links.\n\nDoes Negative SEO Cause Harm?\n\nThe person interviewing Gary Illyes was taking questions from the audience.\n\nShe asked:\n\n“Does negative SEO via spammy link building, a competitor throwing tens of thousands of links at another competitor, does that kind of thing still harm people or has Google kind of pushed that off to the side?\n\nGoogle’s Gary Illyes answered the question by first asking the interviewer if she remembered the Penguin update to which she answered yes.\n\nHe then explained his experience reviewing examples of negative SEO that site owners and SEOs had sent him. He said that out of hundreds of cases he reviewed there was only one case that might have actually been negative SEO but that the web spam team wasn’t 100% sure.\n\nGary explained:\n\n“Around the time we released Penguin, there was tons and tons of tons of complaints about negative SEO, specifically link based negative SEO and then very un-smartly, I requested examples like show me examples, like show me how it works and show me that it worked. And then I got hundreds, literally hundreds of examples of alleged negative SEO and all of them were not negative SEO. It was always something that was so far away from negative SEO that I didn’t even bother looking further, except one that I sent to the web spam team for double checking and that we haven’t made up our mind about it, but it could have been negative SEO. With this, I want to say that the fear about negative SEO is much bigger than or much larger than it needs to be, we disable insane numbers of links…”\n\nThe above is Gary’s experience of negative SEO. Next he explains the exact reason why “negative SEO links” have no effect.\n\nLinks From Irrelevant Topics Are Not Counted\n\nAt about the 30 minute mark of the interview, Gary confirmed something interesting about how links evaluated that is important to understand. Google has, for a very long time, examined the context of the site that’s linking out to match it to the site that’s being linked to, and if they don’t match up then Google wouldn’t pass the PageRank signal.\n\nGary continued his answer:\n\n“If you see links from completely irrelevant sites, be that p–n sites or or pure spam sites or whatever, you can safely assume that we disabled the links from those sites because, one of the things is that we try to match the the topic of the target page plus whoever is linking out, and if they don’t match then why on Earth would we use those links? Like for example if someone is linking to your flower page from a Canadian casino that sells Viagra without prescription, then why would we trust that link? I would say that I would not worry about it. Like, find something else to worry about.”\n\nGoogle Matches Topics From Page To Page\n\nThere was a time, in the early days of SEO, when thousands of links from non-matching topics could boost a site to the top of Google’s search results. Some link builders used to offer “free” traffic counter widgets to universities that when placed in the footer would contain a link back to their client sites and they used to work. But Google tightened up on those kinds of links.\n\nWhat Gary said about links having to be relevant matches up with what link builders have known for at least twenty years. The concept of off topic links not being counted by Google was understood way in the days when people did reciprocal links.\n\nAlthough I can’t remember everything every Googler has ever said about negative SEO, this seems to be one of the rare occasions that a Googler offered a detailed reason why negative SEO doesn’t work.\n\nWatch Gary Illyes answer the question at the 26 minute mark:\n\nFeatured Image by Shutterstock/MDV Edwards"
    },
    {
        "title": "Instagram Algorithm Shift: Why ‘Sends’ Matter More Than Ever",
        "url": "https://www.searchenginejournal.com/instagram-algorithm-shift-why-sends-matter-more-than-ever/521389/",
        "content": "In a recent Instagram Reel, Adam Mosseri, the head of Instagram, revealed a top signal the platform uses to rank content: sends per reach.\n\nThis metric measures the number of people who share a post with friends through direct messages (DMs) relative to the total number of viewers.\n\nMosseri advises creating content people want to share directly with close friends and family, saying it can improve your reach over time.\n\nThis insight helps demystify Instagram’s ranking algorithms and can assist your efforts to improve visibility on the platform.\n\nInstagram’s ‘Sends Per Reach’ Ranking Signal\n\nMosseri describes the sends per reach ranking signal and its reasoning:\n\n“Some advice: One of the most important signals we use in ranking is sends per reach. So out of all the people who saw your video or photo, how many of them sent it to a friend in a DM? At Instagram we’re trying to be a place where people can be creative, but in a way that brings people together. We want to not only be a place where you passively consume content, but where you discover things you want to tell your friends about. A reel that made you laugh so hard you want to send it to your brother or sister. Or a soccer highlight that blew your mind and you want to send it to another fan. That kind of thing. So, don’t force it as a creator. But if you can, think about making content that people would want to send to a friend, or to someone they care about.”\n\nThe emphasis on sends as a ranking factor aligns with Instagram’s desire to become a platform where users discover and share content that resonates with them personally.\n\nAdvice For Creators\n\nWhile encouraging creators to produce shareworthy content, Mosseri cautioned against forced attempts to game the system.\n\nHowever, prompting users to share photos and videos via DM is said to boost reach\n\nWhat Does This Mean For You?\n\nGetting people to share posts and reels with friends can improve reach, resulting in more engagement and leads.\n\nContent creators and businesses can use this information to refine their Instagram strategies.\n\nRather than seeing Instagram’s focus on shareable content as an obstacle, consider it an opportunity to experiment with new approaches.\n\nIf your reach has been declining lately, and you can’t figure out why, this may be the factor that brings it back up.\n\nFeatured Image: soma sekhar/Shutterstock"
    },
    {
        "title": "Instagram Algorithm Shift: Why ‘Sends’ Matter More Than Ever",
        "url": "https://www.searchenginejournal.com/instagram-algorithm-shift-why-sends-matter-more-than-ever/521389/",
        "content": "In a recent Instagram Reel, Adam Mosseri, the head of Instagram, revealed a top signal the platform uses to rank content: sends per reach.\n\nThis metric measures the number of people who share a post with friends through direct messages (DMs) relative to the total number of viewers.\n\nMosseri advises creating content people want to share directly with close friends and family, saying it can improve your reach over time.\n\nThis insight helps demystify Instagram’s ranking algorithms and can assist your efforts to improve visibility on the platform.\n\nInstagram’s ‘Sends Per Reach’ Ranking Signal\n\nMosseri describes the sends per reach ranking signal and its reasoning:\n\n“Some advice: One of the most important signals we use in ranking is sends per reach. So out of all the people who saw your video or photo, how many of them sent it to a friend in a DM? At Instagram we’re trying to be a place where people can be creative, but in a way that brings people together. We want to not only be a place where you passively consume content, but where you discover things you want to tell your friends about. A reel that made you laugh so hard you want to send it to your brother or sister. Or a soccer highlight that blew your mind and you want to send it to another fan. That kind of thing. So, don’t force it as a creator. But if you can, think about making content that people would want to send to a friend, or to someone they care about.”\n\nThe emphasis on sends as a ranking factor aligns with Instagram’s desire to become a platform where users discover and share content that resonates with them personally.\n\nAdvice For Creators\n\nWhile encouraging creators to produce shareworthy content, Mosseri cautioned against forced attempts to game the system.\n\nHowever, prompting users to share photos and videos via DM is said to boost reach\n\nWhat Does This Mean For You?\n\nGetting people to share posts and reels with friends can improve reach, resulting in more engagement and leads.\n\nContent creators and businesses can use this information to refine their Instagram strategies.\n\nRather than seeing Instagram’s focus on shareable content as an obstacle, consider it an opportunity to experiment with new approaches.\n\nIf your reach has been declining lately, and you can’t figure out why, this may be the factor that brings it back up.\n\nFeatured Image: soma sekhar/Shutterstock"
    },
    {
        "title": "You Don’t Need Robots.txt On Root Domain, Says Google",
        "url": "https://www.searchenginejournal.com/you-dont-need-robots-txt-on-root-domain-says-google/521382/",
        "content": "In a recent LinkedIn post, Google Analyst Gary Illyes challenged a long-standing belief about the placement of robots.txt files.\n\nFor years, the conventional wisdom has been that a website’s robots.txt file must reside at the root domain (e.g., example.com/robots.txt).\n\nHowever, Illyes has clarified that this isn’t an absolute requirement and revealed a lesser-known aspect of the Robots Exclusion Protocol (REP).\n\nRobots.txt File Flexibility\n\nThe robots.txt file doesn’t have to be located at the root domain (example.com/robots.txt).\n\nAccording to Illyes, having two separate robots.txt files hosted on different domains is permissible—one on the primary website and another on a content delivery network (CDN).\n\nIllyes explains that websites can centralize their robots.txt file on the CDN while controlling crawling for their main site.\n\nFor instance, a website could have two robots.txt files: one at https://cdn.example.com/robots.txt and another at https://www.example.com/robots.txt.\n\nThis approach allows you to maintain a single, comprehensive robots.txt file on their CDN and redirect requests from their main domain to this centralized file.\n\nIllyes notes that crawlers complying with RFC9309 will follow the redirect and use the target file as the robotstxt file for the original domain.\n\nLooking Back At 30 Years Of Robots.txt\n\nAs the Robots Exclusion Protocol celebrates its 30th anniversary this year, Illyes’ revelation highlights how web standards continue to evolve.\n\nHe even speculates whether the file needs to be named “robots.txt,” hinting at possible changes in how crawl directives are managed.\n\nHow This Can Help You\n\nFollowing Illyes’ guidance can help you in the following ways:\n\nCentralized Management: By consolidating robots.txt rules in one location, you can maintain and update crawl directives across your web presence. Improved Consistency: A single source of truth for robots.txt rules reduces the risk of conflicting directives between your main site and CDN. Flexibility: This approach allows for more adaptable configurations, especially for sites with complex architectures or those using multiple subdomains and CDNs.\n\nA streamlined approach to managing robots.txt files can improve both site management and SEO efforts.\n\nFeatured Image: BestForBest/Shutterstock"
    },
    {
        "title": "You Don’t Need Robots.txt On Root Domain, Says Google",
        "url": "https://www.searchenginejournal.com/you-dont-need-robots-txt-on-root-domain-says-google/521382/",
        "content": "In a recent LinkedIn post, Google Analyst Gary Illyes challenged a long-standing belief about the placement of robots.txt files.\n\nFor years, the conventional wisdom has been that a website’s robots.txt file must reside at the root domain (e.g., example.com/robots.txt).\n\nHowever, Illyes has clarified that this isn’t an absolute requirement and revealed a lesser-known aspect of the Robots Exclusion Protocol (REP).\n\nRobots.txt File Flexibility\n\nThe robots.txt file doesn’t have to be located at the root domain (example.com/robots.txt).\n\nAccording to Illyes, having two separate robots.txt files hosted on different domains is permissible—one on the primary website and another on a content delivery network (CDN).\n\nIllyes explains that websites can centralize their robots.txt file on the CDN while controlling crawling for their main site.\n\nFor instance, a website could have two robots.txt files: one at https://cdn.example.com/robots.txt and another at https://www.example.com/robots.txt.\n\nThis approach allows you to maintain a single, comprehensive robots.txt file on their CDN and redirect requests from their main domain to this centralized file.\n\nIllyes notes that crawlers complying with RFC9309 will follow the redirect and use the target file as the robotstxt file for the original domain.\n\nLooking Back At 30 Years Of Robots.txt\n\nAs the Robots Exclusion Protocol celebrates its 30th anniversary this year, Illyes’ revelation highlights how web standards continue to evolve.\n\nHe even speculates whether the file needs to be named “robots.txt,” hinting at possible changes in how crawl directives are managed.\n\nHow This Can Help You\n\nFollowing Illyes’ guidance can help you in the following ways:\n\nCentralized Management: By consolidating robots.txt rules in one location, you can maintain and update crawl directives across your web presence. Improved Consistency: A single source of truth for robots.txt rules reduces the risk of conflicting directives between your main site and CDN. Flexibility: This approach allows for more adaptable configurations, especially for sites with complex architectures or those using multiple subdomains and CDNs.\n\nA streamlined approach to managing robots.txt files can improve both site management and SEO efforts.\n\nFeatured Image: BestForBest/Shutterstock"
    },
    {
        "title": "Google Shows How To Beat Reddit & Big Brands",
        "url": "https://www.searchenginejournal.com/google-shows-how-to-beat-reddit/521375/",
        "content": "In an interview published on YouTube, Google’s Gary Illyes offered advice on what small sites should consider doing if they want to compete against Reddit, Amazon and other big brand websites.\n\nAbout Big Brand Dominance\n\nGoogle’s Gary Illyes answered questions about SEO back in May that went underreported so I’m correcting that oversight this month. Gary answered a question about how to compete against Reddit and big brands.\n\nWhile it may appear that Gary is skeptical that Reddit is dominating, he’s not disputing that perception and that’s not the context of his answer. The context is larger than Reddit because his answer is about the core issue of competing against big brands in the search engine results pages (SERPs).\n\nThis is the question that an audience member asked:\n\n“Since Reddit and big publishers dominate nowadays in the SERPS for many keywords, what can the smaller brands do besides targeting the long tail keywords?”\n\nThe History Of Big Brands In The SERPs\n\nGary’s answer encompasses the entire history of big brands in the SERPs and the SEO response to that. About.com was a website about virtually any topic of interest and it used to rank for just about everything. It was like the Wikipedia of its day and many SEOs resented how About.com used to rank so well.\n\nHe first puts that context into his answer, that this complaint about Reddit is part of a long history of various brands ranking at the top of the SERPs then washing out of the SERPs as trends change.\n\nGary answered:\n\n“So before I joined Google I was doing some SEO stuff for big publishers. …SEO type. Like I was also server manager like a cluster manager. So, I would have had the same questions and in fact back in the day we saw these kind of questions all the time. Now it’s Reddit. Back then it was Amazon. A few years before that, it was I think …About.com. Pretty much every two years the name that you would put there …changes.”\n\nSmall Sites Can Outcompete Big Brands\n\nGary next shares that the history of SEO is also about small sites figuring out how to outcompete the bigger sites. This is also true. Some big sites started as small sites that figured out a way to outcompete larger big brand sites. For example, Reviewed.com, before it was purchased by USA Today, was literally started by a child whose passion for the topic contributed to it becoming massively successful.\n\nGary says that there are two things to do:\n\nWait until someone else figures out how to outcompete and then copy them Or figure it out yourself and lead the way\n\nBut of course, if you wait for someone else to show the way it’s probably too late.\n\nHe continued:\n\n“It seems that people always figure out ways to compete with whoever would be the second word in that question. So it’s not like, oh my God, like everything sucks now and we can retire. It’s like, one thing you could do is to wait it out and let someone else come up with something for you that you can use to compete with Reddit and the big publishers that allegedly dominate nowadays the SERPs. Or you sit down and you start thinking about how can you employ some marketing strategies that will boost you to around the same positions as the big publishers and Reddit and whatnot. One of the most inspiring presentations I’ve seen was the empathetic marketing… do that. Find a way to compete with these positions in the SERPs because it is possible, you just have to find the the angle to compete with them.”\n\nGary is right. Big brands are slowed down by bureaucracy and scared to take chances. As I mentioned about Reviewed.com, a good strategy can outrun the big brands all day long, I know this from my own experience and from knowing others who have done the same thing, including the founder of Reviewed.com.\n\nLong Tail Keywords & Other Strategies\n\nGary next talked about long tail keywords. A lot of newbie SEO gurus define long tail keyword phrases with a lot of words in it. That’s 100% wrong. Long tail keyword phrases are keyword phrases that searches rarely use. It’s the rareness of keyword use that makes them long tail, not how many words are in the keyword phrase.\n\nThe context this part of Gary’s answer is that the person asking the question essentially dismissed long tail search queries as the crumbs that the big brands leave behind for small sites.\n\nGary explains:\n\n“And also the other thing is that, like saying that you are left with the long tail keywords. It’s like we see like 15 to even more percent of new long tail keywords every single day. There’s lots of traffic in long tail keywords. You you can jump on that bandwagon and capture a ton of traffic.”\n\nSomething left unmentioned is that conquering long tail keyword phrases is one way to create awareness that a site is about a topic. People come for the long tail and return for the head phrases (the queries with more traffic).\n\nThe problem with some small sites is that they’re trying to hit the big traffic keywords without first showing relevance in the long tail. Starting small and building up toward big is one of the secrets of successful sites.\n\nSmall Sites Can Be Powerful\n\nGary is right, there is a lot of traffic in the long tail and emerging trends. The thing that small sites need to remember is that big sites move slow and have to get through layers of bureaucracy in order to make a strategic decision. The stakes for them are also higher so they’re not prone to take big swings either. Speed and the ability to make bold moves is the small site’s super power. Exercise it.\n\nI know from my own experience and from working with clients that it’s absolutely possible to outrank to big sites that have been around for years. The history of SEO is littered with small sites that outpaced the slower moving bigger sites.\n\nWatch Gary answer this question at the 20 minute mark:\n\nFeatured Image by Shutterstock/Volodymyr TVERDOKHLIB"
    },
    {
        "title": "Google Shows How To Beat Reddit & Big Brands",
        "url": "https://www.searchenginejournal.com/google-shows-how-to-beat-reddit/521375/",
        "content": "In an interview published on YouTube, Google’s Gary Illyes offered advice on what small sites should consider doing if they want to compete against Reddit, Amazon and other big brand websites.\n\nAbout Big Brand Dominance\n\nGoogle’s Gary Illyes answered questions about SEO back in May that went underreported so I’m correcting that oversight this month. Gary answered a question about how to compete against Reddit and big brands.\n\nWhile it may appear that Gary is skeptical that Reddit is dominating, he’s not disputing that perception and that’s not the context of his answer. The context is larger than Reddit because his answer is about the core issue of competing against big brands in the search engine results pages (SERPs).\n\nThis is the question that an audience member asked:\n\n“Since Reddit and big publishers dominate nowadays in the SERPS for many keywords, what can the smaller brands do besides targeting the long tail keywords?”\n\nThe History Of Big Brands In The SERPs\n\nGary’s answer encompasses the entire history of big brands in the SERPs and the SEO response to that. About.com was a website about virtually any topic of interest and it used to rank for just about everything. It was like the Wikipedia of its day and many SEOs resented how About.com used to rank so well.\n\nHe first puts that context into his answer, that this complaint about Reddit is part of a long history of various brands ranking at the top of the SERPs then washing out of the SERPs as trends change.\n\nGary answered:\n\n“So before I joined Google I was doing some SEO stuff for big publishers. …SEO type. Like I was also server manager like a cluster manager. So, I would have had the same questions and in fact back in the day we saw these kind of questions all the time. Now it’s Reddit. Back then it was Amazon. A few years before that, it was I think …About.com. Pretty much every two years the name that you would put there …changes.”\n\nSmall Sites Can Outcompete Big Brands\n\nGary next shares that the history of SEO is also about small sites figuring out how to outcompete the bigger sites. This is also true. Some big sites started as small sites that figured out a way to outcompete larger big brand sites. For example, Reviewed.com, before it was purchased by USA Today, was literally started by a child whose passion for the topic contributed to it becoming massively successful.\n\nGary says that there are two things to do:\n\nWait until someone else figures out how to outcompete and then copy them Or figure it out yourself and lead the way\n\nBut of course, if you wait for someone else to show the way it’s probably too late.\n\nHe continued:\n\n“It seems that people always figure out ways to compete with whoever would be the second word in that question. So it’s not like, oh my God, like everything sucks now and we can retire. It’s like, one thing you could do is to wait it out and let someone else come up with something for you that you can use to compete with Reddit and the big publishers that allegedly dominate nowadays the SERPs. Or you sit down and you start thinking about how can you employ some marketing strategies that will boost you to around the same positions as the big publishers and Reddit and whatnot. One of the most inspiring presentations I’ve seen was the empathetic marketing… do that. Find a way to compete with these positions in the SERPs because it is possible, you just have to find the the angle to compete with them.”\n\nGary is right. Big brands are slowed down by bureaucracy and scared to take chances. As I mentioned about Reviewed.com, a good strategy can outrun the big brands all day long, I know this from my own experience and from knowing others who have done the same thing, including the founder of Reviewed.com.\n\nLong Tail Keywords & Other Strategies\n\nGary next talked about long tail keywords. A lot of newbie SEO gurus define long tail keyword phrases with a lot of words in it. That’s 100% wrong. Long tail keyword phrases are keyword phrases that searches rarely use. It’s the rareness of keyword use that makes them long tail, not how many words are in the keyword phrase.\n\nThe context this part of Gary’s answer is that the person asking the question essentially dismissed long tail search queries as the crumbs that the big brands leave behind for small sites.\n\nGary explains:\n\n“And also the other thing is that, like saying that you are left with the long tail keywords. It’s like we see like 15 to even more percent of new long tail keywords every single day. There’s lots of traffic in long tail keywords. You you can jump on that bandwagon and capture a ton of traffic.”\n\nSomething left unmentioned is that conquering long tail keyword phrases is one way to create awareness that a site is about a topic. People come for the long tail and return for the head phrases (the queries with more traffic).\n\nThe problem with some small sites is that they’re trying to hit the big traffic keywords without first showing relevance in the long tail. Starting small and building up toward big is one of the secrets of successful sites.\n\nSmall Sites Can Be Powerful\n\nGary is right, there is a lot of traffic in the long tail and emerging trends. The thing that small sites need to remember is that big sites move slow and have to get through layers of bureaucracy in order to make a strategic decision. The stakes for them are also higher so they’re not prone to take big swings either. Speed and the ability to make bold moves is the small site’s super power. Exercise it.\n\nI know from my own experience and from working with clients that it’s absolutely possible to outrank to big sites that have been around for years. The history of SEO is littered with small sites that outpaced the slower moving bigger sites.\n\nWatch Gary answer this question at the 20 minute mark:\n\nFeatured Image by Shutterstock/Volodymyr TVERDOKHLIB"
    },
    {
        "title": "Google Explains Reasons For Crawled Not Indexed",
        "url": "https://www.searchenginejournal.com/google-explains-crawled-not-indexed/521321/",
        "content": "Back in May Google’s Gary Illyes sat for an interview at the SERP Conf 2024 conference in Bulgaria and answered a question about the causes of crawled but not indexed, offering multiple reasons that are helpful for debugging and fixing this error.\n\nAlthough the interview happened in May, the video of the interview went underreported and not many people have actually watched it. I only heard of it because the always awesome Olesia Korobka (@Giridja) recently drew attention to the interview in a Facebook post.\n\nSo even though the interview happened in May, the information is still timely and useful.\n\nReason For Crawled – Currently Not Indexed\n\nCrawled Currently Not Indexed is a reference to an error report in the Google Search Console Page Indexing report which alerts that a page was crawled by Google but was not indexed.\n\nDuring a live interview someone submitted a question, asking:\n\n“Can crawled but not indexed be a result of a page being too similar to other stuff already indexed? So is Google suggesting there is enough other stuff already and your stuff is not unique enough?”\n\nGoogle’s search console documentation doesn’t provide an answer as to why Google may crawl a page and not index it, so it’s a legitimate question.\n\nGary Illyes answered that yes, one of the reasons could be that there is already other content that is similar. But he also goes on to say that there are other reasons, too.\n\nHe answered:\n\n“Yeah, that that could be one thing that it can mean. Crawled but not indexed is, ideally we would break up that category into more granular chunks, but it’s super hard because of how the data internally exists. It can be a bunch of things, dupe elimination is one of those things, where we crawl the page and then we decide to not index it because there’s already a version of that or an extremely similar version of that content available in our index and it has better signals. But yeah, but it it can be multiple things.”\n\nGeneral Quality Of Site Can Impact Indexing\n\nGary then called attention to another reason why Google might crawl but choose not to index a site, saying that it could be a site quality issue.\n\nIllyes then continued his answer:\n\n“And the general quality of the of the site, that can matter a lot of how many of these crawled but not indexed you see in search console. If the number of these URLs is very high that could hint at general quality issues. And I’ve seen that a lot since February, where suddenly we just decided that we are indexing a vast amount of URLs on a site just because …our perception of the site has changed.”\n\nOther Reasons For Crawled Not Indexed\n\nGary next offered other reasons for why URLs might be crawled but not indexed, saying that it could be that Google’s perception of the site could have changed but that it could be a technical issue.\n\nGary explained:\n\n“…And one possibility is that when you see that number rising, that the perception of… Google’s perception of the site has changed, that could be one thing. But then there could also be that there was an error, for example on the site and then it served the same exact page to every single URL on the site. That could also be one of the reasons that you see that number climbing. So yeah, there could be many things.”\n\nTakeaways\n\nGary provided answers that should help debug why a web page might be crawled but not indexed by Google.\n\nContent is similar to content already ranked in the search engine results pages (SERPs)\n\nExact same content exists on another site that has better signals\n\nGeneral site quality issues\n\nTechnical issues\n\nAlthough Illyes didn’t elaborate on what he meant about another site with better signals, I’m fairly certain that he’s describing the scenario when a site syndicates its content to another site and Google chooses to rank the other site for the content and not the original publisher.\n\nWatch Gary answer this question at the 9 minute mark of the recorded interview:\n\nFeatured Image by Shutterstock/Roman Samborskyi"
    },
    {
        "title": "Google Explains Reasons For Crawled Not Indexed",
        "url": "https://www.searchenginejournal.com/google-explains-crawled-not-indexed/521321/",
        "content": "Back in May Google’s Gary Illyes sat for an interview at the SERP Conf 2024 conference in Bulgaria and answered a question about the causes of crawled but not indexed, offering multiple reasons that are helpful for debugging and fixing this error.\n\nAlthough the interview happened in May, the video of the interview went underreported and not many people have actually watched it. I only heard of it because the always awesome Olesia Korobka (@Giridja) recently drew attention to the interview in a Facebook post.\n\nSo even though the interview happened in May, the information is still timely and useful.\n\nReason For Crawled – Currently Not Indexed\n\nCrawled Currently Not Indexed is a reference to an error report in the Google Search Console Page Indexing report which alerts that a page was crawled by Google but was not indexed.\n\nDuring a live interview someone submitted a question, asking:\n\n“Can crawled but not indexed be a result of a page being too similar to other stuff already indexed? So is Google suggesting there is enough other stuff already and your stuff is not unique enough?”\n\nGoogle’s search console documentation doesn’t provide an answer as to why Google may crawl a page and not index it, so it’s a legitimate question.\n\nGary Illyes answered that yes, one of the reasons could be that there is already other content that is similar. But he also goes on to say that there are other reasons, too.\n\nHe answered:\n\n“Yeah, that that could be one thing that it can mean. Crawled but not indexed is, ideally we would break up that category into more granular chunks, but it’s super hard because of how the data internally exists. It can be a bunch of things, dupe elimination is one of those things, where we crawl the page and then we decide to not index it because there’s already a version of that or an extremely similar version of that content available in our index and it has better signals. But yeah, but it it can be multiple things.”\n\nGeneral Quality Of Site Can Impact Indexing\n\nGary then called attention to another reason why Google might crawl but choose not to index a site, saying that it could be a site quality issue.\n\nIllyes then continued his answer:\n\n“And the general quality of the of the site, that can matter a lot of how many of these crawled but not indexed you see in search console. If the number of these URLs is very high that could hint at general quality issues. And I’ve seen that a lot since February, where suddenly we just decided that we are indexing a vast amount of URLs on a site just because …our perception of the site has changed.”\n\nOther Reasons For Crawled Not Indexed\n\nGary next offered other reasons for why URLs might be crawled but not indexed, saying that it could be that Google’s perception of the site could have changed but that it could be a technical issue.\n\nGary explained:\n\n“…And one possibility is that when you see that number rising, that the perception of… Google’s perception of the site has changed, that could be one thing. But then there could also be that there was an error, for example on the site and then it served the same exact page to every single URL on the site. That could also be one of the reasons that you see that number climbing. So yeah, there could be many things.”\n\nTakeaways\n\nGary provided answers that should help debug why a web page might be crawled but not indexed by Google.\n\nContent is similar to content already ranked in the search engine results pages (SERPs)\n\nExact same content exists on another site that has better signals\n\nGeneral site quality issues\n\nTechnical issues\n\nAlthough Illyes didn’t elaborate on what he meant about another site with better signals, I’m fairly certain that he’s describing the scenario when a site syndicates its content to another site and Google chooses to rank the other site for the content and not the original publisher.\n\nWatch Gary answer this question at the 9 minute mark of the recorded interview:\n\nFeatured Image by Shutterstock/Roman Samborskyi"
    },
    {
        "title": "Google’s AI Overviews Coincide With Drop In Mobile Searches",
        "url": "https://www.searchenginejournal.com/googles-ai-overviews-coincide-with-drop-in-mobile-searches/521303/",
        "content": "A new study by search industry expert Rand Fishkin has revealed that Google’s rollout of AI overviews in May led to a noticeable decrease in search volume, particularly on mobile devices.\n\nThe study, which analyzed millions of Google searches in the United States and European Union, sheds light on the unexpected consequences of AI integration.\n\nAI Overviews Rollout & Reversal\n\nIn May 2024, Google rolled out AI overviews in the United States, which generate summaries for many search queries.\n\nHowever, the feature was met with mixed reactions and was quickly dialed back by the end of the month.\n\nIn a blog post published on May 30, Google admitted to inaccurate or unhelpful AI overviews, particularly for unusual queries.\n\nGoogle says it implemented over a dozen technical improvements to its systems in response.\n\nA subsequent study by SE Ranking found the frequency of these summaries decreased, with only 8% of searches now triggering an AI Overview. However, when shown, these overviews are now longer and more detailed, averaging 25% more content.\n\nSE Ranking also noted that after expansion, AI overviews typically link to fewer sources, usually around four.\n\nDecline In Mobile Searches\n\nFishkin’s analysis reveals that the introduction of AI Overviews coincided with a marked decline in mobile searches in May.\n\nWhile desktop searches saw a slight increase, the drop in mobile searches was significant, considering that mobile accounts for nearly two-thirds of all Google queries.\n\nThis finding suggests that users may have been less inclined to search on their mobile devices when confronted with AI-generated summaries.\n\nFishkin commented:\n\n“The most visible changes in May were shared by both the EU and US, notably… Mobile searches fell a considerable amount (if anything spooked Google into rolling back this feature, I’d put my money on this being it).”\n\nHe adds:\n\n“If I were running Google, that dip in mobile searches (remember, mobile accounts for almost 2/3rds of all Google queries) would scare the stock-price-worshiping-crap outta me.”\n\nImpact On Overall Search Behavior\n\nDespite the dip in mobile searches, the study found that search behavior remained relatively stable during the AI overviews rollout.\n\nThe number of clicks per search on mobile devices increased slightly, while desktop clicks per search remained flat.\n\nThis indicates that while some users may have been deterred from initiating searches, those who did engage with the AI Overviews still clicked on results at a similar or slightly higher rate than the previous months.\n\nImplications For Google & the Search Industry\n\nThe study highlights the challenges Google faces in integrating AI-generated content into its search results.\n\nAdditionally, the research found other concerning trends in Google search behavior:\n\nLow Click-through Rates : Only 360 out of every 1,000 Google searches in the US result in clicks to non-Google websites. The EU fares slightly better with 374 clicks per 1,000 searches.\n\n: Only 360 out of every 1,000 Google searches in the US result in clicks to non-Google websites. The EU fares slightly better with 374 clicks per 1,000 searches. Zero-click Searches Dominate : Nearly 60% of searches in both regions end without any clicks, classified as “zero-click searches.”\n\n: Nearly 60% of searches in both regions end without any clicks, classified as “zero-click searches.” Google’s Self-referral Traffic: About 30% of clicks from US searches go to Google-owned properties, with a somewhat lower percentage in the EU.\n\nWhy SEJ Cares\n\nThis study underscores the need for adaptable SEO strategies.\n\nAs an industry, we may need to shift focus towards optimizing for zero-click searches and diversifying traffic sources beyond Google.\n\nThe findings also raise questions about the future of AI in search.\n\nWhile major tech companies continue to invest in AI technologies, this study suggests that implementation may not always yield the expected results.\n\nFeatured Image: Marco Lazzarini/Shutterstock"
    },
    {
        "title": "Google’s AI Overviews Coincide With Drop In Mobile Searches",
        "url": "https://www.searchenginejournal.com/googles-ai-overviews-coincide-with-drop-in-mobile-searches/521303/",
        "content": "A new study by search industry expert Rand Fishkin has revealed that Google’s rollout of AI overviews in May led to a noticeable decrease in search volume, particularly on mobile devices.\n\nThe study, which analyzed millions of Google searches in the United States and European Union, sheds light on the unexpected consequences of AI integration.\n\nAI Overviews Rollout & Reversal\n\nIn May 2024, Google rolled out AI overviews in the United States, which generate summaries for many search queries.\n\nHowever, the feature was met with mixed reactions and was quickly dialed back by the end of the month.\n\nIn a blog post published on May 30, Google admitted to inaccurate or unhelpful AI overviews, particularly for unusual queries.\n\nGoogle says it implemented over a dozen technical improvements to its systems in response.\n\nA subsequent study by SE Ranking found the frequency of these summaries decreased, with only 8% of searches now triggering an AI Overview. However, when shown, these overviews are now longer and more detailed, averaging 25% more content.\n\nSE Ranking also noted that after expansion, AI overviews typically link to fewer sources, usually around four.\n\nDecline In Mobile Searches\n\nFishkin’s analysis reveals that the introduction of AI Overviews coincided with a marked decline in mobile searches in May.\n\nWhile desktop searches saw a slight increase, the drop in mobile searches was significant, considering that mobile accounts for nearly two-thirds of all Google queries.\n\nThis finding suggests that users may have been less inclined to search on their mobile devices when confronted with AI-generated summaries.\n\nFishkin commented:\n\n“The most visible changes in May were shared by both the EU and US, notably… Mobile searches fell a considerable amount (if anything spooked Google into rolling back this feature, I’d put my money on this being it).”\n\nHe adds:\n\n“If I were running Google, that dip in mobile searches (remember, mobile accounts for almost 2/3rds of all Google queries) would scare the stock-price-worshiping-crap outta me.”\n\nImpact On Overall Search Behavior\n\nDespite the dip in mobile searches, the study found that search behavior remained relatively stable during the AI overviews rollout.\n\nThe number of clicks per search on mobile devices increased slightly, while desktop clicks per search remained flat.\n\nThis indicates that while some users may have been deterred from initiating searches, those who did engage with the AI Overviews still clicked on results at a similar or slightly higher rate than the previous months.\n\nImplications For Google & the Search Industry\n\nThe study highlights the challenges Google faces in integrating AI-generated content into its search results.\n\nAdditionally, the research found other concerning trends in Google search behavior:\n\nLow Click-through Rates : Only 360 out of every 1,000 Google searches in the US result in clicks to non-Google websites. The EU fares slightly better with 374 clicks per 1,000 searches.\n\n: Only 360 out of every 1,000 Google searches in the US result in clicks to non-Google websites. The EU fares slightly better with 374 clicks per 1,000 searches. Zero-click Searches Dominate : Nearly 60% of searches in both regions end without any clicks, classified as “zero-click searches.”\n\n: Nearly 60% of searches in both regions end without any clicks, classified as “zero-click searches.” Google’s Self-referral Traffic: About 30% of clicks from US searches go to Google-owned properties, with a somewhat lower percentage in the EU.\n\nWhy SEJ Cares\n\nThis study underscores the need for adaptable SEO strategies.\n\nAs an industry, we may need to shift focus towards optimizing for zero-click searches and diversifying traffic sources beyond Google.\n\nThe findings also raise questions about the future of AI in search.\n\nWhile major tech companies continue to invest in AI technologies, this study suggests that implementation may not always yield the expected results.\n\nFeatured Image: Marco Lazzarini/Shutterstock"
    },
    {
        "title": "GraphRAG Is A Better RAG And Now It’s Free",
        "url": "https://www.searchenginejournal.com/graphrag/521296/",
        "content": "Microsoft is making publicly available a new technology called GraphRAG, which enables chatbots and answer engines to connect the dots across an entire dataset, outperforming standard Retrieval-Augmented Generation (RAG) by large margins.\n\nWhat’s The Difference Between RAG And GraphRAG?\n\nRAG (Retrieval-Augmented Generation) is a technology that enables an LLM to reach into a database like a search index and use that as a basis for answering a question. It can be used to bridge a large language model and a conventional search engine index.\n\nThe benefit of RAG is that it can use authoritative and trustworthy data in order to answer questions. RAG also enables generative AI chatbots to use up to date information to answer questions about topics that the LLM wasn’t trained on. This is an approach that’s used by AI search engines like Perplexity.\n\nThe upside of RAG is related to its use of embeddings. Embeddings is a way of representing the semantic relationships between words, sentences, and documents. This representation enables the retrieval part of RAG to match a search query to text in a database (like a search index).\n\nBut the downside of using embeddings is that it limits the RAG to matching text at a granular level (as opposed to a global reach across the data).\n\nMicrosoft explains:\n\n“Since naive RAG only considers the top-k most similar chunks of input text, it fails. Even worse, it will match the question against chunks of text that are superficially similar to that question, resulting in misleading answers.”\n\nThe innovation of GraphRAG is that it enables an LLM to answer questions based on the overall dataset.\n\nWhat GraphRAG does is it creates a knowledge graph out of the indexed documents, also known as unstructured data. The obvious example of unstructured data are web pages. So when GraphRAG creates a knowledge graph, it’s creating a “structured” representation of the relationships between various “entities” (like people, places, concepts, and things) which is then more easily understood by machines.\n\nGraphRAG creates what Microsoft calls “communities” of general themes (high level) and more granular topics (low level). An LLM then creates a summarization of each of these communities, a “hierarchical summary of the data” that is then used to answer questions. This is the breakthrough because it enables a chatbot to answer questions based more on knowledge (the summarizations) than depending on embeddings.\n\nThis is how Microsoft explains it:\n\n“Using an LLM to summarize each of these communities creates a hierarchical summary of the data, providing an overview of a dataset without needing to know which questions to ask in advance. Each community serves as the basis of a community summary that describes its entities and their relationships. …Community summaries help answer such global questions because the graph index of entity and relationship descriptions has already considered all input texts in its construction. Therefore, we can use a map-reduce approach for question answering that retains all relevant content from the global data context…”\n\nExamples Of RAG Versus GraphRAG\n\nThe original GraphRAG research paper illustrated the superiority of the GraphRAG approach in being able to answer questions for which there is no exact match data in the indexed documents. The example uses a limited dataset of Russian and Ukrainian news from the month of June 2023 (translated to English).\n\nSimple Text Matching Question\n\nThe first question that was used an example was “What is Novorossiya?” and both RAG and GraphRAG answered the question, with GraphRAG offering a more detailed response.\n\nThe short answer by the way is that “Novorossiya” translates to New Russia and is a reference to Ukrainian lands that were conquered by Russia in the 18th century.\n\nThe second example question required that the machine make connections between concepts within the indexed documents, what Microsoft calls a “query-focused summarization (QFS) task” which is different than a simple text-based retrieval task. It requires what Microsoft calls, “connecting the dots.”\n\nThe question asked of the RAG and GraphRAG systems:\n\n“What has Novorossiya done?”\n\nThis is the RAG answer:\n\n“The text does not provide specific information on what Novorossiya has done.”\n\nGraphRAG answered the question of “What has Novorossiya done?” with a two paragraph answer that details the results of the Novorossiya political movement.\n\nHere’s a short excerpt from the two paragraph answer:\n\n“Novorossiya, a political movement in Ukraine, has been involved in a series of destructive activities, particularly targeting various entities in Ukraine [Entities (6494, 912)]. The movement has been linked to plans to destroy properties of several Ukrainian entities, including Rosen, the Odessa Canning Factory, the Odessa Regional Radio Television Transmission Center, and the National Television Company of Ukraine [Relationships (15207, 15208, 15209, 15210)]… …The Office of the General Prosecutor in Ukraine has reported on the creation of Novorossiya, indicating the government’s awareness and potential concern over the activities of this movement…”\n\nThe above is just some of the answer which was extracted from the limited one-month dataset, which illustrates how GraphRAG is able to connect the dots across all of the documents.\n\nGraphRAG Now Publicly Available\n\nMicrosoft announced that GraphRAG is publicly available for use by anybody.\n\n“Today, we’re pleased to announce that GraphRAG is now available on GitHub, offering more structured information retrieval and comprehensive response generation than naive RAG approaches. The GraphRAG code repository is complemented by a solution accelerator, providing an easy-to-use API experience hosted on Azure that can be deployed code-free in a few clicks.”\n\nMicrosoft released GraphRAG in order to make the solutions based on it more publicly accessible and to encourage feedback for improvements.\n\nRead the announcement:\n\nGraphRAG: New tool for complex data discovery now on GitHub\n\nFeatured Image by Shutterstock/Deemerwha studio"
    },
    {
        "title": "GraphRAG Is A Better RAG And Now It’s Free",
        "url": "https://www.searchenginejournal.com/graphrag/521296/",
        "content": "Microsoft is making publicly available a new technology called GraphRAG, which enables chatbots and answer engines to connect the dots across an entire dataset, outperforming standard Retrieval-Augmented Generation (RAG) by large margins.\n\nWhat’s The Difference Between RAG And GraphRAG?\n\nRAG (Retrieval-Augmented Generation) is a technology that enables an LLM to reach into a database like a search index and use that as a basis for answering a question. It can be used to bridge a large language model and a conventional search engine index.\n\nThe benefit of RAG is that it can use authoritative and trustworthy data in order to answer questions. RAG also enables generative AI chatbots to use up to date information to answer questions about topics that the LLM wasn’t trained on. This is an approach that’s used by AI search engines like Perplexity.\n\nThe upside of RAG is related to its use of embeddings. Embeddings is a way of representing the semantic relationships between words, sentences, and documents. This representation enables the retrieval part of RAG to match a search query to text in a database (like a search index).\n\nBut the downside of using embeddings is that it limits the RAG to matching text at a granular level (as opposed to a global reach across the data).\n\nMicrosoft explains:\n\n“Since naive RAG only considers the top-k most similar chunks of input text, it fails. Even worse, it will match the question against chunks of text that are superficially similar to that question, resulting in misleading answers.”\n\nThe innovation of GraphRAG is that it enables an LLM to answer questions based on the overall dataset.\n\nWhat GraphRAG does is it creates a knowledge graph out of the indexed documents, also known as unstructured data. The obvious example of unstructured data are web pages. So when GraphRAG creates a knowledge graph, it’s creating a “structured” representation of the relationships between various “entities” (like people, places, concepts, and things) which is then more easily understood by machines.\n\nGraphRAG creates what Microsoft calls “communities” of general themes (high level) and more granular topics (low level). An LLM then creates a summarization of each of these communities, a “hierarchical summary of the data” that is then used to answer questions. This is the breakthrough because it enables a chatbot to answer questions based more on knowledge (the summarizations) than depending on embeddings.\n\nThis is how Microsoft explains it:\n\n“Using an LLM to summarize each of these communities creates a hierarchical summary of the data, providing an overview of a dataset without needing to know which questions to ask in advance. Each community serves as the basis of a community summary that describes its entities and their relationships. …Community summaries help answer such global questions because the graph index of entity and relationship descriptions has already considered all input texts in its construction. Therefore, we can use a map-reduce approach for question answering that retains all relevant content from the global data context…”\n\nExamples Of RAG Versus GraphRAG\n\nThe original GraphRAG research paper illustrated the superiority of the GraphRAG approach in being able to answer questions for which there is no exact match data in the indexed documents. The example uses a limited dataset of Russian and Ukrainian news from the month of June 2023 (translated to English).\n\nSimple Text Matching Question\n\nThe first question that was used an example was “What is Novorossiya?” and both RAG and GraphRAG answered the question, with GraphRAG offering a more detailed response.\n\nThe short answer by the way is that “Novorossiya” translates to New Russia and is a reference to Ukrainian lands that were conquered by Russia in the 18th century.\n\nThe second example question required that the machine make connections between concepts within the indexed documents, what Microsoft calls a “query-focused summarization (QFS) task” which is different than a simple text-based retrieval task. It requires what Microsoft calls, “connecting the dots.”\n\nThe question asked of the RAG and GraphRAG systems:\n\n“What has Novorossiya done?”\n\nThis is the RAG answer:\n\n“The text does not provide specific information on what Novorossiya has done.”\n\nGraphRAG answered the question of “What has Novorossiya done?” with a two paragraph answer that details the results of the Novorossiya political movement.\n\nHere’s a short excerpt from the two paragraph answer:\n\n“Novorossiya, a political movement in Ukraine, has been involved in a series of destructive activities, particularly targeting various entities in Ukraine [Entities (6494, 912)]. The movement has been linked to plans to destroy properties of several Ukrainian entities, including Rosen, the Odessa Canning Factory, the Odessa Regional Radio Television Transmission Center, and the National Television Company of Ukraine [Relationships (15207, 15208, 15209, 15210)]… …The Office of the General Prosecutor in Ukraine has reported on the creation of Novorossiya, indicating the government’s awareness and potential concern over the activities of this movement…”\n\nThe above is just some of the answer which was extracted from the limited one-month dataset, which illustrates how GraphRAG is able to connect the dots across all of the documents.\n\nGraphRAG Now Publicly Available\n\nMicrosoft announced that GraphRAG is publicly available for use by anybody.\n\n“Today, we’re pleased to announce that GraphRAG is now available on GitHub, offering more structured information retrieval and comprehensive response generation than naive RAG approaches. The GraphRAG code repository is complemented by a solution accelerator, providing an easy-to-use API experience hosted on Azure that can be deployed code-free in a few clicks.”\n\nMicrosoft released GraphRAG in order to make the solutions based on it more publicly accessible and to encourage feedback for improvements.\n\nRead the announcement:\n\nGraphRAG: New tool for complex data discovery now on GitHub\n\nFeatured Image by Shutterstock/Deemerwha studio"
    },
    {
        "title": "Robots.txt Turns 30: Google Highlights Hidden Strengths",
        "url": "https://www.searchenginejournal.com/robots-txt-turns-30-google-highlights-hidden-strengths/521276/",
        "content": "In a recent LinkedIn post, Gary Illyes, Analyst at Google, highlights lesser-known aspects of the robots.txt file as it marks its 30th year.\n\nThe robots.txt file, a web crawling and indexing component, has been a mainstay of SEO practices since its inception.\n\nHere’s one of the reasons why it remains useful.\n\nRobust Error Handling\n\nIllyes emphasized the file’s resilience to errors.\n\n“robots.txt is virtually error free,” Illyes stated.\n\nIn his post, he explained that robots.txt parsers are designed to ignore most mistakes without compromising functionality.\n\nThis means the file will continue operating even if you accidentally include unrelated content or misspell directives.\n\nHe elaborated that parsers typically recognize and process key directives such as user-agent, allow, and disallow while overlooking unrecognized content.\n\nUnexpected Feature: Line Commands\n\nIllyes pointed out the presence of line comments in robots.txt files, a feature he found puzzling given the file’s error-tolerant nature.\n\nHe invited the SEO community to speculate on the reasons behind this inclusion.\n\nResponses To Illyes’ Post\n\nThe SEO community’s response to Illyes’ post provides additional context on the practical implications of robots.txt’s error tolerance and the use of line comments.\n\nAndrew C., Founder of Optimisey, highlighted the utility of line comments for internal communication, stating:\n\n“When working on websites you can see a line comment as a note from the Dev about what they want that ‘disallow’ line in the file to do.”\n\nNima Jafari, an SEO Consultant, emphasized the value of comments in large-scale implementations.\n\nHe noted that for extensive robots.txt files, comments can “help developers and the SEO team by providing clues about other lines.”\n\nProviding historical context, Lyndon NA, a digital marketer, compared robots.txt to HTML specifications and browsers.\n\nHe suggested that the file’s error tolerance was likely an intentional design choice, stating:\n\n“Robots.txt parsers were made lax so that content might still be accessed (imagine if G had to ditch a site, because someone borked 1 bit of robots.txt?).”\n\nWhy SEJ Cares\n\nUnderstanding the nuances of the robots.txt file can help you optimize sites better.\n\nWhile the file’s error-tolerant nature is generally beneficial, it could potentially lead to overlooked issues if not managed carefully.\n\nRead also: 8 Common Robots.txt Issues And How To Fix Them\n\nWhat To Do With This Information\n\nReview your robots.txt file: Ensure it contains only necessary directives and is free from potential errors or misconfigurations. Be cautious with spelling: While parsers may ignore misspellings, this could result in unintended crawling behaviors. Leverage line comments: Comments can be used to document your robots.txt file for future reference.\n\nFeatured Image: sutadism/Shutterstock"
    },
    {
        "title": "Robots.txt Turns 30: Google Highlights Hidden Strengths",
        "url": "https://www.searchenginejournal.com/robots-txt-turns-30-google-highlights-hidden-strengths/521276/",
        "content": "In a recent LinkedIn post, Gary Illyes, Analyst at Google, highlights lesser-known aspects of the robots.txt file as it marks its 30th year.\n\nThe robots.txt file, a web crawling and indexing component, has been a mainstay of SEO practices since its inception.\n\nHere’s one of the reasons why it remains useful.\n\nRobust Error Handling\n\nIllyes emphasized the file’s resilience to errors.\n\n“robots.txt is virtually error free,” Illyes stated.\n\nIn his post, he explained that robots.txt parsers are designed to ignore most mistakes without compromising functionality.\n\nThis means the file will continue operating even if you accidentally include unrelated content or misspell directives.\n\nHe elaborated that parsers typically recognize and process key directives such as user-agent, allow, and disallow while overlooking unrecognized content.\n\nUnexpected Feature: Line Commands\n\nIllyes pointed out the presence of line comments in robots.txt files, a feature he found puzzling given the file’s error-tolerant nature.\n\nHe invited the SEO community to speculate on the reasons behind this inclusion.\n\nResponses To Illyes’ Post\n\nThe SEO community’s response to Illyes’ post provides additional context on the practical implications of robots.txt’s error tolerance and the use of line comments.\n\nAndrew C., Founder of Optimisey, highlighted the utility of line comments for internal communication, stating:\n\n“When working on websites you can see a line comment as a note from the Dev about what they want that ‘disallow’ line in the file to do.”\n\nNima Jafari, an SEO Consultant, emphasized the value of comments in large-scale implementations.\n\nHe noted that for extensive robots.txt files, comments can “help developers and the SEO team by providing clues about other lines.”\n\nProviding historical context, Lyndon NA, a digital marketer, compared robots.txt to HTML specifications and browsers.\n\nHe suggested that the file’s error tolerance was likely an intentional design choice, stating:\n\n“Robots.txt parsers were made lax so that content might still be accessed (imagine if G had to ditch a site, because someone borked 1 bit of robots.txt?).”\n\nWhy SEJ Cares\n\nUnderstanding the nuances of the robots.txt file can help you optimize sites better.\n\nWhile the file’s error-tolerant nature is generally beneficial, it could potentially lead to overlooked issues if not managed carefully.\n\nRead also: 8 Common Robots.txt Issues And How To Fix Them\n\nWhat To Do With This Information\n\nReview your robots.txt file: Ensure it contains only necessary directives and is free from potential errors or misconfigurations. Be cautious with spelling: While parsers may ignore misspellings, this could result in unintended crawling behaviors. Leverage line comments: Comments can be used to document your robots.txt file for future reference.\n\nFeatured Image: sutadism/Shutterstock"
    }
]